review_id,id,paper_title,abstract,review,rating,conf_rev,metareview,conf_meta,recommendation,labels
2017_12,https://openreview.net/forum?id=S1vyujVye,Deep Unsupervised Learning Through Spatial Contrasting,"Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images.  This criterion can be employed within conventional neural net- works and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.","Deep Unsupervised Learning Through Spatial Contrasting. This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. ----------------Strengths:----------------- The training objective is reasonable. In particular, high-level features show translation invariance. ----------------- The proposed methods are effective for initializing neural networks for supervised training on several datasets. ------------------------Weaknesses:----------------- The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). ----------------- The paper is experimentally misleading.--------The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. ----------------Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. ----------------The proposed method seems useful only for natural images where different patches from the same image can be similar to each other. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_13,https://openreview.net/forum?id=S1vyujVye,Deep Unsupervised Learning Through Spatial Contrasting,"Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images.  This criterion can be employed within conventional neural net- works and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.","Deep Unsupervised Learning Through Spatial Contrasting. The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_14,https://openreview.net/forum?id=S1vyujVye,Deep Unsupervised Learning Through Spatial Contrasting,"Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images.  This criterion can be employed within conventional neural net- works and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.","Deep Unsupervised Learning Through Spatial Contrasting. This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:------------------------The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).----------------I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.----------------Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?----------------I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).----------------The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?------------------------All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_15,https://openreview.net/forum?id=S1HEBe_Jl,Learning To Protect Communications With Adversarial Neural Cryptography,"We ask whether neural networks can learn to use secret keys to protect information from other neural networks.  Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary.  Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn  how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.","Learning To Protect Communications With Adversarial Neural Cryptography. The submission proposes to modify the typical GAN architecture slightly to include ""encrypt"" (Alice) and ""decrypt"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.----------------While this is a nice thought experiment, there are significant barriers to this submission having a practical impact:--------1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.--------2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Interesting paper but not over the accept bar.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_16,https://openreview.net/forum?id=S1HEBe_Jl,Learning To Protect Communications With Adversarial Neural Cryptography,"We ask whether neural networks can learn to use secret keys to protect information from other neural networks.  Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary.  Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn  how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.","Learning To Protect Communications With Adversarial Neural Cryptography. The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.----------------The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.----------------I have two more minor concerns:----------------1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.----------------2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).----------------I like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,Interesting paper but not over the accept bar.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_17,https://openreview.net/forum?id=S1HEBe_Jl,Learning To Protect Communications With Adversarial Neural Cryptography,"We ask whether neural networks can learn to use secret keys to protect information from other neural networks.  Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary.  Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn  how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.","Learning To Protect Communications With Adversarial Neural Cryptography. This paper proposed to use GAN for encrypted communications.----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.----------------In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.",4: Ok but not good enough - rejection,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Interesting paper but not over the accept bar.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_31,https://openreview.net/forum?id=HkNKFiGex,Neural Photo Editing With Introspective Adversarial Networks,"The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network,   a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.","Neural Photo Editing With Introspective Adversarial Networks. The paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.----------------The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.----------------On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.----------------On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a ""list of things to make it work"" fashion. I would like to see more empirical results in that direction to help clear up things.----------------Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication.----------------UPDATE: The rating has been updated to a 6 following the authors' reply.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_32,https://openreview.net/forum?id=HkNKFiGex,Neural Photo Editing With Introspective Adversarial Networks,"The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network,   a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.","Neural Photo Editing With Introspective Adversarial Networks. UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:--------+ The IAN model itself is interesting as standard GAN-based approaches do not simultaneously train an autoencoder.----------------Cons:--------- The writing is unclear at times and the mathematical formulation of the IAN is not very precise.--------- Many different ideas are proposed in the paper without sufficient empirical validation to characterize their individual contributions.--------- The photo editing interface, though interesting, is probably better suited for a conference with more of an HCI focus.----------------* Section 2: The gradient descent step procedure seems to be quite similar to the approach proposed by [2]. More elaboration on the differences would be helpful.--------* Section 3: It is unclear whether the discriminator has three binary outputs or if there is a softmax over the three possible labels. The paper does not mention whether L_G and L_D are minimized or maximized. Presumably they are both minimized, but in that case it is counter-intuitive that the generator attempts to decrease the probability that the discriminator assigns the ""real"" label to the generated samples and reconstructions. In addition, in the minimization scenario L_D maximizes the probability of the correct labels being assigned to X_gen and \hat{X} but minimizes the probability of the ""real"" label being assigned to X.--------* Section 3.2: It is odd that not training MADE leads to better performance, as training MADE should lead to a better variational approximation to the true posterior. More exploration seems warranted here.--------* Section 3.3.2: One drawback of autoregressive approaches is that sampling is slow. How do you reconcile this with its use in an interactive application, where speed is important?--------* Figure 7: The Inception score is typically expressed as an exponentiated KL-divergence. It is odd that the scores are being presented here as percents.--------* Section 4.1: The Inception score's direct application to non-natural images is indeed problematic. One potential workaround is to compute exponentiated KL for a discriminative net trained specifically for the dataset, e.g. to predict binary attributes on CelebA. ----------------Overall, the paper attempts to simultaneously do too many things. It could be made much stronger by focusing on the primary contribution, the IAN, and performing a comparison against similar approaches such as [1]. The other techniques, such as IAF with randomized MADE, MDC blocks, and orthogonal regularization are potentially interesting in their own right but the current results are not conclusive as to their specific benefits.----------------[1] Larsen, Anders Boesen Lindbo, Søren Kaae Sønderby, and Ole Winther. ""Autoencoding beyond pixels using a learned similarity metric."" arXiv preprint arXiv:1512.09300 (2015).--------[2] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative Visual Manipulation on the Natural Image Manifold,” ECCV 2016.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_33,https://openreview.net/forum?id=HkNKFiGex,Neural Photo Editing With Introspective Adversarial Networks,"The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network,   a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.","Neural Photo Editing With Introspective Adversarial Networks. After rebuttal:----------------I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing. Therefore I raise my rating. Still, the paper could use polishing. If written in a better way, it would be a definite accept in my opinion.---------------------------------Initial review:----------------The paper presents a tool for exploring latent spaces of generative models, and ""introspective adversarial network"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE). On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well. On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al. [1] is not discussed in enough detail. Overall, I am in the borderline mode, and may change my opinion depending on how the discussion phase goes. ----------------Detailed comments:----------------1) The presented model combines elements of a large number of existing techniques: GAN, VAE, VAE/GAN (Lamb et al. 2016), inverse autoregressive flow (IAF), PixelRNN, ResNet, dilated convolutions. In addition, the authors propose new modifications: orthogonal regularization (inspired by Saxe et al.), ternary discriminator in a GAN. This makes the overall architecture complicated. An extensive ablation study could allow to judge about the effect of different components, but the ablation study presented in the paper is somewhat restricted. What do we learn from the proposed architecture? Can other researchers gain any new insights? What is important, what is not? Answering these question would significantly raise the potential impact of this paper.----------------2) Related to the previous point, proper analysis requires adequate measures of performance. Qualitative results are nice, but with the current surge of interest in generative models it gets very difficult to rely on qualitative evaluations: many methods produce visually similar results, and unless there is an obvious large jump in the quality of the produced images, it is unclear how to compare these. I do appreciate the effort authors have already put into evaluating the model: especially the keypoint error is interesting. Unfortunately, none of the presented measures evaluates visual quality of the images. A user study would be useful - I realize it is additional effort, but what is so restrictively difficult about it? Perhaps not with AMT, but with some fellow researchers/students.----------------3) Work [1] looks very related to the proposed visualization tool and deserves a more thorough discussion than a single sentence in the related work section. The paper by Zhu et al. appeared on arxiv more than 1,5 months before the ICLR deadline, and, more importantly, it has been published at ECCV before the ICLR deadline. This is unfortunate for the authors, but I think this makes the paper count as prior work, not concurrent. In the end this is up to ACs and PCs to decide. Anyway, I strongly suggest the authors to add a detailed discussion of differences of the two approaches, their capabilities, strengths and weaknesses. The authors could also try to directly compare to the approach of Zhu et al. or explain why it is impossible.----------------4) May be a good idea to extend Appendix A with more approaches (VAE and DCGAN are not state of the art, are they? why not show at least VAE/GAN?) and more datasets. If this is not possible, please explain why. Samples of faces from IAN do look very impressive, but a fair comparison with SOTA would strengthen your point. By the way, I assume the samples are random, not cherry-picked? Please mention it in the paper. ----------------5) The analysis capabilities of the proposed tool are not fully explored. What does it teach us about generative models? Does it work on non-face datasets? Overall, it seems that since the paper includes two largely disjoint contributions (a tool and a generative model), none of the two gets analyzed in depth, which makes the paper look somewhat incomplete.  ----------------Small remarks:----------------1) Why not include 8% result of IAN on SVHN into the table? It is easy to miss otherwise. From the table it is absolutely unclear that some of the results are not comparable. The table should be more self-explanatory.----------------[1] Zhu et al., ""Generative Visual Manipulation on the Natural Image Manifold"", ECCV 2016, https://arxiv.org/pdf/1609.03552v2.pdf",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_40,https://openreview.net/forum?id=HyQWFOVge,Significance Of Softmax-based Features Over Metric Learning-based Features,"The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. End-to-end DML approaches such as Magnet Loss and lifted structured feature embedding show state-of-the-art performance in several image recognition tasks. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmax-based network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmax-based features are markedly better than the state-of-the-art DML features for tasks such as fine-grained recognition, attribute estimation, clustering, and retrieval.","Significance Of Softmax-based Features Over Metric Learning-based Features. I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. ----------------Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. ----------------It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_41,https://openreview.net/forum?id=HyQWFOVge,Significance Of Softmax-based Features Over Metric Learning-based Features,"The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. End-to-end DML approaches such as Magnet Loss and lifted structured feature embedding show state-of-the-art performance in several image recognition tasks. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmax-based network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmax-based features are markedly better than the state-of-the-art DML features for tasks such as fine-grained recognition, attribute estimation, clustering, and retrieval.","Significance Of Softmax-based Features Over Metric Learning-based Features. I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single ""retrieval""-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a ""classification-by-retrieval"" approach which seems contrived. While ostensibly evaluating ""retrieval"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.----------------It seems obvious that if your task is classification, a network trained with a classification loss will be best. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. It's still a classification task. ----------------I don't really see the point of using DML in these scenarios. I guess prior work claims to outperform SoftMax in these settings so this paper is fighting back against this and I should be thankful for this paper. But I think this paper's narrative is a bit off. The narrative shouldn't be ""We can get good retrieval features from softmax networks with appropriate normalization"". It should be ""It never made sense to train or evaluate these things as retrieval tasks. Direct classification is better"". For example, why are you taking the second to last layer or pool5 layer from these networks? Why aren't you taking the last layer? That should do well in these evaluations, right? Table 1 and 2 do show that using softmax probabilities directly tends to be better than doing classification-by-retrieval (works better or the same as doing retrieval with an earlier layer of features, except on Oxford flowers).----------------GoogLeNet is quite deep and gets auxiliary supervision. By the second-to-last layer of the network, the activations could look a lot like category membership already. And category membership is all that's needed for the tasks in 4.2 and 4.3. -------- --------I don't think my pre-review question was adequately addressed. I was getting at this concern by pointing out numerous scenarios where distance metric learning makes sense because you have fine-grained associations between instances at training time, NOT categorical associations -- e.g. this product photo corresponds to this photo of the object in a scene [Bell et al. 2015], this 3d model correspond to this sketch [Wang et al. 2015], this sketch corresponds to this photo [Sangkloy et al. 2016], this ground view corresponds to this aerial view [Lin et al., 2015]. DeepFace and follow-up works on LFW could also fit into this space because there are few training samples per class (few training samples per person identity). You cite DeepFace and Bell et al. 2015 but you don't compare on those benchmarks. I think those are exactly the tasks where DML makes sense.----------------Maybe the ""retrieval on classification datasets"" would be a reasonable benchmark if the test and train classes were completely different. Then you could argue that softmax is learning a useful representation yet the last layer isn't directly useful since the categories change. But that's not the case here, is it?----------------With all of this said, I'm not sure whether I'm positive or negative about this paper. I think you're onto something significant -- people have been using DML where it is not appropriate -- but addressed it in the wrong way -- by using softmax for ""classification by retrieval"". But you don't need to do retrieval! Softmax is already telling you the class prediction! Why go through the extra step of finding nearest neighbors with some intermediate feature?----------------AnonReviewer3 also raises some good points and you should be thankful that a reviewer is willing to dig so deep to help make your experiments sound! I don't think his/her concerns are disqualifying for this paper, though, as long as it is fixed.----------------I look forward to hearing your response. I want this paper to be published, but I think it needs to be tweaked.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_42,https://openreview.net/forum?id=HyQWFOVge,Significance Of Softmax-based Features Over Metric Learning-based Features,"The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. End-to-end DML approaches such as Magnet Loss and lifted structured feature embedding show state-of-the-art performance in several image recognition tasks. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmax-based network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmax-based features are markedly better than the state-of-the-art DML features for tasks such as fine-grained recognition, attribute estimation, clustering, and retrieval.","Significance Of Softmax-based Features Over Metric Learning-based Features. There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets / tasks for which the DML approaches were designed. ----------------This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully/properly tune the softmax baselines, or that comparisons were not apples-to-apples. Also, one change in the current paper is the addition of L2 normalization, which is well motivated and helps improve SoftMax feature distances. Different dimensionality reduction approaches are also tested. These changes are minor, but especially the L2 normalization proves to be a simple but effective improvement for SoftMax features.----------------A big issue is with how pre-training is performed (in Magnet Loss the softmax baselines were pretrained for less time on ImageNet). The approach taken here is reasonable, but so is the approach in Magnet Loss (for different reasons). Ultimately, both are fine. Unfortunately, due to use of different schemes, the results are not comparable. Let me copy-paste what I wrote in an earlier comment: ----------------My main concern about the paper is that the comparisons in Tables 1 and 2 and Figure 4 to Rippel et al. are not apples-to-apples. Basically, the papers shows that absolute results of using SoftMax w full pre-training (PT) on ImageNet is superior to any of the results in Rippel's paper (including both the Softmax and Magnet results). But as the current results show, PT appears to be critical to obtaining such good numbers - The Rippel SoftMax numbers use only 3 PT stages and are dramatically worse than the full PT on ImageNet. As it stands, I am not convinced that SoftMax is actually better than Magnet. Here is the evidence we have (I'll use Stanford Dogs as an example, but any of the datasets have the same conclusion): (1) Softmax w 3 stages of PT: 26.6% (from Rippel paper) and 32.7% (from authors' reproduction) (2) Magnet w 3 stages of PT: 24.9% (3) Softmax w full PT: 18.3% (4) Magnet w full PT: not shown From this all I see is that PT is critical for getting absolute good results. However, what about Magnet w full PT? These results are not shown either here or in the original Rippel paper (I went back and looked). As such, I do not think it is justifiable to claim superiority of Softmax to Magnet based on available evidence. (Note: I looked back carefully at Rippel's paper, and it appears that the authors use 3 PT stages as a form of ""warmup"". There is a statement that using full PT would ""defeat the purpose of pursuing DML"". I'm not sure if I agree w Rippel's statement since in the present paper there is clear evidence that full PT is hugely helpful, at least for softmax. That being said, I did not see any evidence in the Rippel paper that PT is harmful or that DML wouldn't work with full PT.)----------------The authors responded to my concern by claiming that “from Rippel's results, it is no doubt that Magnet@3epochPT > Magnet@FullPT and Magnet@3epoch > Softmax@3epochPT.” However, I went back to Rippel’s paper, and simply the Magnet@FullPT experiment never appears. I further went and contacted Oren Rippel himself, and he verified he never ran the Magnet@FullPT experiment. I encourage the authors to contact Oren Rippel regarding this if they wish to verify (I have asked Oren Rippel to not reveal my identity). [Disclaimer: I am NOT Oren Rippel]. The authors mentioned that they are training Magnet@FullPT. If results were shown for Magnet@FullPT and also retrain the Magnet@3epochPT as a sanity check, that would help alleviate this concern. Alternatively, the language in Section 4 and the Tables could be altered to make clear that the methods use different pretraining and hence are not comparable.----------------Overall, I am actually quite sympathetic to this work. I think it could serve as an important sanity-check paper for the community and quite relevant to ICLR. Having proper and strong SoftMax baselines should prove quite useful to the DML community and to this line of work.----------------However, currently I find the main results (table 1, table 2, figure 4, etc.) to be misleading. If indeed it were the case that Magnet@3epochPT > Magnet@FullPT, then it would be fine. However, at this point as far as I know no one has actually tried Magnet@FullPT. And, given the general importance and effectiveness of pre-training, especially when transferring to small dataset, I would be hugely surprised if Magnet@FullPT was not superior by a large margin. I think either having this experiment in place or altering the writing / presentation of the results would be critical to allow for publishing.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_46,https://openreview.net/forum?id=rJXTf9Bxg,Conditional Image Synthesis With Auxiliary Classifier Gans,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.","Conditional Image Synthesis With Auxiliary Classifier Gans. Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.----------------Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.----------------Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_47,https://openreview.net/forum?id=rJXTf9Bxg,Conditional Image Synthesis With Auxiliary Classifier Gans,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.","Conditional Image Synthesis With Auxiliary Classifier Gans. This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. ----------------Questions for the authors: --------(1) Why do you think splitting the imagenet training into 100 different models improves performance?  Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. --------(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize  log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. ----------------Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. ----------------[1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498)--------[2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)--------[3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_48,https://openreview.net/forum?id=rJXTf9Bxg,Conditional Image Synthesis With Auxiliary Classifier Gans,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.","Conditional Image Synthesis With Auxiliary Classifier Gans. This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.----------------* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?--------* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.--------* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.----------------[1] Salimans, Tim, et al. ""Improved techniques for training GANs."" Advances in Neural Information Processing Systems. 2016.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_61,https://openreview.net/forum?id=HyAddcLge,Revisiting Distributed Synchronous Sgd,"Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.","Revisiting Distributed Synchronous Sgd. This paper was easy to read, the main idea was presented very clearly.----------------The main points of the paper (and my concerns are below) can be summarized as follows:--------1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a ""MPI_Reduce"" would be used (even if we wait for the slowest guy)?--------2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.--------3.they propose to take gradient from the first ""N"" workers out of ""N+b"" --------workers available. My concern here is that they focused only on the --------workers, but what if the ""parameter server"" will became to slow? What --------if the parameter server would be the bottleneck? How would you address --------this situation? But still if the number of nodes (N) is not large, and --------the deep DNN is used, I can imagine that the communciation will not --------take more than 30% of the run-time.------------------------My largest concern is with the experiments. Different batch size --------implies that different learning rate should be chosen, right? How did --------you tune the learning rates and other parameters for e.g. Figure 5 you --------provide some formulas in (A2) but clearly this can bias your Figures, --------right? meaning, that if you tune ""\gamma, \beta"" for each N, it could --------be somehow more representative? also it would be nicer if you run the --------experiment many times and then report average, best and worst case --------behaviour. because now it can be just coinsidence, right? ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_62,https://openreview.net/forum?id=HyAddcLge,Revisiting Distributed Synchronous Sgd,"Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.","Revisiting Distributed Synchronous Sgd. This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. ----------------My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:----------------- provide more experiments to show the performance with different efficiency distributions of learners.--------- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_63,https://openreview.net/forum?id=HyAddcLge,Revisiting Distributed Synchronous Sgd,"Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.","Revisiting Distributed Synchronous Sgd. The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is solving difficult enough problem with novel enough idea. ----------------In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism --------to cut off updates of too much staledness just as the proposed method ignores all the remaining --------updates after having N updates. For example, one can measure the average time spent to --------obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD --------so that Async-SGD does not perform so poorly.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_70,https://openreview.net/forum?id=BJ46w6Ule,Dynamic Partition Models,"We present a new approach for learning compact and intuitive distributed representations with binary encoding. Rather than summing up expert votes as in products of experts, we employ for each variable the opinion of the most reliable expert. Data points are hence explained through a partitioning of the variables into expert supports. The partitions are dynamically adapted based on which experts are active. During the learning phase we adopt a smoothed version of this model that uses separate mixtures for each data dimension. In our experiments we achieve accurate reconstructions of high-dimensional data points with at most a dozen experts.","Dynamic Partition Models. The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_71,https://openreview.net/forum?id=BJ46w6Ule,Dynamic Partition Models,"We present a new approach for learning compact and intuitive distributed representations with binary encoding. Rather than summing up expert votes as in products of experts, we employ for each variable the opinion of the most reliable expert. Data points are hence explained through a partitioning of the variables into expert supports. The partitions are dynamically adapted based on which experts are active. During the learning phase we adopt a smoothed version of this model that uses separate mixtures for each data dimension. In our experiments we achieve accurate reconstructions of high-dimensional data points with at most a dozen experts.","Dynamic Partition Models. The goal of this paper is to learn “ a collection of experts that are individually--------meaningful and that have disjoint responsibilities.” Unlike a standard mixture model, they “use a different mixture for each dimension d.” While the results seem promising, the paper exposition needs significant improvement.----------------Comments:----------------The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.----------------The subsequent exposition is not very clear. There are assertions made with no justification, e.g. “the experts only have a small variance for some subset of the variables while the variance of the other variables is large.” ----------------Since you’re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.----------------The horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_72,https://openreview.net/forum?id=BJ46w6Ule,Dynamic Partition Models,"We present a new approach for learning compact and intuitive distributed representations with binary encoding. Rather than summing up expert votes as in products of experts, we employ for each variable the opinion of the most reliable expert. Data points are hence explained through a partitioning of the variables into expert supports. The partitions are dynamically adapted based on which experts are active. During the learning phase we adopt a smoothed version of this model that uses separate mixtures for each data dimension. In our experiments we achieve accurate reconstructions of high-dimensional data points with at most a dozen experts.","Dynamic Partition Models. This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed “EM-like” algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?--------We also note that the “product of unifac models” from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf--------I tried to derive the update rule on top of page 4 from the “conditional objective for p(x|h)” in sec. 3.2 But I am getting something different (apart form the extra smoothing factors eps and mu_o). Does this follow? (If we define R=R_nk, mu-mu_k and X=X_n, I get mu = (XR)*inv(R^TR) as the optimal solution, which then needs to be projected back onto the probability simplex).--------The experiments are only illustrative. They don’t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images. I have no idea whether what we see is impressive or not. ",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_82,https://openreview.net/forum?id=ByOK0rwlx,Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network,"This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware. In our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.","Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network. This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_83,https://openreview.net/forum?id=ByOK0rwlx,Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network,"This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware. In our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.","Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network. I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_84,https://openreview.net/forum?id=ByOK0rwlx,Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network,"This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware. In our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.","Ternary Weight Decomposition And Binary Activation Encoding For Fast And Compact Neural Network. This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.----------------My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.----------------[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, https://arxiv.org/abs/1510.00149--------[2] Compressing Deep Convolutional Networks using Vector Quantization, https://arxiv.org/abs/1412.6115--------[3] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, https://arxiv.org/abs/1603.05279",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_91,https://openreview.net/forum?id=HyAbMKwxe,Tighter Bounds Lead To Improved Classifiers,"The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.","Tighter Bounds Lead To Improved Classifiers. The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).----------------[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.--------[2] Direct Loss Minimization for Structured Prediction, McAllester et al. --------[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.----------------Final comment:--------I believe the material presented in this paper is of interest to a wide audience at ICLR.--------The problem studied is interesting and the proposed approach is sound. --------I recommend to accept the paper and increase my score (from 7 to 8). ","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_92,https://openreview.net/forum?id=HyAbMKwxe,Tighter Bounds Lead To Improved Classifiers,"The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.","Tighter Bounds Lead To Improved Classifiers. The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an ""uncertain"" label. ----------------While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): --------(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and --------(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. ----------------The reason why I am confused is that ""The standard approach to supervised classification"", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.----------------=== question:--------In the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest ""Assuming the class is chosen according to p(y|X, θ)""), or the more standard deterministic classifier argmax_y P(y|x, theta) ?----------------As far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). ----------------=== comments:--------- The section ""allowing uncertainty in the decision"" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) ""Classification with a Reject Option using a Hinge Loss"" or Sayedi et al. (2010) ""Trading off Mistakes and Don’t Know Predictions"".----------------- there seems to be a ""-"" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.----------------- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.----------------Final comments:--------I think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the ""smoothed"" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional ""uncertain"" label. I increase my score from 5 to 6.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_93,https://openreview.net/forum?id=HyAbMKwxe,Tighter Bounds Lead To Improved Classifiers,"The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.","Tighter Bounds Lead To Improved Classifiers. The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_94,https://openreview.net/forum?id=SJGPL9Dex,Understanding Trainable Sparse Coding With Matrix Factorization,"Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.  In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the  ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.","Understanding Trainable Sparse Coding With Matrix Factorization. This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.----------------It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. ----------------Minor comments:----------------- E(z_k) in (3) and (4) are not defined.----------------- E_x in (19) is not defined.----------------- Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_95,https://openreview.net/forum?id=SJGPL9Dex,Understanding Trainable Sparse Coding With Matrix Factorization,"Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.  In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the  ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.","Understanding Trainable Sparse Coding With Matrix Factorization. This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. ----------------The analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.----------------The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?----------------Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding' - basically, it is not immediate to understand what 'neural sparse coding' means...","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_96,https://openreview.net/forum?id=SJGPL9Dex,Understanding Trainable Sparse Coding With Matrix Factorization,"Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in \cite{Gregor10}, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately.  In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the  ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.","Understanding Trainable Sparse Coding With Matrix Factorization. This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).----------------FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.----------------Overall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. ----------------Minor comments/typos:--------- p. 6: ""memory taps"" -> tapes?--------- sec 3.2: ""a gap appears has the number of iterations increases"" -> as?--------- sec. 4: ""numerical experiments of 3"" -> of sec 3",5: Marginally below acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_97,https://openreview.net/forum?id=SJDaqqveg,An Actor-critic Algorithm For Sequence Prediction,"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ","An Actor-critic Algorithm For Sequence Prediction. This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. --------In particular, experiments are shown in a synthetic denoising task as well as in machine translation. ----------------I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?--------If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.----------------The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. ----------------While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update \phi' with interpolation, penalize the variance, reducing the value of rare actions, etc. --------Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. --------It seems that the authors need more work to make the model work without so many heuristics.----------------The authors also mentioned several optimization difficulties (some of which are non-intuitive), --------1) why does the critic assign very high value to actions with very low probability according to the actor?--------2) why is a lower square error on Q resulting in much worst performance?----------------The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework.--------Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. ----------------The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this?----------------The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. ----------------I'll revise my score if the authors address my questions.----------------In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_98,https://openreview.net/forum?id=SJDaqqveg,An Actor-critic Algorithm For Sequence Prediction,"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ","An Actor-critic Algorithm For Sequence Prediction. This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.----------------The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic, the critic-based models still outperform other baselines.  It would be nice to include more discussion of the bias-variance tradeoff and future advantages of Actor-Critic (from the pre-review question response) in the paper.  The paper is solid and deserves acceptance","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_99,https://openreview.net/forum?id=SJDaqqveg,An Actor-critic Algorithm For Sequence Prediction,"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ","An Actor-critic Algorithm For Sequence Prediction. The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful. ----------------The pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models. One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity. Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field. ","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_100,https://openreview.net/forum?id=r1y1aawlg,Iterative Refinement For Machine Translation,"Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.","Iterative Refinement For Machine Translation. This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.----------------This paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that ""the probability of correctly labelling a word as a mistake remains low (62%)"" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. ----------------Although I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.----------------Minor comments:----------------I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?----------------In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?------------------------------------------------------------------------ ",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_101,https://openreview.net/forum?id=r1y1aawlg,Iterative Refinement For Machine Translation,"Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.","Iterative Refinement For Machine Translation. This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.----------------My second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.----------------Overall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)----------------Related work:--------I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.----------------“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear. Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that. “distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.----------------Section 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.----------------The use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).----------------Finally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).----------------The relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.----------------The section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_102,https://openreview.net/forum?id=r1y1aawlg,Iterative Refinement For Machine Translation,"Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.","Iterative Refinement For Machine Translation. Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon only one type of machine translation. --------Which immediately prompts these questions:--------- why did they choose that approach to improve on?--------- what is the part of the improvement that comes from the choice of the--------  initial draft (maybe it was a very bad draft)? ----------------Here are some minor typos:--------- p.2: ... a lookup table that replace*S* each word... ?--------- p.3: I might be mistanken but it seems to me that j is used for two--------  different things. It is confusing.--------- p.3: ...takes as input these representation*S* and outputs... ?","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_103,https://openreview.net/forum?id=r1y1aawlg,Iterative Refinement For Machine Translation,"Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.","Iterative Refinement For Machine Translation. This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? --------- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.--------- How does the approach compare to a model that simply re-ranks the k-best output?--------- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.----------------Minor comments:--------- Iteratively improving a generated text was also explored in https://arxiv.org/pdf/1510.09202v1.pdf from a reinforcement learning angle.--------- I don't understand footnote 1.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_110,https://openreview.net/forum?id=SypU81Ole,Sampling Generative Networks,We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation.  Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.,"Sampling Generative Networks. This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons / quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_111,https://openreview.net/forum?id=SypU81Ole,Sampling Generative Networks,We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation.  Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.,"Sampling Generative Networks. This paper proposed a set of different things under the name of ""sampling generative models"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.----------------Aside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.----------------Overall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_112,https://openreview.net/forum?id=SypU81Ole,Sampling Generative Networks,We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation.  Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.,"Sampling Generative Networks. In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.----------------I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I’ve seen in many other ICLR submissions. But it does more than just describing useful “tricks”. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_116,https://openreview.net/forum?id=SJNDWNOlg,What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?,"Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them.  The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify  the best choices for different factors and propose a new multi-scale image feature representation method to  encode the image effectively. Finally, we show that the proposed method generalises well and outperforms  the state-of-the-art methods on four typical datasets used for visual instance retrieval.","What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?. The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.----------------Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.----------------While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval""). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_117,https://openreview.net/forum?id=SJNDWNOlg,What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?,"Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them.  The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify  the best choices for different factors and propose a new multi-scale image feature representation method to  encode the image effectively. Finally, we show that the proposed method generalises well and outperforms  the state-of-the-art methods on four typical datasets used for visual instance retrieval.","What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?. Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:----------------I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used ""as large as possible"" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).----------------The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. ----------------Furthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be ""What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?"" - I don't think this is sufficiently novel nor interesting for the community.",3: Clear rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_118,https://openreview.net/forum?id=SJNDWNOlg,What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?,"Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them.  The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify  the best choices for different factors and propose a new multi-scale image feature representation method to  encode the image effectively. Finally, we show that the proposed method generalises well and outperforms  the state-of-the-art methods on four typical datasets used for visual instance retrieval.","What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?. This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as ""traditional wisdom"".----------------Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. ----------------While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it ""ignores"" 2 major recent works that are in direct contradictions with many claims of the paper ([a] ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by  Gordo et al. and [b] ""CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples"" by Radenović et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.----------------Here are some of the misleading claims: ----------------  - ""Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.""--------  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).--------  --------  - ""the proposed method [...] outperforms the state-of-the-art methods on four typical datasets""--------  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].--------  --------  - ""Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option."".--------  This is a questionable opinion. The method exposed in ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.----------------  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.----------------In addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). ----------------To conclude, the paper is one year too late with respect to recent developments in the state of the art.",3: Clear rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_119,https://openreview.net/forum?id=r1osyr_xg,Fuzzy Paraphrases In Learning Word Representations With A Lexicon,"A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally.  In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.","Fuzzy Paraphrases In Learning Word Representations With A Lexicon. This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.--------2) The tables need better and more descriptive labels.--------3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?--------4) Why was ""Enriched CBOW"" not included in the analogy task?--------5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,The reviewers agree that the paper's clarity and experimental evaluation can be improved.,3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_120,https://openreview.net/forum?id=r1osyr_xg,Fuzzy Paraphrases In Learning Word Representations With A Lexicon,"A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally.  In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.","Fuzzy Paraphrases In Learning Word Representations With A Lexicon. This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,The reviewers agree that the paper's clarity and experimental evaluation can be improved.,3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_121,https://openreview.net/forum?id=r1osyr_xg,Fuzzy Paraphrases In Learning Word Representations With A Lexicon,"A synonym of a polysemous word is usually only the paraphrase of one sense among many. When lexicons are used to improve vector-space word representations, such paraphrases are unreliable and bring noise to the vector-space. The prior works use a coefficient to adjust the overall learning of the lexicons. They regard the paraphrases equally.  In this paper, we propose a novel approach that regards the paraphrases diversely to alleviate the adverse effects of polysemy. We annotate each paraphrase with a degree of reliability. The paraphrases are randomly eliminated according to the degrees when our model learns word representations. In this way, our approach drops the unreliable paraphrases, keeping more reliable paraphrases at the same time. The experimental results show that the proposed method improves the word vectors. Our approach is an attempt to address the polysemy problem keeping one vector per word. It makes the approach easier to use than the conventional methods that estimate multiple vectors for a word. Our approach also outperforms the prior works in the experiments.","Fuzzy Paraphrases In Learning Word Representations With A Lexicon. This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.----------------Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. ----------------Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,The reviewers agree that the paper's clarity and experimental evaluation can be improved.,3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_125,https://openreview.net/forum?id=HyEeMu_xx,Progressive Attention Networks For Visual Attribute Prediction,We propose a novel attention model which can accurately attend to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or suppress features at certain spatial locations for use in the next layer. We further employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in visual attribute prediction tasks.,"Progressive Attention Networks For Visual Attribute Prediction. This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results.----------------Pros:--------- The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]--------- Good visualisations.----------------Cons:--------- No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.--------- Not clear how the query is fed into the localisation networks of baselines.--------- The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets.--------- No real novelty.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_126,https://openreview.net/forum?id=HyEeMu_xx,Progressive Attention Networks For Visual Attribute Prediction,We propose a novel attention model which can accurately attend to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or suppress features at certain spatial locations for use in the next layer. We further employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in visual attribute prediction tasks.,"Progressive Attention Networks For Visual Attribute Prediction. This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.----------------Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  ----------------An important missing citation is Graves 2013 which had an early version of the attention model. ----------------Minor typo:--------""It confins possible attributes.."" -> It confines..--------""ImageNet (Deng et al., 2009), is used, and three additional"" -> "".., are used,""",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_127,https://openreview.net/forum?id=HyEeMu_xx,Progressive Attention Networks For Visual Attribute Prediction,We propose a novel attention model which can accurately attend to target objects of various scales and shapes in images. The model is trained to gradually suppress irrelevant regions in an input image via a progressive attentive process over multiple layers of a convolutional neural network. The attentive process in each layer determines whether to pass or suppress features at certain spatial locations for use in the next layer. We further employ local contexts to estimate attention probability at each location since it is difficult to infer accurate attention by observing a feature vector from a single location only. The experiments on synthetic and real datasets show that the proposed attention network outperforms traditional attention methods in visual attribute prediction tasks.,"Progressive Attention Networks For Visual Attribute Prediction. The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.----------------1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.--------2. Qualitative results: --------2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the “number” dataset and more on the Visual Genome dataset.--------2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.--------3. Qualitative results:--------3.1. The paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches.--------3.2. For the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach.--------3.3. It would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied.--------4. It would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable.------------------------Other (minor/discussion points)--------- Something seems wrong in the last two columns in Figure 11: the query “7” is blue not green. Either the query or the answer seem wrong.--------- Section 3: “In each layer, the each attended feature map” -> “In each layer, each attended feature map”--------- I think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on.------------------------Summary:--------While the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_134,https://openreview.net/forum?id=ByQPVFull,Training Group Orthogonal Neural Networks With Privileged Information,"Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.","Training Group Orthogonal Neural Networks With Privileged Information. This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional “background suppression” term.------------------------Pros:----------------Proposes a “group-wise model diversity” loss term which is novel, to my knowledge.----------------The use of foreground segmentation masks to improve image classification is also novel.----------------The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.------------------------Cons:----------------The evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.----------------It would be nice to see the results with “Incomplete Privileged Information” on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it’s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.----------------The presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled “A Unified Architecture: GoCNN”, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).----------------Minor: calling eq 3 a “regression loss” and writing “||0 - x||” rather than just “||x||” is not necessary and makes understanding more difficult -- I’ve never seen a norm regularization term written this way or described as a “regression to 0”.----------------Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).------------------------An additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?----------------The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.----------------===============----------------Edit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.----------------(I'll reiterate a very minor point about Figure 1 though: I still think the ""0"" and ""1"" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text ""suppress foreground"", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_135,https://openreview.net/forum?id=ByQPVFull,Training Group Orthogonal Neural Networks With Privileged Information,"Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.","Training Group Orthogonal Neural Networks With Privileged Information. This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.----------------Pros:--------- The paper is clear and easy to follow--------- The experimental results seem to show some benefit from the proposed approach----------------Cons:--------(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation--------(2) No comparison with an ensemble--------(3) Full experiments on ImageNet under the ""partial privileged information"" setting would be more impactful----------------This paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.----------------First, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.----------------Second, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.----------------Minor suggestions / comments:--------- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)--------- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_136,https://openreview.net/forum?id=ByQPVFull,Training Group Orthogonal Neural Networks With Privileged Information,"Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN  model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models.","Training Group Orthogonal Neural Networks With Privileged Information. The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives ""complementary viewpoints"" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. ----------------For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a ""foreground"" and a ""background"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. ----------------They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a ""vanilla"" baseline that does not use these losses.----------------I enjoyed reading the paper because the idea is simple, smart, and seems to be effective. --------But there are a few concerns;---------firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. https://arxiv.org/abs/1412.1283--------To be fair, this is not truly the same thing as what the authors are doing, because in the reference above the masking is computed  during both training and testing, while here it is used as a method of decorrelating neurons at training time.--------But I understand that to the broader iclr community this may seem as ""yet another vision-specific trick"", while to the vision community one would ask why not just use the mask during both training and testing, since one can compute it in the first place. ----------------More importantly, the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet for testing. They do get a substantial boost, but it is not clear if this will transfer to more data/layers. ----------------The authors could at least have also tried CIFAR-10/100. I would expect to see some more results during the rebuttal period. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_154,https://openreview.net/forum?id=rkKCdAdgx,Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters,"The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.","Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters. Description:----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"".----------------The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.----------------Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.----------------Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.----------------The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach.------------------------Evaluation:----------------It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test.----------------The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step.----------------In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?)----------------Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable.------------------------Minor points:----------------As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned.----------------The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_155,https://openreview.net/forum?id=rkKCdAdgx,Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters,"The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.","Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters. The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) ""Hash Kernels for Structured Data"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) ""Robust Bloom Filters for Large MultiLabel Classification Tasks"". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.----------------The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,----------------1) the model compression approach of https://arxiv.org/abs/1510.00149--------2) training with integer/binary weights https://arxiv.org/abs/1511.00363----------------Overall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_156,https://openreview.net/forum?id=rkKCdAdgx,Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters,"The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.","Compact Embedding Of Binary-coded Inputs And Outputs Using Bloom Filters. The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.----------------Pros:----------------- Can be applied to practically any model, either at the input or hte output.--------- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. ----------------Cons:----------------- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.--------- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.----------------This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_166,https://openreview.net/forum?id=Sk36NgFeg,Filling In The Details: Perceiving From Low Fidelity Visual Input,"Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g., dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our goal is to initiate investigations that will be fruitful both for engineering better networks and also for eventually testing hypotheses on the neural mechanisms responsible for our own visual system's ability to perceive missing information. We find that networks can compensate for low acuity input by learning global feature functions that allow the network to fill in some of the missing details. For example, the networks accurately perceive shape and color in the periphery, even when 75\% of the input is achromatic and low resolution. On the other hand, the network is prone to similar mistakes as humans; for example, when presented with a fully grayscale landscape image, it perceives the sky as blue when the sky is actually a red sunset. ","Filling In The Details: Perceiving From Low Fidelity Visual Input. This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.----------------Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, ""This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need...."".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?----------------Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss?  A prime example is texture, which the autoencoder fails to recover.  But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region.  So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures.  A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense.----------------Similarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene:  If there is an ambiguity, the network won't know which to ""choose"" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones.  However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.----------------There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction.  But overall it's hard to know what to take away from this paper.  What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about?  Can these be more thoroughly explored with more focus?",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_167,https://openreview.net/forum?id=Sk36NgFeg,Filling In The Details: Perceiving From Low Fidelity Visual Input,"Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g., dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our goal is to initiate investigations that will be fruitful both for engineering better networks and also for eventually testing hypotheses on the neural mechanisms responsible for our own visual system's ability to perceive missing information. We find that networks can compensate for low acuity input by learning global feature functions that allow the network to fill in some of the missing details. For example, the networks accurately perceive shape and color in the periphery, even when 75\% of the input is achromatic and low resolution. On the other hand, the network is prone to similar mistakes as humans; for example, when presented with a fully grayscale landscape image, it perceives the sky as blue when the sky is actually a red sunset. ","Filling In The Details: Perceiving From Low Fidelity Visual Input. I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:----------------- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.--------- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine this making a difference.--------- On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea. While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed. To answer the actual question ""how much additional information about the periphery can be extracted from the fovea"", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for. Then any decreased error is only due to the additional fovea information.----------------Other issues:--------- The images in Figure 2 (a) and (b) in the rows ""factor 2"", ""factor 4"", ""factor 8"" look very blurry. There seems some interpolation to be going on (although slighly different than the bilinear interpolation). This makes it hard to asses how much information is in these images. I think it would be much more insightfull to print them with ""nearest"" interpolation.--------- Figure 3 caption too vague. Maybe add something like footnote 2?--------- Often figures appear too early in paper which leads to lots of distance between text and figures.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_168,https://openreview.net/forum?id=Sk36NgFeg,Filling In The Details: Perceiving From Low Fidelity Visual Input,"Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g., dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our goal is to initiate investigations that will be fruitful both for engineering better networks and also for eventually testing hypotheses on the neural mechanisms responsible for our own visual system's ability to perceive missing information. We find that networks can compensate for low acuity input by learning global feature functions that allow the network to fill in some of the missing details. For example, the networks accurately perceive shape and color in the periphery, even when 75\% of the input is achromatic and low resolution. On the other hand, the network is prone to similar mistakes as humans; for example, when presented with a fully grayscale landscape image, it perceives the sky as blue when the sky is actually a red sunset. ","Filling In The Details: Perceiving From Low Fidelity Visual Input. This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https://arxiv.org/abs/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks? I think it would be much better if the authors can provide a more general conclusion.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_169,https://openreview.net/forum?id=BJ3filKll,Efficient Representation Of Low-dimensional Manifolds Using Deep Networks,"We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.","Efficient Representation Of Low-dimensional Manifolds Using Deep Networks. The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very ""compatible"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?----------------(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.----------------(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the ""bound for this case is very loose"". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.----------------I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.------------------------Minor comments: ----------------- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009).--------- What loss do the authors use in their experiments? Using ""the difference between the ground truth distance ... and the distance computed by the network"" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_170,https://openreview.net/forum?id=BJ3filKll,Efficient Representation Of Low-dimensional Manifolds Using Deep Networks,"We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.","Efficient Representation Of Low-dimensional Manifolds Using Deep Networks. Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.----------------However, the current version of the paper could use some more work:----------------The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.----------------It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression----------------The theory sections could do with being more clearly written -- I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_171,https://openreview.net/forum?id=BJ3filKll,Efficient Representation Of Low-dimensional Manifolds Using Deep Networks,"We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.","Efficient Representation Of Low-dimensional Manifolds Using Deep Networks. SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. --------- On page 5, mention how the orthogonal projection on S_k is realized in the network. --------- On page 6 ``divided into segments'' here `segments' is maybe not the best word. --------- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? ","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_172,https://openreview.net/forum?id=H1oyRlYgg,On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say -- data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.","On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima. Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),1.0
2017_173,https://openreview.net/forum?id=H1oyRlYgg,On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say -- data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.","On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima. The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.","10: Top 5% of accepted papers, seminal paper",3: The reviewer is fairly confident that the evaluation is correct,"The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),0.0
2017_174,https://openreview.net/forum?id=H1oyRlYgg,On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data,  say -- data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a  degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We  discuss several  strategies to attempt to help large-batch methods eliminate this generalization gap.","On Large-batch Training For Deep Learning: Generalization Gap And Sharp Minima. I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),1.0
2017_184,https://openreview.net/forum?id=rJRhzzKxl,Knowledge Adaptation: Teaching To Adapt,"Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics.  To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains.  When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.","Knowledge Adaptation: Teaching To Adapt. This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_185,https://openreview.net/forum?id=rJRhzzKxl,Knowledge Adaptation: Teaching To Adapt,"Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics.  To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains.  When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.","Knowledge Adaptation: Teaching To Adapt. The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. ----------------The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method. --------Questions: --------1. Why did you choose to use different combination schemes in equation (3) and (5)? For example, in equation (5), what if minimizing H( (1-\lambda) y_teacher + \lambda P_t, P_s) instead? --------2. how will you extend the MCD definition to multi-class settings?  ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_186,https://openreview.net/forum?id=rJRhzzKxl,Knowledge Adaptation: Teaching To Adapt,"Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics.  To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains.  When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher's accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.","Knowledge Adaptation: Teaching To Adapt. Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.----------------What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better?----------------One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope.-------- --------Authors shall refer in the related-work to similar ideas proposed in the related field of acoustic modelling (and adaptation of acoustic models), in particular, works of Yu et al. [1] and the follow up work of Li et al. [2] which to an extent are a prior the work on knowledge distillation. Reasonably related is also work on deep relationship networks [3], where MT generative approach is proposed to avoid negative transfers, something of central role in this paper.----------------Minors: --------The student S similarly has an output probability -> models an output probability----------------[1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vocabulary Speech Recognition, Dong Yu, Kaisheng Yao, Hang Su, Gang Li, Frank Seide--------[2] Learning Small-Size DNN with Output-Distribution-Based Criteria, Jinyu Li, Rui Zhao, Jui-Ting Huang and Yifan Gong--------[3] Learning Multiple Tasks with Deep Relationship Networks, Mingsheng Long, Jianmin Wang",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_205,https://openreview.net/forum?id=r1kQkVFgl,Learning Python Code Suggestion With A Sparse Pointer Network,"To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.","Learning Python Code Suggestion With A Sparse Pointer Network. This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. --------2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.----------------Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.----------------While the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_206,https://openreview.net/forum?id=r1kQkVFgl,Learning Python Code Suggestion With A Sparse Pointer Network,"To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.","Learning Python Code Suggestion With A Sparse Pointer Network. This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and/or pointers truly show an advantage in capturing long term dependencies.----------------The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.----------------The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.----------------Overall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_207,https://openreview.net/forum?id=r1kQkVFgl,Learning Python Code Suggestion With A Sparse Pointer Network,"To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.","Learning Python Code Suggestion With A Sparse Pointer Network. This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_208,https://openreview.net/forum?id=Bk67W4Yxl,Improved Architectures For Computer Go,AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.,"Improved Architectures For Computer Go. The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. ----------------The paper’s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper).----------------It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction.----------------Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.----------------The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.----------------Results: ----------------The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication. ",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_209,https://openreview.net/forum?id=Bk67W4Yxl,Improved Architectures For Computer Go,AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.,"Improved Architectures For Computer Go. This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.--------Overall a good paper.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_210,https://openreview.net/forum?id=Bk67W4Yxl,Improved Architectures For Computer Go,AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.,"Improved Architectures For Computer Go. The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_211,https://openreview.net/forum?id=Bk67W4Yxl,Improved Architectures For Computer Go,AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.,"Improved Architectures For Computer Go. The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.--------4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? ----------------Please also see my original questions.",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_218,https://openreview.net/forum?id=SJx7Jrtgl,Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders,"We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.","Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders. This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).----------------A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \eta (eq. (3)) and \alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.----------------I really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.------------------------Overall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_219,https://openreview.net/forum?id=SJx7Jrtgl,Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders,"We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.","Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders. The authors posit a mixture of Gaussian prior for variational--------auto-encoders. They also consider a regularization term motivated--------from information theory.----------------The modeling extension is simple and the inference follows--------mechanically from what's already standard in the literature. Instead--------of using discrete latent variable samples they collapse the expected--------KL; this works for few mixture components and has been considered--------before in more general contexts, e.g., Titsias and Lazaro-Gredilla--------(2015). It will not scale to many mixture components.----------------I find the discussion in Section 3.2.2 difficult to parse and, if I--------understood it correctly, not necessarily correct. Many arguments are--------introduced and few fleshed out. First, there is a claim that a--------multinomial prior with equal class probabilities assigns the same--------number of data points to each class on average; this is true a priori--------but certainly not true given data. Second, they claim the KL--------regularizer forces the approximate posterior to be close to this--------uniform; this is only true for small data, certainly the energy term--------in the ELBO (expected log-likelihood) will overpower the regularizer;--------is this not the case in a mean-field approximation to a mixture of--------Gaussians model? Third, there is a claim that ""under the mean-field--------approximation, this constraint is enforced on each sample""; how does--------the mean-field approximation enforce a constraint on the effect of--------Monte Carlo sampling? Fourth, they argue Johnson et al. (2016) can--------overcome this issue partly due to SVI; how does data subsampling--------affect this behavior? Fifth, they derive the exact posterior in--------Equation 6; so to what extent are these arguments relevant?----------------The experiments are limited on toy data and only a few mixture--------components are considered (not enough where the collapsed approach--------will not scale).----------------+ Titsias, M. K., & Lázaro-Gredilla, M. (2015). Local Expectation Gradients for Black Box Variational Inference. In Neural Information Processing Systems.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_220,https://openreview.net/forum?id=SJx7Jrtgl,Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders,"We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.","Deep Unsupervised Clustering With Gaussian Mixture Variational Autoencoders. The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the ""Best Run"". Was the best run maximizing the validation performance or the test performance ? Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that ""we have advanced the state of the art in deep unsupervised clustering both in theory and practice"".--------The consistency violation regularization might be interesting. But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models. So making an experiment on this simpler model on a synthetic dataset should also be done.--------In general, I would recommend running more experiments as to solidify your claims.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_230,https://openreview.net/forum?id=BymIbLKgl,Learning Invariant Representations Of Planar Curves,"We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.","Learning Invariant Representations Of Planar Curves. I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_231,https://openreview.net/forum?id=BymIbLKgl,Learning Invariant Representations Of Planar Curves,"We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.","Learning Invariant Representations Of Planar Curves. Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).----------------In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.",5: Marginally below acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_232,https://openreview.net/forum?id=BymIbLKgl,Learning Invariant Representations Of Planar Curves,"We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.","Learning Invariant Representations Of Planar Curves. Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_236,https://openreview.net/forum?id=SkgSXUKxx,An Analysis Of Feature Regularization For Low-shot Learning,"Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.","An Analysis Of Feature Regularization For Low-shot Learning. Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a soft whitening effect, the feature regularizer--------is like a soft version of Batch Normalization.----------------Experiments show small improvements on a synthetic XOR test set.--------On the Omniglot dataset feature regularization is better than most baselines,--------but is worse than Moment Matching Networks. An experiment on ImageNet similar--------to Hariharan and Girshick 2016 also shows effective low-shot learning.------------------------Strengths--------===----------------* The core proposal is a simple modification of Hariharan and Girshick 2016.----------------* The idea of feature regularization is analyzed from multiple angles--------both theoretically and empirically.----------------* The connection with Batch Normalization could have broader impact.------------------------Weaknesses--------===----------------* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.--------While introducing the concept, some concern is expressed about the motivation:--------""And it is not very clear why small gradients on every sample produces--------good generalization experimentally."" This seems to be the central issue to me.--------The paper details some related analysis, it does not offer a clear answer to--------this problem.------------------------* The purpose and generality of section 2.1 is not clear.----------------The analysis provides a specific case (XOR with a non-standard architecture)--------where feature regularization intuitively helps learn a better representation.--------However, the intended take-away is not clear.----------------The take-away may be that since a feature penalty helps in this case it--------should help in other cases. I am hesitant to buy that argument because of the--------specific architecture used in this section. The result seems to rely on the--------choice of an x^2 non-linearity, which is not often encountered in recent neural--------net literature.----------------The point might also be to highlight the difference between a weight--------penalty and a feature penalty because the two seem to encourage--------different values of b in this case. However, there is no comparison to--------a weight penalty on b in section 2.1.------------------------* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy--------loss. A more general class of losses for which eq. 3 holds is not provided. This--------should be made clear before eq. 3 is presented.------------------------* The Omniglot and ImageNet experiments are performed with Batch Normalization,--------yet the paper points out that feature regularization may be similar in effect--------to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are--------clear improvements over that baseline, the proposed regularizer has a clear--------additional positive effect. However, results should be provided without--------Batch Norm so a 1-1 comparison between the two methods can be performed.------------------------* The ImageNet experiment should be more like Hariharan and Girshick.--------In particular, the same split of classes should be used (provided in--------the appendix) and performance should be measured using n > 1 novel examples--------per class (using k nearest neighbors).------------------------Minor:----------------* A brief comparison to Matching Networks is provided in section 3.2, but the--------performance of Matching Networks should also be reported in Table 1.----------------* From the approach section: ""Intuitively when close to convergence, about half--------of the data-cases recommend to update a parameter to go left, while--------the other half recommend to go right.""----------------Could the intuition be clarified? There are many directions in high--------dimensional space and many ways to divide them into two groups.----------------* Is the SGM penalty of Hariharan and Girshick implemented for this paper--------or using their code? Either is acceptable, but clarification would be appreciated.----------------* Should the first equal sign in eq. 13 be proportional to, not equal to?----------------* The work is dense in nature, but I think the presentation could be improved.--------In particular, more detailed derivations could be provided in an appendix--------and some details could be removed from the main version in order to increase--------focus on the results (e.g., the derviation in section 2.2.1).------------------------Overall Evaluation--------===----------------This paper provides an interesting set of analyses, but their value is not clear.--------There is no clear reason why a gradient or feature regularizer should improve--------low-shot learning performance. Despite that, experiments support that conclusion,--------the analysis is interesting by itself, and the analysis may help lead to a--------clearer explanation.----------------The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.--------Some points are not completely clear, as mentioned above.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_237,https://openreview.net/forum?id=SkgSXUKxx,An Analysis Of Feature Regularization For Low-shot Learning,"Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.","An Analysis Of Feature Regularization For Low-shot Learning. This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about ""results can be derived for φ(x) with convex differentiable non-linear activation functions such as ReLU"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.----------------Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.----------------I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.----------------notes:--------- ""an expectation taken with respect to the empirical distribution generated by the training set"", generally the training set is viewed as a ""montecarlo"" sample of the underlying, unknown data distribution \mathcal{D}.--------- ""we can see that our model learns meaningful representations"", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.--------- ""Table 13.2"" should be ""Table 2"".--------- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_238,https://openreview.net/forum?id=SkgSXUKxx,An Analysis Of Feature Regularization For Low-shot Learning,"Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.","An Analysis Of Feature Regularization For Low-shot Learning. The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on ""low-shot"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?------------------ edits after revised version:----------------Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:--------- on Omniglot, the paper is still significantly far from the current state of the art.--------- the new experiments do not really confirm/infirm the relationship with BN.--------- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.--------I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_265,https://openreview.net/forum?id=Sy1rwtKxg,Parallel Stochastic Gradient Descent With Sound Combiners,"Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm — at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD’s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13× speedup over our heavily optimized sequential baseline on 16 cores.","Parallel Stochastic Gradient Descent With Sound Combiners. This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.----------------I feel that there might be some fundamental misunderstanding on SGD.----------------''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have--------thousands if not millions of features.""----------------I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. ----------------Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.--------I suggest authors to make the following changes to make this paper more clear and theoretically solid--------- provide computational complexity per step of the proposed algorithm--------- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_266,https://openreview.net/forum?id=Sy1rwtKxg,Parallel Stochastic Gradient Descent With Sound Combiners,"Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm — at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD’s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13× speedup over our heavily optimized sequential baseline on 16 cores.","Parallel Stochastic Gradient Descent With Sound Combiners. Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:----------------- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. ----------------- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). ----------------- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing. ",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_267,https://openreview.net/forum?id=Sy1rwtKxg,Parallel Stochastic Gradient Descent With Sound Combiners,"Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm — at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD’s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13× speedup over our heavily optimized sequential baseline on 16 cores.","Parallel Stochastic Gradient Descent With Sound Combiners. This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.--------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.--------2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.--------3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.----------------Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_268,https://openreview.net/forum?id=Sywh5KYex,Learning Identity Mappings With Residual Gates,"We propose a layer augmentation technique that adds shortcut connections with a linear gating mechanism, and can be applied to almost any network model. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Highway Neural Networks and Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. In our experiments, augmented plain networks -- which can be interpreted as simplified Highway Neural Networks -- outperform ResNets, raising new questions on how shortcut connections should be designed. We also evaluate our model on CIFAR-10 and CIFAR-100 using augmented Wide ResNets, achieving 3.65% and 18.27% test error, respectively.","Learning Identity Mappings With Residual Gates. This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.----------------It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.----------------The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_269,https://openreview.net/forum?id=Sywh5KYex,Learning Identity Mappings With Residual Gates,"We propose a layer augmentation technique that adds shortcut connections with a linear gating mechanism, and can be applied to almost any network model. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Highway Neural Networks and Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. In our experiments, augmented plain networks -- which can be interpreted as simplified Highway Neural Networks -- outperform ResNets, raising new questions on how shortcut connections should be designed. We also evaluate our model on CIFAR-10 and CIFAR-100 using augmented Wide ResNets, achieving 3.65% and 18.27% test error, respectively.","Learning Identity Mappings With Residual Gates. This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.----------------The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily ""shutting off"" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.----------------Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?----------------For the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.----------------For CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.----------------Some questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_270,https://openreview.net/forum?id=Sywh5KYex,Learning Identity Mappings With Residual Gates,"We propose a layer augmentation technique that adds shortcut connections with a linear gating mechanism, and can be applied to almost any network model. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Highway Neural Networks and Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. In our experiments, augmented plain networks -- which can be interpreted as simplified Highway Neural Networks -- outperform ResNets, raising new questions on how shortcut connections should be designed. We also evaluate our model on CIFAR-10 and CIFAR-100 using augmented Wide ResNets, achieving 3.65% and 18.27% test error, respectively.","Learning Identity Mappings With Residual Gates. The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_274,https://openreview.net/forum?id=B1TTpYKgx,On The Expressive Power Of Deep Neural Networks,"We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. They suggest that parameters earlier in a network have greater influence on its expressive power – in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also explore the effect of training on the input-output map, and find that it trades off between the stability and expressivity of the input-output map.","On The Expressive Power Of Deep Neural Networks. This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide""----------------I don’t think so. In ""Deep Belief Networks are Compact Universal Approximators"" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n−1 + 1 layers of n units (with n the number of input neutron).----------------“Comparing architectures in such a fashion limits the generality of the conclusions”----------------To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).----------------It is much harder to generalise the approach you propose, based on random networks which are not used in practice.----------------“[we study] a family of networks arising in practice: the behaviour of networks after random initialisation”----------------These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.----------------“results on random networks provide natural baselines to compare trained networks with”----------------random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).----------------p5----------------“As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.”----------------As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.----------------p6----------------the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial----------------p7----------------in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_275,https://openreview.net/forum?id=B1TTpYKgx,On The Expressive Power Of Deep Neural Networks,"We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. They suggest that parameters earlier in a network have greater influence on its expressive power – in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also explore the effect of training on the input-output map, and find that it trades off between the stability and expressivity of the input-output map.","On The Expressive Power Of Deep Neural Networks. SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. --------Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. --------The connection to previous works could also be clearer. ----------------- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' ----------------This is partially true, but it neglects important parts of the discussion conducted in the cited papers. --------In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. --------That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. --------* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. --------In particular, such statements can be directly interpreted in terms of networks with random weights. ----------------- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. ----------------- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' --------The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. --------This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''----------------OTHER SPECIFIC COMMENTS --------In Theorem 1 --------- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. ----------------- The notation ``g \geq O(f)'' used in the theorem reads literally as |g| \geq \leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\geq 0. --------For expressing asymptotic lower bounds one can use the notation \Omega (see https://en.wikipedia.org/wiki/Big_O_notation). ----------------- It would be helpful to mention that the expectation is being taken with respect to the network weights and that these are normally distributed with variance \sigma. ----------------- Theorem 2. Here it would be good to be more specific about the kind of sign transitions. Is this about transitions at any units of the network, or about sign transitions at the scalar output of the entire network. ----------------- Theorem 3 is quite trivial. --------The bijection between transitions and activation patterns is not clear. --------Take a regular n-gon in the plane and a circle that crosses each edge twice. --------This makes 2n transitions but only n+1 activation patterns. ----------------- Theorem 4. --------Where is the proof of this statement? --------How does this relate to the simple fact that each activation pattern corresponds to the vector indicating the units that are `active'? ------------------------MINOR COMMENTS--------- The names of the theorems (e.g. ``Bound on ...'' in Theorem 1) could be separated more clearly from the statements, for instance using bold font, a dot, or parentheses. --------- On page 4, in Latex one can use \gg for the `much larger' symbol. --------- On page 4, explain the notation \delta z_\orth.  --------- On page 4, explain that ``latent image'' refers to the image in the last layer. --------- Why are there no error bars in Figure 2?  --------- On page 5 explain that the hyperplane is in the last hidden layer. --------- On page 5, ``is transitioning for any input''. This is not clearly stated, since a transition takes place at a point in a trajectory of inputs, not for a single input. --------- The y-axis labels in Figure 1 (c) and (d) are too small. --------- Why are there no error bars in Figure 1 (a) and (b)? The caption could at least mention that shown are the averages over experiments. --------- In Figure 4 (b) the curves are occluded by the labels. --------- The numbering of results is confusing. In the Appendix some numbers are repeated with the main part and some are missing. --------- On page 19. Theorem 6. As far as I remember Stanley also provides an elementary proof of case with hyperplanes in general position. Many other works also provide elementary proofs using the same induction arguments in what is known as the sweep hyperplane method. ",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_276,https://openreview.net/forum?id=B1TTpYKgx,On The Expressive Power Of Deep Neural Networks,"We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. They suggest that parameters earlier in a network have greater influence on its expressive power – in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also explore the effect of training on the input-output map, and find that it trades off between the stability and expressivity of the input-output map.","On The Expressive Power Of Deep Neural Networks. Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the ‘trajectory length’ of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : ‘transitions’,’activation patterns ’ and ‘Dichotomies’. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. ----------------Novelty:----------------Studying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.----------------Significance:----------------The geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.----------------Detailed comments:----------------Theorem 1:----------------- As raised by reviewer one the definition of a one dimensional input trajectory is missing. --------- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. ----------------Theorem 2:----------------- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})? I understand you don't want to overload notation but maybe E_{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T_{d+1} for you is a random variable and t_{d} is fixed. Are you fixing W_d and then looking at W_{d+1} as  random?----------------- In the same proof:  the recursion  is for d>1  ? your analysis is for W \in R^{k\times k}, you don't not study the W \in \mathbb{R}^{k\times m}. In this case you can not assume assume that |z^(0)|=1.----------------- should d=1, be analyzed alone to know how it scales with m?----------------Theorem 4 in main text:----------------- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?----------------Figures 8 and 9:----------------- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks https://arxiv.org/pdf/1601.04920.pdf----------------- How much the plot depends on the shape of the trajectory? have you tried other then circular trajectory?----------------- In these plots the 2 mnist points had same label ? or different label?  both cases should be studied, to see the tradeoff between contraction and selectivity to the class label.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_283,https://openreview.net/forum?id=SygvTcYee,"Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders","Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such ""nested"" functions is the ""method of auxiliary coordinates (MAC)"". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.","Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders. The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.----------------Questions:--------1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.--------2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?--------3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.--------4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?----------------The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.----------------I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_284,https://openreview.net/forum?id=SygvTcYee,"Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders","Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such ""nested"" functions is the ""method of auxiliary coordinates (MAC)"". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.","Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders. UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, ""...ICLR submission focus on the ParMAC algorithm..."", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.----------------Section 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.----------------Section 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by ""submodel"". Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don't understand how are they initialized (identically?), and more importantly I don't understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. --------The fact that I am not able to understand what is actually happening, I see as major issue.----------------I don't like the later paragraphs on extensions, model for speedup, convergence and topologies. I don't understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can't find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don't see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does ""true SGD"", without explaining what is ""true SGD"" seems very strange. Other statements in this section seem also very vague and unjustified/unexplained.----------------Experimental section seems to suggest that the method is interesting for binary autoencoders, but I don't see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.----------------Conclusion contains statements that are too strong or misleading based on what I saw. In particular, ""we analysed its parallel speedup and convergence"" seems ungrounded. Further, the claim ""The convergence properties of MAC remain essentially unaltered in ParMAC"" is unsupported, regardless of the meaning of ""essentially unchanged"".----------------In summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn't be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.",4: Ok but not good enough - rejection,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_285,https://openreview.net/forum?id=SygvTcYee,"Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders","Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such ""nested"" functions is the ""method of auxiliary coordinates (MAC)"". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.","Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders. This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. ----------------Also, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like:----------------1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.--------2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure.--------3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point.----------------So for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)---------------- It would be really helpful to see how this compares in practice. It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_286,https://openreview.net/forum?id=SygvTcYee,"Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders","Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such ""nested"" functions is the ""method of auxiliary coordinates (MAC)"". MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.","Parmac: Distributed Optimisation Of Nested Functions, With Application To Binary Autoencoders. This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. ----------------The organization of the paper is well written, and the presentation is clear. My questions are included in the following:--------- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?--------- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_302,https://openreview.net/forum?id=S1xh5sYgx,Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size,"Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).","Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size. Strengths---------- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. ---------- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.---------- x50 less memory usage than AlexNet, keeping similar accuracy ---------- strong experimental results----------------Weaknesses----------Would be nice to test Sqeezenet on multiple tasks------------------lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_303,https://openreview.net/forum?id=S1xh5sYgx,Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size,"Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).","Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size. Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn’t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_304,https://openreview.net/forum?id=S1xh5sYgx,Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size,"Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).","Squeezenet: Alexnet-level Accuracy With 50x Fewer Parameters And <0.5mb Model Size. The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.----------------Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.----------------On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.----------------Oh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_305,https://openreview.net/forum?id=BJuysoFeg,Revisiting Batch Normalization For Practical Domain Adaptation,"Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.","Revisiting Batch Normalization For Practical Domain Adaptation. This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.------------------------Pros:----------------The method is very simple and easy to understand and apply.----------------The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.----------------The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.------------------------Cons:----------------There is little novelty -- the method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what’s done in the Inception BN results in Table 1-2?)----------------The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.----------------Section 3.3: it’s not clear to me what point is being made here.------------------------Overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_306,https://openreview.net/forum?id=BJuysoFeg,Revisiting Batch Normalization For Practical Domain Adaptation,"Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.","Revisiting Batch Normalization For Practical Domain Adaptation. Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.----------------Overall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:----------------1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). ----------------2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_307,https://openreview.net/forum?id=BJuysoFeg,Revisiting Batch Normalization For Practical Domain Adaptation,"Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.","Revisiting Batch Normalization For Practical Domain Adaptation. Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.----------------Detailed comments:----------------Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.----------------Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)----------------Experiments:----------------- section 4.3.1 is not an accurate measure of the ""effectiveness"" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.----------------- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single ""whitening"" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_311,https://openreview.net/forum?id=S1jmAotxg,Stick-breaking Variational Autoencoders,"We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE’s.","Stick-breaking Variational Autoencoders. This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.----------------There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic ""black-box"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.----------------Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.----------------I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.----------------The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.----------------Overall, I found this to be an interesting paper, it would be a good fit for ICLR.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_312,https://openreview.net/forum?id=S1jmAotxg,Stick-breaking Variational Autoencoders,"We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE’s.","Stick-breaking Variational Autoencoders. The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.--------After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation) but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.--------With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior.  is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.--------Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.--------The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_313,https://openreview.net/forum?id=S1jmAotxg,Stick-breaking Variational Autoencoders,"We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE’s.","Stick-breaking Variational Autoencoders. Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.----------------In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.----------------Comments:--------- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.--------- sec 2.2: two comma's--------- text flow eq 6: please refer to appendix with the closed-form KL divergence--------- ""The v's are sampled via"" => ""In the posterior, the v's are sampled via"". It's not clear you're talking about the posterior here, instead of the prior.--------- The last paragraph of section 4 is great.--------- Sec 7.1: ""Density estimation"" => Technically you're also doing mass estimation.--------- Sec 7.1: 100 IS samples is a bit on the low side. --------- Figure 3(f). Interesting that k-NN works so well on raw pixels.","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_314,https://openreview.net/forum?id=H1Heentlx,Deep Variational Canonical Correlation Analysis,"We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variables'' underlying both views, extract the ``private variables'' within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.","Deep Variational Canonical Correlation Analysis. This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.----------------In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.----------------[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.----------------There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].----------------[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.--------[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.--------[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.--------[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.----------------I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.----------------However, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.----------------Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.----------------The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_315,https://openreview.net/forum?id=H1Heentlx,Deep Variational Canonical Correlation Analysis,"We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variables'' underlying both views, extract the ``private variables'' within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.","Deep Variational Canonical Correlation Analysis. UPDATE: I have read the replies on this thread. My opinion has not changed.----------------The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.----------------Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).----------------The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)----------------That said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of ""private variables"" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. ----------------There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. ----------------+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.--------+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_316,https://openreview.net/forum?id=H1Heentlx,Deep Variational Canonical Correlation Analysis,"We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variables'' underlying both views, extract the ``private variables'' within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.","Deep Variational Canonical Correlation Analysis. 7----------------Summary:--------This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.----------------Review:--------Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.----------------As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.----------------The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?----------------In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.----------------Minor:--------In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_320,https://openreview.net/forum?id=rksfwnFxl,Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection,"In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate `highly normal' sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.","Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection. This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model.--------+This is is well written and more of ideas are clearly presented.--------+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem---------The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. ---------The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.    Pros:  + Nice application of LSTMs to HIDS task    Cons:  - Nothing really novel from the algorithmic/ML side  - The significance of the results are difficult to assess without more formal understanding of the problem domains    The PCs have thus decided that this paper isn't ready to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_321,https://openreview.net/forum?id=rksfwnFxl,Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection,"In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate `highly normal' sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.","Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection. The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem ""heavy"" to me.----------------Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).----------------But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.----------------Therefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.----------------References:--------1. Debar, Herve, Monique Becker, and Didier Siboni. ""A neural network component for an intrusion detection system."" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.--------2. Creech, Gideon, and Jiankun Hu. ""A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns."" IEEE Transactions on Computers 63.4 (2014): 807-819.--------3. Staudemeyer, Ralf C. ""Applying long short-term memory recurrent neural networks to intrusion detection."" South African Computer Journal 56.1 (2015).",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.    Pros:  + Nice application of LSTMs to HIDS task    Cons:  - Nothing really novel from the algorithmic/ML side  - The significance of the results are difficult to assess without more formal understanding of the problem domains    The PCs have thus decided that this paper isn't ready to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_322,https://openreview.net/forum?id=rksfwnFxl,Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection,"In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate `highly normal' sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems.","Lstm-based System-call Language Modeling And Ensemble Method For Host-based Intrusion Detection. In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.--------The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. --------Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.--------The combination of the LMs is done by averaging transformations of the likelihoods. ----------------I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. --------The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:----------------- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. --------- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach. ","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were.    Pros:  + Nice application of LSTMs to HIDS task    Cons:  - Nothing really novel from the algorithmic/ML side  - The significance of the results are difficult to assess without more formal understanding of the problem domains    The PCs have thus decided that this paper isn't ready to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_333,https://openreview.net/forum?id=HJhcg6Fxg,Binary Paragraph Vectors,"Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.","Binary Paragraph Vectors. The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.-------- --------For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.-------- --------Pros:-------- --------- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09-------- --------- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM-------- --------Cons:-------- --------- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental-------- --------- the explanation is too abstract and difficult to follow for a non-expert (see details below)-------- --------- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16-------- --------Detailed comments:-------- --------Section 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings-------- --------figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why ""embedding lookup"" and ""linear projection"" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).-------- --------p2: ""This way, the length of binary codes is not tied to the dimensionality of word embeddings."" -> why not?-------- --------section 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.-------- --------""similarity of the inferred codes"": say here that codes are compared using Hamming distances.-------- --------""binary codes perform very well, despite their far lower capacity"" -> do you mean smaller size than real vectors?-------- --------fig 5: these plots could be dropped if space is needed.-------- --------section 3.1: one could argue that ""transferring"" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains-------- --------section 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?-------- --------fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.",6: Marginally above acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,0.0
2017_334,https://openreview.net/forum?id=HJhcg6Fxg,Binary Paragraph Vectors,"Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.","Binary Paragraph Vectors. This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques. The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture. Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization. (2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation). The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.----------------On the plus side, the application presented in the paper is interesting and important. The exposition of the paper is clean and clear. However, the novelty of the approach is limited from a machine learning standpoint. The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained. An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)----------------Given the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding. Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.----------------More comments:--------- I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing. That said, an end-to-end approach as proposed in this paper may perform better. I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.--------- See ""Estimating or Propagating Gradients Through Stochastic Neurons"" By Bengio et al - discussing straight through estimation and some other alternatives.--------- The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing. This is not quite right given multi-probe hashing mechanisms, for example see ""Mult-index Hashing"" by Norouzi et al.--------- See ""Hashing for Similarity Search: A Survey"" by Wang et al. for a survey of related work on binary hashing and quantization. You seem to ignore the extensive work done on binary hashing.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,1.0
2017_335,https://openreview.net/forum?id=HJhcg6Fxg,Binary Paragraph Vectors,"Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.","Binary Paragraph Vectors. This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.----------------The paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.----------------Figure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.----------------Minor comments:--------First line after the introduction: is sheer -> is the sheer--------4th line from the bottom of P1: words embeddings -> word embeddings--------In table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?--------5th line from the bottom of P5: W -> We--------5th line after section 3.1: covers wide -> covers a wide",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,0.0
2017_351,https://openreview.net/forum?id=BJlxmAKlg,Reasonet: Learning To Stop Reading In Machine Comprehension,"Teaching a computer to read a document and answer general questions pertaining to the document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called Reasoning Network ({ReasoNet}) for machine comprehension tasks. ReasoNet makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNet introduces a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNet can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNet has achieved state-of-the-art performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a structured Graph Reachability dataset. ",Reasonet: Learning To Stop Reading In Machine Comprehension. The paper proposes an architecture called ReasoNet that reason over the relation. The paper addresses important tasks but there are many other related works. The comparison to other methods are not comprehensive. The Graph Reachability dataset is not a good example to use.,5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper introduces a method for estimating the number of iterations of an attention mechanism in a neural machine reading module using REINFORCE and a custom baseline, which is estimated on the data. Estimating a baseline from data, multi-hop attention, and modelling latent variable models with REINFORCE are not new, so their composition, while sensible, is slightly incremental. There were some concerns from several of the reviewers with the impact the experiments have in terms of validating the model changes. Improvements on ""real"" tasks such as CNN/DailyMail did not impress reviewers, some of whom believe the benchmarks compared to were not representative of the best comparable architectures tried on these datasets (e.g. pointer networks, which definitely can be used). Overall, I do not find this paper strong enough to recommend acceptance to the main conference in its current state. With better evaluation, it could be a decently strong paper if the results come through.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_352,https://openreview.net/forum?id=BJlxmAKlg,Reasonet: Learning To Stop Reading In Machine Comprehension,"Teaching a computer to read a document and answer general questions pertaining to the document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called Reasoning Network ({ReasoNet}) for machine comprehension tasks. ReasoNet makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNet introduces a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNet can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNet has achieved state-of-the-art performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a structured Graph Reachability dataset. ","Reasonet: Learning To Stop Reading In Machine Comprehension. The paper aims to consolidate some recent literature in simple types of ""reading comprehension"" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into ""aggregation readers"" and ""explicit reference readers."" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.----------------I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. ----------------The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the ""explicit reference readers"" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for ""dramatic improvements in performance"" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.----------------I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper introduces a method for estimating the number of iterations of an attention mechanism in a neural machine reading module using REINFORCE and a custom baseline, which is estimated on the data. Estimating a baseline from data, multi-hop attention, and modelling latent variable models with REINFORCE are not new, so their composition, while sensible, is slightly incremental. There were some concerns from several of the reviewers with the impact the experiments have in terms of validating the model changes. Improvements on ""real"" tasks such as CNN/DailyMail did not impress reviewers, some of whom believe the benchmarks compared to were not representative of the best comparable architectures tried on these datasets (e.g. pointer networks, which definitely can be used). Overall, I do not find this paper strong enough to recommend acceptance to the main conference in its current state. With better evaluation, it could be a decently strong paper if the results come through.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_353,https://openreview.net/forum?id=BJlxmAKlg,Reasonet: Learning To Stop Reading In Machine Comprehension,"Teaching a computer to read a document and answer general questions pertaining to the document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called Reasoning Network ({ReasoNet}) for machine comprehension tasks. ReasoNet makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNet introduces a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNet can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNet has achieved state-of-the-art performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a structured Graph Reachability dataset. ","Reasonet: Learning To Stop Reading In Machine Comprehension. This paper proposes a new architecture for document comprehension. The main addition to the model, as claimed by the authors, is that the model is able to adaptively determine how many inference ‘hops’ is required in order to solve a particular problem. This is in contrast to previous work, where the number of hops is fixed.----------------Overall, the change of adding a termination gate is rather modest, and it leads to rather small gains on the tested CNN/ Daily Mail dataset (is it possible to include significance here?). Actually, I’m not sure why having an adaptive number of hops would be better performance-wise than using the maximum number of hops – unless I missed it the authors don’t argue this point well (other than saying it mimics humans). Does the model forget some things if it performs too many hops? The authors do say:--------“The results suggest that the termination gate variable in the ReasoNet is helpful when training with sophisticated examples, and makes models converge faster”--------but don’t elaborate much beyond this. I could see for example an argument being made that it reduces the amount of computation required per question. The authors do show results comparing the model without a termination gate on the Graph Reachability dataset, and the full model does seem to perform quite a bit better, but I would like this to also be done on the CNN/ Daily Mail datasets, and for there to be more insights into why the performance is improved vs. the ReasoNet-Last model.----------------One of the contributions I like most from this paper is not the actual model, but the Graph Reachability dataset. It is designed to test the reasoning abilities of the ReasoNet model in more detail. One of the benefits is that the inference procedure necessary to solve the task is very clear, as opposed to the CNN/ Daily Mail dataset, thus it is easier to see what the model is actually doing. I would like to see future models also tested on this dataset.----------------Overall, I think this is a borderline paper.----------------Other remarks:----------------Note that learning a baseline for REINFORCE has previously been studied, see: https://arxiv.org/pdf/1606.01541v4.pdf (although the authors mention only it briefly in the paper)----------------“ReasoNets are devised to mimic the inference process of human readers.”--------I think this is too strong a claim (that humans use the same kind of ‘iterative hop’ method for answering questions), unless it is supported by actual analysis of humans – I think the similarities to humans are more at a surface level. Also, I think it’s unnecessary to actually understanding the model. I would change ‘mimic’ to ‘inspired by’, or something along those lines.------------------------EDIT: I thank the authors for taking the time to reply, and clearing some things up with regards to the idea of multiple hops. I am keeping my score the same for the following reason: in my opinion, accepted papers involving new model architectures should either consist of (1) a small adjustment that leads to a large improvement, or (2) a very innovative idea that leads to comparable performance (or better), but provides many new insights about the problem or can be transferred to many different domains. This paper seems to be a rather small adjustment (allowing multiple hops) which results in small improvements on CNN/Daily Mail (which as Reviewer 3 points out has little headroom). I think this paper would be suitable for a conference such as EMNLP.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper introduces a method for estimating the number of iterations of an attention mechanism in a neural machine reading module using REINFORCE and a custom baseline, which is estimated on the data. Estimating a baseline from data, multi-hop attention, and modelling latent variable models with REINFORCE are not new, so their composition, while sensible, is slightly incremental. There were some concerns from several of the reviewers with the impact the experiments have in terms of validating the model changes. Improvements on ""real"" tasks such as CNN/DailyMail did not impress reviewers, some of whom believe the benchmarks compared to were not representative of the best comparable architectures tried on these datasets (e.g. pointer networks, which definitely can be used). Overall, I do not find this paper strong enough to recommend acceptance to the main conference in its current state. With better evaluation, it could be a decently strong paper if the results come through.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_354,https://openreview.net/forum?id=BJa0ECFxe,Information Dropout: Learning Optimal Representations Through Noise,"We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.","Information Dropout: Learning Optimal Representations Through Noise. Paper summary--------This paper develops a generalization of dropout using information theoretic--------principles. The basic idea is that when learning a representation z of input x--------with the aim of predicting y, we must choose a z such that it carries the least--------amount of information about x, as long as it can predict y. This idea can be--------formalized using the Information Bottleneck Lagrangian. This leads to an--------optimization problem which is similar to the one derived for variational--------dropout, the difference being that Information dropout allows for a scaling--------factor associated with the KL divergence term that encourages noise. The amount--------of noise being added is made a parameterized function of the data and this--------function is optimized along with the rest of the model. Experimental results on--------CIFAR-10 and MNIST show (small) improvements over binary dropout.----------------Strengths--------- The paper highlights an important conceptual link between probabilistic--------  variational methods and information theoretic methods, showing that dropout--------can be generalized using both formalisms to arrive at very similar models.--------- The presentation of the model is excellent.--------- The experimental results on cluttered MNIST are impressive.----------------Weaknesses--------- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless--------  the axis label is a typo). It is not clear why the test set was not used. This--------makes it hard to compare to results reported in Springenberg et al, as well as--------other results in literature.----------------Quality--------The theoretical exposition is high quality. Figure 2 gives a nice qualitative--------assessment of what the model is doing. However, the experimental results--------section can be made better, for example, by matching the results on CIFAR-10 as--------reported in Springenberg et al. and trying to improve on those using information--------dropout.----------------Clarity--------The paper is well written and easy to follow.----------------Originality--------The derivation of the information dropout optimization problem using IB--------Lagrangian is novel. However, the final model is quite close to variational--------dropout.----------------Significance--------This paper will be of general interest to researchers in representation learning--------because it highlights an alternative way to think about latent variables (as--------information bottlenecks). However, unless the model can be shown to achieve--------significant improvements over simple dropout, its wider impact is likely to be--------limited.----------------Overall--------The paper presents an insightful theoretical derivation and good preliminary--------results. The experimental section can be improved.----------------Minor comments and suggestions ---------- expecially -> especially--------- trough -> through--------- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).--------- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those--------for Figure 3(b) as well.--------- Please consider comparing Figure 2 with the activity map of a standard CNN--------  trained with binary dropout, so we can see if similar filtering out is--------happening there already.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the ""information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper"". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_355,https://openreview.net/forum?id=BJa0ECFxe,Information Dropout: Learning Optimal Representations Through Noise,"We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.","Information Dropout: Learning Optimal Representations Through Noise. An interesting connection is made between dropout, Tishby et al's ""information bottleneck"" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.----------------The objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.----------------- Related work (section 2) is discussed sufficiently. --------- In section 3, would be better to remind us the definition of mutual information.--------- Connection to VAEs in section 5 is interesting.--------- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.--------- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.--------- It's unclear which version of 'beta' was used in figure 3a.----------------Overall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the ""information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper"". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_356,https://openreview.net/forum?id=BJa0ECFxe,Information Dropout: Learning Optimal Representations Through Noise,"We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.","Information Dropout: Learning Optimal Representations Through Noise. The authors propose ""information dropout"", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. ----------------It remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.----------------The experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the ""information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper"". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_357,https://openreview.net/forum?id=ryxB0Rtxx,Identity Matters In Deep Learning,"An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.  In this work, we put the principle of identity parameterization on a more  solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate.  Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.  Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.","Identity Matters In Deep Learning. This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.----------------One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. ----------------Minors: one line before Eq. (3.1), U \in R ? \times k","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes.   I think the paper meets the threshold to be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_358,https://openreview.net/forum?id=ryxB0Rtxx,Identity Matters In Deep Learning,"An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.  In this work, we put the principle of identity parameterization on a more  solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate.  Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.  Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.","Identity Matters In Deep Learning. Paper Summary:----------------Authors investigate identity re-parametrization in the linear and the non linear case. ----------------Detailed comments:----------------— Linear Residual Network:----------------The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. ---------------- — Non linear Residual Network:----------------Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. ----------------1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify ----------------2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?--------In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  ----------------3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? --------A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).----------------4- What does the construction tell us about the number of layers? ----------------5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block.--------I don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction--------does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. ----------------— Experiments : ----------------- last layer is not trained means the layer before the linear layer preceding the softmax?----------------Minor comments:----------------Abstract:  how  the identity mapping motivated batch normalization?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes.   I think the paper meets the threshold to be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_359,https://openreview.net/forum?id=ryxB0Rtxx,Identity Matters In Deep Learning,"An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.  In this work, we put the principle of identity parameterization on a more  solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate.  Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.  Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.","Identity Matters In Deep Learning. This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper:-----------------Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks.-----------------Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this.-----------------Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.----------------Minor comments:----------------1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea.----------------2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1---------------- ",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes.   I think the paper meets the threshold to be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_360,https://openreview.net/forum?id=B1akgy9xx,Making Stochastic Neural Networks From Deterministic Ones,"It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN -> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.","Making Stochastic Neural Networks From Deterministic Ones. Strengths----------------- interesting to explore the connection between ReLU DNN and simplified SFNN--------- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally--------- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)------------------------Weaknesses-----------------no results are reported on real tasks with large training set-----------------not clear exploration on the scalability of the learning methods when training data becomes larger-----------------when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”.-----------------would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_361,https://openreview.net/forum?id=B1akgy9xx,Making Stochastic Neural Networks From Deterministic Ones,"It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN -> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.","Making Stochastic Neural Networks From Deterministic Ones. This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.----------------The connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.----------------My main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_364,https://openreview.net/forum?id=Sys6GJqxl,Delving Into Transferable Adversarial Examples And Black-box Attacks,"An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.","Delving Into Transferable Adversarial Examples And Black-box Attacks. The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs.----------------I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples.  ----------------There are, however, some concerns:----------------1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: ----------------- First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3)----------------- Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai)----------------- Also, here are all the other details and explorations. ----------------2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper).",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_365,https://openreview.net/forum?id=Sys6GJqxl,Delving Into Transferable Adversarial Examples And Black-box Attacks,"An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.","Delving Into Transferable Adversarial Examples And Black-box Attacks. This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of ""attacks"" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.----------------The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on ""clarifai.com"" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).----------------To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).----------------Arguably, The paper still has some weaknesses:---------------- - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that ""One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet."", i.e., the three ResNet-based networks.---------------- - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).---------------- - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.---------------- - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).---------------- - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).------------------------To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper).",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_366,https://openreview.net/forum?id=Sys6GJqxl,Delving Into Transferable Adversarial Examples And Black-box Attacks,"An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.","Delving Into Transferable Adversarial Examples And Black-box Attacks. I reviewed the manuscript as of December 7th.----------------Summary:--------The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.----------------Major Comments:--------1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.----------------2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.----------------3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.----------------4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). ----------------As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.----------------Areas to Trim the Paper:--------- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.--------- Condense Section 2.2.1 and cite heavily.--------- Figure 2 panels may be overlaid to highlight a comparison.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper).",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_370,https://openreview.net/forum?id=BkXMikqxx,Cortical-inspired Open-bigram Representation For Handwritten Word Recognition,"Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, i.e. couple of letters that are not necessarily next to each other.  In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction.  For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this  representation and the bagged RNN prediction to retrieve the vocabulary word.  We compare this method to standard word recognition techniques based on  sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable  to more conventional decoding methods based on sequences of letters.","Cortical-inspired Open-bigram Representation For Handwritten Word Recognition. This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. ----------------Pros:--------- The use of OBs is novel and interesting.--------- Clearly written and explained.----------------Cons:--------- No comparison to previous state of the art, only with author-generated results. --------- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.--------- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results. --------- Why not use a graph-based decoder like in Fig 2 b?----------------Overall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"There is consistent agreement towards the originality of this work and that the topic here is ""interesting"". Additionally there is consensus that the work is ""clearly written"", and (excepting questions of the word ""cortical"") all would be primed to accept this style of work.     However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_371,https://openreview.net/forum?id=BkXMikqxx,Cortical-inspired Open-bigram Representation For Handwritten Word Recognition,"Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, i.e. couple of letters that are not necessarily next to each other.  In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction.  For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this  representation and the bagged RNN prediction to retrieve the vocabulary word.  We compare this method to standard word recognition techniques based on  sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable  to more conventional decoding methods based on sequences of letters.","Cortical-inspired Open-bigram Representation For Handwritten Word Recognition. This paper uses an LSTM model to predict what it calls ""open bigrams"" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:----------------- I find the ""cortical inspired"" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more ""findings in cognitive neurosciences [sic] research on reading"" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that ""deep neural networks are based on a series of about five pairs of neurons [sic] layers"". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is ""quite close to the number of layers of an efficient deep NN"" -- what network? what task? etc.----------------- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it ""is focused on the decoder"" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?----------------- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called ""extremities"") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.----------------I very much like the idea of the paper, but I am simply not convinced by its claims.----------------Minor points:--------- There are quite a few typos. Just a sample: ""independant"" (Fig.1), ""we evaluate an handwritten"", "", hand written words [..], an the results"", ""their approach include"", ""the letter bigrams of a word w is"", ""for the two considered database""--------- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.--------- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is consistent agreement towards the originality of this work and that the topic here is ""interesting"". Additionally there is consensus that the work is ""clearly written"", and (excepting questions of the word ""cortical"") all would be primed to accept this style of work.     However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_372,https://openreview.net/forum?id=BkXMikqxx,Cortical-inspired Open-bigram Representation For Handwritten Word Recognition,"Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, i.e. couple of letters that are not necessarily next to each other.  In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction.  For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this  representation and the bagged RNN prediction to retrieve the vocabulary word.  We compare this method to standard word recognition techniques based on  sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable  to more conventional decoding methods based on sequences of letters.","Cortical-inspired Open-bigram Representation For Handwritten Word Recognition. This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.----------------Overall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.----------------It would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.----------------Language models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.----------------For your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.----------------I do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.----------------End of page 1: ""whole language method"" - please explain what is meant by this.----------------Page 6: define your notation for rnn_d(x,t).----------------The number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.------------------------Minor comments: a spell check is recommended--------p. 2: state-of-art -> state-of-the-art--------p. 2: predict character sequence -> predict a character sequence--------p. 3, top: Their approach include -> Their approach includes--------p. 3, top: an handwritten -> a handwritten--------p. 3, bottom: consituent -> constituent--------p. 4, top: in classical approach -> in the classical approach--------p. 4, top: transformed in a vector -> transformed into a vector--------p. 5: were build -> were built--------References: first authors name written wrongly: Thodore Bluche -> Theodore Bluche","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"There is consistent agreement towards the originality of this work and that the topic here is ""interesting"". Additionally there is consensus that the work is ""clearly written"", and (excepting questions of the word ""cortical"") all would be primed to accept this style of work.     However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_373,https://openreview.net/forum?id=SJUdkecgx,Submission By Masoud Faraki,  ,"Submission By Masoud Faraki. This paper presents extensions of the KISSME metric learning method.--------First, a subspace learning component is integrated, which avoids the need of ad-hoc pre-processing in the case of high-dimensional data.--------Second, the approach is integrated with a CNN model that derives the features from an input image before it is entered into the Mahalanobis distance computation.----------------The related work section would benefit from a disucssion of previous work that also learns low-rank Mahalanobis metrics, such as e.g. [A]. One might argue that the referenced work, and similar ones, solves a non-convex objective and is therefore of less interest. This argument, however, seems weak at the moment that CNN training is integrated, since this also involves hihgly non-convex optimisation, yet leads to excellent results. A discussion on this point would be useful to include in the paper.----------------In the first experiment, the authors compare to related metric learning work. They  conclude that the proposed approach is superior, but this seems unjustified as the CNNs used to generate the features by the authors is different from the one used in related work. Thus it seems to me that no conclusions on the quality of the metric learning approach can be drawn from these experiments.----------------In the second experiment the authors show that a baseline where a factorisation of the PSD matrix M is absorbed in the low-rank projection matrix W leads to worse results than the proposed over-parametrised method. It would be useful if the authors could provide an analysis of why this might be the case; the current text does not seem to provide an explanation or hyposthesis. Is the same cost function used by the proposed method and the pairwise and triplet baseline ?----------------I found the title of the paper slightly misleading, as the contribution of the paper is not to provide an extension to deep feature training: this has been done extensively in the past. In my understanding, the contribution lies in the proposed (overparameterized) formulation of learning a low-rank Mahalanobis metric for the KISSME objective function. Certainly, the extension to deep feature training is interesting, but relatively straightforward as compared to the main contribution. The title could reflect this more accurately.----------------The main motivation of the paper to base itself on KISSME is scalability, see introduction. However, for the case where a CNN is trained it seems that it is likely to be in a large-scale data and iterative training regime, and it is not obvious that KISSME would be more efficient than other metric learning objectives based on a pairwise or triplet loss. Therefore, comparison with other metric learning  objective functions in both performance and run time would be very useful to get a complete picture of which methods are most effective under what conditions. ------------------------[A] Guillaumin, M.; Mensink, T.; Verbeek, J. & Schmid, C. Face recognition from caption-based supervision IJCV, 2012, 96, 64-82",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"We are students of University of Michigan registered for the ICLR Reproducibility Challenge 2018. As a part of that, we are trying to replicate the results stated in the paper 'Time Limits in Reinforcement Learning'. The authors have mentioned that the code would be made publicly available on https://sites.google.com/view/time-limits-in-rl . However, the code is not up yet. Would it be possible to get access to the code? That would greatly help us verifying our implementation of the algorithms. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_374,https://openreview.net/forum?id=SJUdkecgx,Submission By Masoud Faraki,  ,"Submission By Masoud Faraki. KISSME was a promising metric learning approach that got lost in the deep learning flood. This is a shame since it had very good results at the time, but it was hindered by the quirks of the PCA pre-processing stage. In a nutshell, this paper reimagines this approach using deep convnets, gets good results on modern benchmark datasets and does not require meticulous preprocessing tricks. It therefore deserves to be published, since it rescues a promising approach from obscurity and does a competent job of illustrating its potential in a modern context, with all the necessary discussion of related work on old (LMNN, MMC, PCCA) and new (deep, triplet-loss based) metric learning approaches.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"We are students of University of Michigan registered for the ICLR Reproducibility Challenge 2018. As a part of that, we are trying to replicate the results stated in the paper 'Time Limits in Reinforcement Learning'. The authors have mentioned that the code would be made publicly available on https://sites.google.com/view/time-limits-in-rl . However, the code is not up yet. Would it be possible to get access to the code? That would greatly help us verifying our implementation of the algorithms. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_375,https://openreview.net/forum?id=SJUdkecgx,Submission By Masoud Faraki,  ,"Submission By Masoud Faraki. The paper modifies the KISSME algorithm in two ways: by incorporating a dimensionality reduction step and by integrating it in a loss function for learning deep networks. Experiments show that the latter solution is more stable than standard approaches to metric learning with deep nets.----------------The paper contribution could be good, but the paper could use some polish. After carefully reading the authors' response to my original questions, I have to say that I am still confused about several details.----------------Lemma 1 states that ""log(delta) identifies a Mahalanobis metric ... as M = Proj(...)"". I still think that the wording of the Lemma is imprecise; the authors should probably simply say that log(delta) results in an approximation of a certain metric, up to a constant term.  The word ""identifies"" definitely does not mean ""approximates"", which, since the word ""approximation"" is contained in the proof of the Lemma, is what is happening here. The authors have promised to fix this issue in the final version by changing the wording to ""log(\delta) determines a Mahalanobis metric"", but ""determines"" means that log(\delta) is a metric, whereas it seems to only *approximate* a metric.----------------Note also that the proof contains the sentence ""the Mahalanobis distance should approximate"". It is quite odd to find the verb ""should"" in a mathematical proof. Ultimately, we are not sure what is being proved since the statement of the lemma is imprecise.----------------Following up on this discussion, about the relationships between Eq. 4 and 6, the authors comment that ""While, the form of Eq.6 may suggest that it is an approximation to Eq.4, as discussed above, it is indeed the form of metric obtained in the latent space"".  I agree that, taken in isolation, this is a valid definition of some metric, but then, at the very least, the paper links this up with the rest of the discussion in a very confusing manner. Note that Eq. 6 is obtained ""using lemma 1"", which contains the world ""approximation"" in the proof.----------------While none of these issues necessarily invalidates the *method* and the *results* of the paper, they make the *premises* a little tenuous and confusing.----------------A second contribution of the paper is to incorporate KISSME in metric learning using deep networks and pairwise loss. Here the authors clarify satisfactorily the difference between standard metric learning in deep networks and their approach in their answers. I would recommend putting the comparison with the baselines upfront to make this message clearer to the reader (e.g. by moving Section 4.2 before Section 4.1).----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Eq. (5) and (6) are supposed to ""reflect better"" Eq. (1) than Eq. (4). In fact, Eq. (6) is a regularised version of Eq. (4) (by the low-dimensional bottleneck), so it can only be a worse approximation of Eq. (1). Probably the authors mean that the regularisation can make Eq. (6) preferable to Eq. (4) on the test data.----------------I would also like to understand better how the method is integrated in deep learning.  As noted by the authors,  with CNNs one can simply learn a projection matrix L as the last FC layer and define the metric to be L'L. Learning can use the usual Siamese scheme with pairwise or triplet losses. I am rather confused how the proposed algorithm relates to this approach according (Section 3). My understanding is that one first fixes L and trains the rest of the network optimising the pairwise loss Eq. (14). Once this is done, one updates L as the square root of the matrix M obtained by the modified KISSME algorithm. Is that correct? If so, the authors should compare to the obvious baseline of optimising L directly as part of back-propagation while training the network, as this is a much more direct approach.----------------I am also confused about the dimensionality reduction role of matrix W. In Eq. (5) W^T is used to take the data down from dimension D to dimension d. However, when applied to a CNN, the dimensionality reduction is assumed to be incorporated in the network itself, such that the dimension of the CNN representation is d from the outset. Hence, in this case the modified KISSME algorithm does not need to perform any dimensionality reduction. Is that so? Can the authors give us an intuition of what their algorithm brings on top of standard Siamese learning in this case?----------------Experiments:----------------Comparison with state of the art on the Car dataset is not very meaningful as every method appear to use a different underlying deep net. The paper uses GoogLeNet, which is significantly better than VGG-M used by Liu et al (2016). Hence it is not surprising that the authors obtain a better performance.----------------I am more interested in understanding the comparison with the KISSME baseline. How is this baseline applied to this data? Is there any fine tuning of the CNN at all? If not, this may be enough to explain the difference with the proposed JDR-KISSME.----------------There is one last experiment on CIFAR after conclusion and references. Should we consider that as well?","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"We are students of University of Michigan registered for the ICLR Reproducibility Challenge 2018. As a part of that, we are trying to replicate the results stated in the paper 'Time Limits in Reinforcement Learning'. The authors have mentioned that the code would be made publicly available on https://sites.google.com/view/time-limits-in-rl . However, the code is not up yet. Would it be possible to get access to the code? That would greatly help us verifying our implementation of the algorithms. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_382,https://openreview.net/forum?id=H12GRgcxg,Training Deep Neural-networks Using A Noise Adaptation Layer,"The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.","Training Deep Neural-networks Using A Noise Adaptation Layer. This paper looks at how to train if there are significant label noise present.--------This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.----------------The second directly integrates out the true label and simply optimizes the p(z|x).----------------Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.--------Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.----------------- comments:--------Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?--------It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. ----------------Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes.     During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_383,https://openreview.net/forum?id=H12GRgcxg,Training Deep Neural-networks Using A Noise Adaptation Layer,"The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.","Training Deep Neural-networks Using A Noise Adaptation Layer. The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. ----------------The experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.----------------For example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. ----------------From Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful. ---------------- ","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes.     During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_384,https://openreview.net/forum?id=H12GRgcxg,Training Deep Neural-networks Using A Noise Adaptation Layer,"The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.","Training Deep Neural-networks Using A Noise Adaptation Layer. This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.----------------A few comments.--------1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. ----------------2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?----------------3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. ----------------4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes.     During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_401,https://openreview.net/forum?id=SJzCSf9xg,On Detecting Adversarial Perturbations,"Machine learning and  deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust.  We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack. ","On Detecting Adversarial Perturbations. This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.----------------My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.--------That being said, the novelty of this paper is still significant.----------------Minor comment:--------The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_402,https://openreview.net/forum?id=SJzCSf9xg,On Detecting Adversarial Perturbations,"Machine learning and  deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust.  We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack. ",On Detecting Adversarial Perturbations. This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.----------------This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.----------------The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.----------------The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.,"7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_403,https://openreview.net/forum?id=SJzCSf9xg,On Detecting Adversarial Perturbations,"Machine learning and  deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust.  We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack. ","On Detecting Adversarial Perturbations. I reviewed the manuscript on December 5th.----------------Summary:--------The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.----------------Major comments:----------------The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. ----------------A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.----------------My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.----------------Minor comments:----------------If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.----------------- X-axis label is wrong in Figure 2 right.----------------Measure the transferability of the detector?----------------- How is \sigma labeled on Figure 5?----------------- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_404,https://openreview.net/forum?id=ryh_8f9lg,Classless Association Using Neural Networks,"The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  We propose a novel model called Classless Association.  It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa.  In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution.  We generate four classless datasets based on MNIST, where the input is two different instances of the same digit.  In addition,  the digits have a uniform distribution.  Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised.  In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms.","Classless Association Using Neural Networks. The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.------------------------The presentation of the paper is not very clear, the writing can be improved.--------Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  ----------------Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. ----------------Overall, I think this work should be clarified and improved to be a good fit for this venue.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_405,https://openreview.net/forum?id=ryh_8f9lg,Classless Association Using Neural Networks,"The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  We propose a novel model called Classless Association.  It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa.  In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution.  We generate four classless datasets based on MNIST, where the input is two different instances of the same digit.  In addition,  the digits have a uniform distribution.  Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised.  In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms.","Classless Association Using Neural Networks. The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.----------------The basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior? The current setup needs justifications. ----------------What would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. ----------------At the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.----------------Few more points:--------Typo: Figure1. second line in the caption ""that"" -> ""than""--------Necessity of Equation 2 is not clear--------Batch size M is enormous compared to classical models, there is no explanation for this--------Why uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)--------Typo: Page 6, second paragraph line 3: ""that"" -> ""than""",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_406,https://openreview.net/forum?id=ryh_8f9lg,Classless Association Using Neural Networks,"The goal of this paper is to train a model based on the relation between two instances that represent the same unknown class.  This scenario is inspired by the Symbol Grounding Problem and the association learning in infants.  We propose a novel model called Classless Association.  It has two parallel Multilayer Perceptrons (MLP) that uses one network as a target of the other network, and vice versa.  In addition, the presented model is trained based on an EM-approach, in which the output vectors are matched against a statistical distribution.  We generate four classless datasets based on MNIST, where the input is two different instances of the same digit.  In addition,  the digits have a uniform distribution.  Furthermore, our classless association model is evaluated against two scenarios: totally supervised and totally unsupervised.  In the first scenario, our model reaches a good performance in terms of accuracy and the classless constraint.  In the second scenario, our model reaches better results against two clustering algorithms.",Classless Association Using Neural Networks. The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.,6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures, but it is poorly motivated. The experimental evaluation of the proposed model is very limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_413,https://openreview.net/forum?id=SJBr9Mcxl,Understanding Trained Cnns By Indexing Neuron Selectivity,"The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ","Understanding Trained Cnns By Indexing Neuron Selectivity. This paper makes three main methodological contributions:-------- - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron-------- - ranking of neurons based on color selectivity-------- - ranking of neurons based on class selectivity----------------The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.----------------However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:-------- - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported.-------- - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)-------- - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation!----------------Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence.------------------------Other small comments:-------- - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite)---------------- - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.---------------- - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_414,https://openreview.net/forum?id=SJBr9Mcxl,Understanding Trained Cnns By Indexing Neuron Selectivity,"The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ","Understanding Trained Cnns By Indexing Neuron Selectivity. The authors analyze trained neural networks by quantifying the selectivity of individual neurons in the network for a variety of specific features, including color and category.   ----------------Pros:--------   * The paper is clearly written and has good figures. --------   * I think they executed their specific stated goal reasonably well technically.   E.g. the various indexes they use seem well-chosen for their purposes. ----------------Cons:--------   * I must admit that I am biased against the whole enterprise of this paper.   I do not think it is well-motivated or provides any useful insight whatever.   What I view their having done is produced, and then summarized anecdotally, a catalog of piecemeal facts about a neural network without any larger reason to think these particular facts are important.  In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop.  As if that were in and of itself important or useful information.   I do not feel that either the original neural version of that project, or this current model-based virtual electrophysiology, is that useful.   Why should we care about the distribution of color selectivities?   Why does knowing distribution as such constitute ""understanding""?    To my mind it doesn't, at least not directly.   ----------------Here's what they could have done to make a more useful investigation:--------  --------     (a) From a neuroscience point of view, they could have compared the properties that they measure in models to the same properties as measured in neurons the real brain.   If they could show that some models are better matches on these properties to the actual neural data than others, that would be a really interesting result.   That is is to say, the two isolated catalogs of selectivities (from model neurons and real neurons)  alone seem pretty pointless.  But if the correspondence between the two catalogs was made -- both in terms of where the model neurons and the real neurons were similar, and (especially importantly) where they were different --- that would be the beginning of nontrivial understanding.   Such results would also complement a growing body of literature that attempts to link CNNs to visual brain areas.  Finding good neural data is challenging, but whatever the result, the comparison would be interesting. ----------------and/or ----------------    (b) From an artificial intelligence point of view, they could have shown that their metrics are *prescriptive* constraints.   That is, suppose they had shown that the specific color and class selectivity indices that they compute, when imposed as a loss-function criterion on an untrained neural network, cause the network to develop useful filters and achieve significantly above-chance performance on the original task the networks were trained on.     This would be a really great result, because it would not only give us a priori reason to care about the specific property metrics they chose, but it would also help contribute to efforts to find unsupervised (or semi-supervised) learning procedures, since the metrics they compute can be estimated from comparatively small numbers of stimuli and/or high-level semantic labels.    To put this in perspective, imagine that they had actually tested the above hypothesis and found it to be false:  that is, that their metrics, when used as loss function constraints, do not improve performance noticeably above chance performance.  What would we then make of this whole investigation?  It would then be reasonable to think that the measured properties were essentially epiphenomenal and didn't contribute at all to the power of neural networks in solving perceptual tasks.  (The same could be said about neurophysiology experiments doing the same thing.)  --------     [--> NB: I've actually tried things just like this myself over the years, and have found exactly this disappointing result.  Specifically,  I've found a number of high-level generic statistical property of DNNs that seem like they might potentially ""interesting"", e.g. because they apparently correlate with complexity or appear to illustrate difference between low, intermediate and high layers of DNNs.  Every single one of these, when imposed as optimization constraints, has basically lead nowhere on the challenging tasks (like ImageNet) that cause the DNNs to be interesting in the first place.  Basically, there is to my mind no evidence at this point that highly-summarized generic statistical distributions of selectivities, like those illustrated here, place any interesting constraints on filter weights at all.   Of course, I haven't tried the specific properties the authors highlight in these papers, so maybe there's something important there.]----------------I know that both of these asks are pretty hard, but I just don't know what else to say -- this work otherwise seems like a step backwards for what the community ought to be spending its time on. -------- ",3: Clear rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_415,https://openreview.net/forum?id=SJBr9Mcxl,Understanding Trained Cnns By Indexing Neuron Selectivity,"The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ","Understanding Trained Cnns By Indexing Neuron Selectivity. This paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.----------------This seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  ","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_426,https://openreview.net/forum?id=HkvS3Mqxe,Coarse Pruning Of Convolutional Neural Networks With Random Masks,"The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns further increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning changes the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple strategy to choose the least adversarial pruning masks. The proposed approach is generic and can select good pruning masks for feature map, kernel and intra-kernel pruning. The pruning masks are generated randomly, and the best performing one is selected using the evaluation set. The sufficient number of random pruning masks to try depends on the pruning ratio, and is around 100 when 40% complexity reduction is needed. The pruned network is retrained to compensate for the loss in accuracy. We have extensively evaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the misclassification rate of the baseline network.","Coarse Pruning Of Convolutional Neural Networks With Random Masks. This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being ""one shot"" and ""near optimal"" that cannot be supported: it is ""N-shot"" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is ""near optimal.""----------------Pros:--------- Nice taxonomy of pruning levels--------- Comparison to the recent weight-sum pruning method----------------Cons:--------- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)--------- Paper is somewhat hard to follow--------- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements----------------Another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.----------------In summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_427,https://openreview.net/forum?id=HkvS3Mqxe,Coarse Pruning Of Convolutional Neural Networks With Random Masks,"The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns further increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning changes the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple strategy to choose the least adversarial pruning masks. The proposed approach is generic and can select good pruning masks for feature map, kernel and intra-kernel pruning. The pruning masks are generated randomly, and the best performing one is selected using the evaluation set. The sufficient number of random pruning masks to try depends on the pruning ratio, and is around 100 when 40% complexity reduction is needed. The pruned network is retrained to compensate for the loss in accuracy. We have extensively evaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the misclassification rate of the baseline network.","Coarse Pruning Of Convolutional Neural Networks With Random Masks. This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. --------However, this paper also has the following problems. --------1) The method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. --------2) Experiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). --------3) It is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.--------4) (*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. --------Note that I agree with that a smaller network may be more generalizable than a larger network. ------------------------------------------------------------------------------Comments to the authors's response:----------------Thanks for replying to my comments. ----------------1) I still believe that the proposed methods are trivial.--------2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?--------3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.--------4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_428,https://openreview.net/forum?id=HkvS3Mqxe,Coarse Pruning Of Convolutional Neural Networks With Random Masks,"The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns further increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning changes the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple strategy to choose the least adversarial pruning masks. The proposed approach is generic and can select good pruning masks for feature map, kernel and intra-kernel pruning. The pruning masks are generated randomly, and the best performing one is selected using the evaluation set. The sufficient number of random pruning masks to try depends on the pruning ratio, and is around 100 when 40% complexity reduction is needed. The pruned network is retrained to compensate for the loss in accuracy. We have extensively evaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the misclassification rate of the baseline network.","Coarse Pruning Of Convolutional Neural Networks With Random Masks. Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.----------------Pros:--------Proposes a method to choose pruning mask out of N trials. --------Analysis on different pruning methods.----------------Cons & Questions:--------“The proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.” How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)--------Missing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)--------Since reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.----------------Misc:--------Typo in figure 6 a) caption: “Featuer” (corrected)",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_429,https://openreview.net/forum?id=SkhU2fcll,Deep Multi-task Representation Learning: A Tensor Factorisation Approach,"Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.","Deep Multi-task Representation Learning: A Tensor Factorisation Approach. The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.----------------The framework is nice and appealing. ----------------However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication.     Pros:  - Reviewers in general found the paper clear and well written.  - Multi-task learning in deep models is of interest to the community  - The approach is sensible and the experiments show that it seems to work    Cons:  - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental  - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods  - Not all reviewers were convinced by the experiments  - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper    The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_430,https://openreview.net/forum?id=SkhU2fcll,Deep Multi-task Representation Learning: A Tensor Factorisation Approach,"Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.","Deep Multi-task Representation Learning: A Tensor Factorisation Approach. The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. ----------------As mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. ----------------One question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication.     Pros:  - Reviewers in general found the paper clear and well written.  - Multi-task learning in deep models is of interest to the community  - The approach is sensible and the experiments show that it seems to work    Cons:  - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental  - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods  - Not all reviewers were convinced by the experiments  - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper    The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_431,https://openreview.net/forum?id=SkhU2fcll,Deep Multi-task Representation Learning: A Tensor Factorisation Approach,"Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.","Deep Multi-task Representation Learning: A Tensor Factorisation Approach. This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-deﬁned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.----------------Although factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. ----------------BTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., ----------------Huang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication.     Pros:  - Reviewers in general found the paper clear and well written.  - Multi-task learning in deep models is of interest to the community  - The approach is sensible and the experiments show that it seems to work    Cons:  - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental  - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods  - Not all reviewers were convinced by the experiments  - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper    The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_442,https://openreview.net/forum?id=Bkepl7cee,Parametric Exponential Linear Unit For Deep Convolutional Neural Networks,"The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.","Parametric Exponential Linear Unit For Deep Convolutional Neural Networks. The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature.----------------There is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of ""maybe"" ""could mean"", ""perhaps"" etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is.----------------The theoretical section could be made a little clearer.----------------Finally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_443,https://openreview.net/forum?id=Bkepl7cee,Parametric Exponential Linear Unit For Deep Convolutional Neural Networks,"The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.","Parametric Exponential Linear Unit For Deep Convolutional Neural Networks. This paper presents a new non-linear function for CNN and deep neural networks. --------The new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minimal increase in computation.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_444,https://openreview.net/forum?id=Bkepl7cee,Parametric Exponential Linear Unit For Deep Convolutional Neural Networks,"The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.","Parametric Exponential Linear Unit For Deep Convolutional Neural Networks. This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer. This parameter is proposed to more effectively counter vanishing gradients. ----------------My main concern regarding this paper is related to the authors' claims about the effectiveness of PELU. The analysis in Sections 2 and 3 discusses how PELU might improve training by combating gradient propagation issues. This by itself does not imply that improved generalization will result, only that models may be easier to train. However, the experiments all seek to demonstrate improved generalization performance.--------But this could in principle be due to a better inductive bias, and have nothing to do with the optimization analysis. None of the experiments are designed to directly support the stated theoretical advantage of PELU compared to ELU in optimizing models.----------------In the response to the pre-review question, the authors state that the claims in Section 2 and 3.3 are meant to apply to generalization performance. I fail to see how this is true for most claims, except the flexibility claim. As the authors agree, better training may or may not lead to better out-of-sample performance. I can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem (instead of overfitting), but this is a much weaker claim compared to the mathematical justifications for improved optimization.----------------On selection of learning hyperparameters:--------The authors state in the discussion on OpenReview that the learning rates selected were favorable to ReLU, and not PELU. However, this does not guarantee that they were not unfavorable to ELU. It raises the question: can a regime be constructed where ELU has better performance than PELU? If so, how can we draw the conclusion that PELU is better?----------------Overall, I am not yet convinced by the experimental setup and the match between theory and experiments in this paper.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_445,https://openreview.net/forum?id=Bkepl7cee,Parametric Exponential Linear Unit For Deep Convolutional Neural Networks,"The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.",Parametric Exponential Linear Unit For Deep Convolutional Neural Networks. Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. ----------------Interesting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.-------- ,6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_449,https://openreview.net/forum?id=B1IzH7cxl,A Neural Stochastic Volatility Model,"In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones. Our focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.","A Neural Stochastic Volatility Model. The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.’s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. ----------------The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). ----------------The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.----------------I have a few comments and reservations with the paper:--------1) Although not  mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons.-------- --------2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. ----------------3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.----------------4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4)----------------5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. ----------------6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed.----------------All in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_450,https://openreview.net/forum?id=B1IzH7cxl,A Neural Stochastic Volatility Model,"In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones. Our focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.","A Neural Stochastic Volatility Model. The authors propose a recurrent neural network approach for constructing a--------stochastic volatility model for financial time series. They introduce an--------inference network based on a recurrent neural network that computes the--------approximation to the posterior distribution for the latent variables given the--------past data. This variational approximation is used to maximize the marginal--------likelihood in order to learn the parameters of the model. The proposed method--------is validated in experiments with synthetic and real-world time series, showing--------to outperform parametric GARCH models and a Gaussian process volatility model.----------------Quality:----------------The method proposed seems technically correct, with the exception that in--------equation (19) the inference model is doing filtering and not smoothing, in the--------sense that the posterior for z_t' only depends on those other z_t and x_t--------values with t<t', but in the true posterior p(Z|X) each z_t depends on all the--------X. This means the proposed learning method is inefficient. The reference below--------shows how to perform smoothing too and the results in that paper show indeed that--------smoothing produces better results for learning the model.----------------Sequential Neural Models with Stochastic Layers Fraccaro, Marco and S\o nderby,--------S\o ren Kaae and Paquet, Ulrich and Winther, Ole In NIPS 2016.----------------It is not clear if the method proposed in the above reference would perform better --------just because of using smoothing when learning the model parameters.----------------Clarity:----------------The paper is clearly written and easy to read.----------------For the results on real-world data, in Table 1, how is the NLL computed? Is the--------average NLL across the 162 time series?----------------Originality:----------------The method proposed is not very original. Previous work has already used the--------variational approach with the reparametrization trick to learn recurrent neural--------networks with stochastic units (see the reference above). It seems that the main--------contribution of the authors is to apply this type of techniques to the problem--------of modeling financial time series.----------------Significance:----------------The results shown are significant, the method proposed by the authors--------outperforms previous approaches. However, there is a huge amount of techniques--------available for modeling financial time-series. The number of GARCH variants is--------probably close to hundreds, each one claiming to be better than the others.--------This makes difficult to quantify how important the results are.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_451,https://openreview.net/forum?id=B1IzH7cxl,A Neural Stochastic Volatility Model,"In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones. Our focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.","A Neural Stochastic Volatility Model. Thank you for an interesting read.----------------I found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. ----------------To me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_464,https://openreview.net/forum?id=H1Gq5Q9el,Unsupervised Pretraining For Sequence To Sequence Learning,"This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then  fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models.  Our main result is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English->German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English->German. On summarization, our method beats the supervised learning baseline.","Unsupervised Pretraining For Sequence To Sequence Learning. In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.----------------While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.    Pros:  - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself.   - From an impact perspective, the reviewers found the approach clear and implementable.     Cons:  - Novelty criticisms are that the method is a ""compilation"" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are ""highly empirical"" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.  - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_465,https://openreview.net/forum?id=H1Gq5Q9el,Unsupervised Pretraining For Sequence To Sequence Learning,"This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then  fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models.  Our main result is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English->German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English->German. On summarization, our method beats the supervised learning baseline.","Unsupervised Pretraining For Sequence To Sequence Learning. strengths:----------------A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.----------------It is shown that pretraining accelerates training and improves generalization of seq2seq models.----------------The main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.------------------------weaknesses:----------------The objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.----------------The pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. ",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.    Pros:  - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself.   - From an impact perspective, the reviewers found the approach clear and implementable.     Cons:  - Novelty criticisms are that the method is a ""compilation"" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are ""highly empirical"" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.  - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_466,https://openreview.net/forum?id=H1Gq5Q9el,Unsupervised Pretraining For Sequence To Sequence Learning,"This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then  fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models.  Our main result is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English->German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English->German. On summarization, our method beats the supervised learning baseline.","Unsupervised Pretraining For Sequence To Sequence Learning. Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. ----------------The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.----------------The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.----------------You should probably give credit for encoder-decoder like-RNN models published in 1990s.----------------Minors:--------Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.    Pros:  - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself.   - From an impact perspective, the reviewers found the approach clear and implementable.     Cons:  - Novelty criticisms are that the method is a ""compilation"" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are ""highly empirical"" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.  - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_473,https://openreview.net/forum?id=ByG8A7cee,Reference-aware Language Models,"We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.","Reference-aware Language Models. This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.----------------Despite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses.----------------The first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right?----------------A related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).----------------To mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.----------------The other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.    Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_474,https://openreview.net/forum?id=ByG8A7cee,Reference-aware Language Models,"We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.","Reference-aware Language Models. This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences. While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:----------------1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.----------------2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.----------------3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).----------------Overall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_475,https://openreview.net/forum?id=ByG8A7cee,Reference-aware Language Models,"We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.","Reference-aware Language Models. This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.----------------The proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.----------------The empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.------------------------Finally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:--------- Eq. (1) is missing a sum over .--------- ""into the a decoder LSTM"" -> ""into the decoder LSTM""--------- ""denoted as his"" -> ""denoted as""--------- ""Surprising,"" -> ""Surprisingly,""--------- ""torkens"" -> ""tokens""--------- ""if follows that the next token"" -> ""the next token""--------- In the ""COREFERENCE BASED LANGUAGE MODEL"" sub-section, what does  denote?--------- In the sentence: ""The attribute of each column is denoted as cs_cp(z_{i,v} |s_{i,v}) = 1p(z_{i,v} |s_{i,v}) = 0$.--------- In the ""Table Pointer"" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.------------------------Other comments:--------- For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.--------- What's the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.--------- In the Related Work section, the following sentence is not quite accurate: ""For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly."". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_476,https://openreview.net/forum?id=rJ8Je4clg,Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening,"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time.  We evaluate the performance of our approach on the 49 games of the challenging Arcade  Learning Environment, and report significant improvements in both training time and accuracy.","Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening. In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. ----------------The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. ----------------Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method.----------------The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions:----------------First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. ----------------Second, it would be great if the authors could include some kind of theoretically analysis about the approach.----------------Finally, I would like to apologize for the late review.","9: Top 15% of accepted papers, strong accept",3: The reviewer is fairly confident that the evaluation is correct,"The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_477,https://openreview.net/forum?id=rJ8Je4clg,Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening,"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time.  We evaluate the performance of our approach on the 49 games of the challenging Arcade  Learning Environment, and report significant improvements in both training time and accuracy.","Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening. In this paper, a Q-Learning variant is proposed that aims at ""propagating"" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark.----------------The paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising.----------------This submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR:----------------1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called ""target network"" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method.----------------[Update: some of these concerns were addressed in OpenReview comments]----------------2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected ""ablative"" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say ""We believe that our method can be readily combined with other techniques developed for DQN"", however providing actual results showing this would have made the paper much stronger.----------------In conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits.----------------Minor comments:--------- Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper--------- The description of Q-Learning in section 3 says ""The estimated future reward is computed based on the current state s or a series of past states s_t if available."" I am not sure what you mean by ""a series of past states"", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way.--------- The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 ""In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training"")--------- In Fig. S1 the legend should not say ""10M"" since the plot is from 1M to 10M",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_478,https://openreview.net/forum?id=rJ8Je4clg,Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening,"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time.  We evaluate the performance of our approach on the 49 games of the challenging Arcade  Learning Environment, and report significant improvements in both training time and accuracy.","Learning To Play In A Day: Faster Deep Reinforcement Learning By Optimality Tightening. This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games.----------------I have a few suggestions for improving the paper:--------The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance.----------------It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds.----------------As mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version?----------------Also, a couple questions:--------- Do you think the performance of this method would continue to improve after 10M frames?--------- Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?","9: Top 15% of accepted papers, strong accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_488,https://openreview.net/forum?id=Bk0FWVcgx,Topology And Geometry Of Half-rectified Network Optimization,"The loss surface of deep neural networks has recently attracted interest  in the optimization and machine learning communities as a prime example of  high-dimensional non-convex problem. Some insights were recently gained using spin glass  models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.  In this work, we do not make any such approximation and study conditions  on the data distribution and model architecture that prevent the existence  of bad local minima. Our theoretical work quantifies and formalizes two  important folklore facts: (i) the landscape of deep linear networks has a radically different topology  from that of deep half-rectified ones, and (ii) that the energy landscape  in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.  The conditioning of gradient descent is the next challenge we address.  We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks.  Our empirical results show that these level sets remain connected throughout  all the learning phase, suggesting a near convex behavior, but they become  exponentially more curvy as the energy level decays, in accordance to what is observed in practice with  very low curvature attractors.","Topology And Geometry Of Half-rectified Network Optimization. This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.",2: Strong rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function.   I agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.  The relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_489,https://openreview.net/forum?id=Bk0FWVcgx,Topology And Geometry Of Half-rectified Network Optimization,"The loss surface of deep neural networks has recently attracted interest  in the optimization and machine learning communities as a prime example of  high-dimensional non-convex problem. Some insights were recently gained using spin glass  models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.  In this work, we do not make any such approximation and study conditions  on the data distribution and model architecture that prevent the existence  of bad local minima. Our theoretical work quantifies and formalizes two  important folklore facts: (i) the landscape of deep linear networks has a radically different topology  from that of deep half-rectified ones, and (ii) that the energy landscape  in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.  The conditioning of gradient descent is the next challenge we address.  We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks.  Our empirical results show that these level sets remain connected throughout  all the learning phase, suggesting a near convex behavior, but they become  exponentially more curvy as the energy level decays, in accordance to what is observed in practice with  very low curvature attractors.","Topology And Geometry Of Half-rectified Network Optimization. This paper studies the energy landscape of the loss function in neural networks.  It is generally clearly written and nicely provides intuitions for the results.  One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.  It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path.  It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent.  The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path.  Using this they show that the loss seems to become more nonconvex when the loss is smaller.  This is also quite interesting.----------------The work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function.  However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss.  I would have also appreciated a little more practical discussion of the bound in Theorem 2.4.  It is hard to tell whether this bound is tight enough to be practically relevant.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function.   I agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.  The relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_490,https://openreview.net/forum?id=Bk0FWVcgx,Topology And Geometry Of Half-rectified Network Optimization,"The loss surface of deep neural networks has recently attracted interest  in the optimization and machine learning communities as a prime example of  high-dimensional non-convex problem. Some insights were recently gained using spin glass  models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.  In this work, we do not make any such approximation and study conditions  on the data distribution and model architecture that prevent the existence  of bad local minima. Our theoretical work quantifies and formalizes two  important folklore facts: (i) the landscape of deep linear networks has a radically different topology  from that of deep half-rectified ones, and (ii) that the energy landscape  in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.  The conditioning of gradient descent is the next challenge we address.  We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks.  Our empirical results show that these level sets remain connected throughout  all the learning phase, suggesting a near convex behavior, but they become  exponentially more curvy as the energy level decays, in accordance to what is observed in practice with  very low curvature attractors.","Topology And Geometry Of Half-rectified Network Optimization. This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.----------------Pros:--------1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.--------2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. ----------------Cons:--------The results are very specific in both topology and geometry analysis.--------1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. --------2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.----------------With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function.   I agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.  The relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_491,https://openreview.net/forum?id=Bkbc-Vqeg,Learning Word-like Units From Joint Audio-visual Analylsis,"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.","Learning Word-like Units From Joint Audio-visual Analylsis. This paper is a follow-up on the NIPS 2016 paper ""Unsupervised learning of spoken language with visual context"", and does exactly what that paper proposes in its future work section: ""to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units"" using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this.----------------My main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.----------------Apart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_492,https://openreview.net/forum?id=Bkbc-Vqeg,Learning Word-like Units From Joint Audio-visual Analylsis,"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.","Learning Word-like Units From Joint Audio-visual Analylsis. This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects. The general this is a very interesting direction of research as it allows for a richer representation of data: regularizing visual signal with audio and visa versa. This allows for training of visual models from video, etc. ----------------A major concern is the amount of novelty between this work and the author's previous publication at NIPs 2016. The authors claim a more sophisticated architecture and indeed show an improvement in recall. However, the improvements are marginal, and the added complexity to the architecture is a bit ad hoc. Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.----------------Overall, I really like this direction of research, and encourage the authors to continue developing algorithms that can train from such multimodal datasets. However, the work isn't quite novel enough from NIPs 2016.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_493,https://openreview.net/forum?id=Bkbc-Vqeg,Learning Word-like Units From Joint Audio-visual Analylsis,"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.","Learning Word-like Units From Joint Audio-visual Analylsis. CONTRIBUTIONS --------This paper introduces a method for learning semantic ""word-like"" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.------------------------NOVELTY+SIGNIFICANCE--------As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).----------------However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.----------------The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.------------------------MISSING CITATION--------There is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:----------------Ngiam, et al. ""Multimodal deep learning."" ICML 2011------------------------POSITIVE POINTS--------- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval--------- The presented method performs efficient acoustic pattern discovery--------- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs----------------NEGATIVE POINTS--------- Limited novelty, especially compared with Harwath et al, NIPS 2016--------- Although it gives good results, the clustering method has limited novelty and feels heuristic--------- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_497,https://openreview.net/forum?id=Hy-lMNqex,Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied,  TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability. The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.----------------Comments:----------------1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.  ----------------2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units).----------------3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_498,https://openreview.net/forum?id=Hy-lMNqex,Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied,  TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability. Summary:----------------The paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.----------------Strengths:----------------Using variable precision for each layer of the network is useful - but was previously reported in Judd (2015)----------------Good evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)----------------Weaknesses:----------------The idea of combining bit-serial arithmetic with the DaDN architecture is a small one.----------------The authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.----------------The energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.----------------The authors don’t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.----------------Overall:----------------The Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_499,https://openreview.net/forum?id=Hy-lMNqex,Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied,  TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability. This seems like a reasonable study, though it's not my area of expertise. I found no fault with the work or presentation, but did not follow the details or know the comparable literature. ----------------There seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task. The tasks chosen (Alexnet / VGG) seem reasonable. The results are in simulation rather than in actual hardware.----------------The topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes ""hardware"".  I think the appeal among attendees will be rather limited. ----------------Please learn to use parenthetical references correctly. As is your references make reading harder. ",6: Marginally above acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_500,https://openreview.net/forum?id=Hy-lMNqex,Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied,  TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability. I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.",4: Ok but not good enough - rejection,1: The reviewer's evaluation is an educated guess,"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_501,https://openreview.net/forum?id=Hy-lMNqex,Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied,  TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","Tartan: Accelerating Fully-connected And Convolutional Layers In Deep Learning Networks By Exploiting Numerical Precision Variability. This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.----------------The first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. ----------------The second concern is the ""take-away for machine learning community"", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers. ",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_511,https://openreview.net/forum?id=S1VaB4cex,Fractalnet: Ultra-deep Neural Networks Without Residuals,"We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.","Fractalnet: Ultra-deep Neural Networks Without Residuals. Looking through the comment section here, I agree to a large degree with the author's standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don't think those contributions are groundbreaking, I believe they are significant enough to merit acceptance.----------------The reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments ""the experimental evaluation is not convincing, e.g. no improvement on SVHN"" or ""the effect of drop-path seems to vanish with data augmentation"" below.----------------I believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance.----------------Further, I believe the trend to focus excessively on performance is problematic for a number of reasons:---------------- - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar.---------------- - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true?---------------- - How does one even draw a ""fair comparison"" on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What's worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline ""against which the improvements can be clearly demonstrated by making isolated changes"" seems unrealistic to me.---------------- - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to ""training on the test set"", but also wastes the time of researcher that could be better spent exploring new ideas.---------------- - It gives too much power to bad research. In science, there is always a certain background rate of ""bad"" results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What's worse, these ""bad"" results are far more likely to hold the SOTA title at any given time than a ""good"" result. By requiring new publications to beat SOTA, we give too much power to bad results.---------------- - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don't think that was a coincidence. ----------------The same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting ""ultra-deep"" in the paper title seems exaggerated and I would recommend scaling back the language. However, I don't think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement.----------------In summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_512,https://openreview.net/forum?id=S1VaB4cex,Fractalnet: Ultra-deep Neural Networks Without Residuals,"We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.","Fractalnet: Ultra-deep Neural Networks Without Residuals. This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks.--------pros:--------- the idea is novel--------- the approach is described clearly--------cons:--------- the experimental evaluation is not convincing, e.g. no improvement on SVHN--------- number of parameters should be mentioned for all models for fair comparison--------- the effect of drop-path seems to vanish with data augmentation",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_513,https://openreview.net/forum?id=S1VaB4cex,Fractalnet: Ultra-deep Neural Networks Without Residuals,"We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.","Fractalnet: Ultra-deep Neural Networks Without Residuals. This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep” networks while residuals are incidental.----------------The main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep” networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.----------------Authors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.----------------DenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. ----------------Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.----------------Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. ----------------Although the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. ----------------Pros:--------Provides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis----------------cons:--------     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters--------The claims are intuitive but not supported well with empirical evidence--------Path regularization does not yield improvement when the data augmentation is used--------     -     The empirical results do not show whether the method is promising for “ultra-deep” networks ",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_514,https://openreview.net/forum?id=S1VaB4cex,Fractalnet: Ultra-deep Neural Networks Without Residuals,"We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.","Fractalnet: Ultra-deep Neural Networks Without Residuals. This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. ----------------This paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.----------------However, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.--------The number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:----------------- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.----------------- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.----------------- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].----------------- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.----------------- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.----------------Overall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. ----------------[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. ""Inception-v4, inception-resnet and the impact of residual connections on learning."" arXiv preprint arXiv:1602.07261 (2016).",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_518,https://openreview.net/forum?id=Hk6a8N5xe,Classify Or Select: Neural Architectures For Extractive Document Summarization,"We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to generate the extractive summary.   Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy.   We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.","Classify Or Select: Neural Architectures For Extractive Document Summarization. This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA.----------------Technical comments:----------------- In equation (1), there is a position-relevant component call ""positional importance"". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.--------- A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of  with respect to the whole document.--------- For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. --------- I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures.--------- It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (""only slightly better""). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting ""old"" problems to provide significant improvements in accuracy or novel modeling.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_519,https://openreview.net/forum?id=Hk6a8N5xe,Classify Or Select: Neural Architectures For Extractive Document Summarization,"We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to generate the extractive summary.   Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy.   We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.","Classify Or Select: Neural Architectures For Extractive Document Summarization. This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used ""pseudo-ground truth generation"" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.----------------The proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?--------In order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (""only slightly better""). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting ""old"" problems to provide significant improvements in accuracy or novel modeling.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_520,https://openreview.net/forum?id=Hk6a8N5xe,Classify Or Select: Neural Architectures For Extractive Document Summarization,"We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to generate the extractive summary.   Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy.   We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.","Classify Or Select: Neural Architectures For Extractive Document Summarization. This paper provides two RNN-based architectures for extractive document summarization. The first, ""Classify"", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, ""Select"",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. ----------------Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (""only slightly better""). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting ""old"" problems to provide significant improvements in accuracy or novel modeling.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_521,https://openreview.net/forum?id=ryZqPN5xe,Beyond Fine Tuning: A Modular Approach To Learning On Small Data,"In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.  ","Beyond Fine Tuning: A Modular Approach To Learning On Small Data. This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard “fine-tuning” approach.----------------Pros:---------The method is simple and clearly explained.---------Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.---------Experiments are performed in multiple domains -- vision and NLP.----------------Cons:---------The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the “stiched” network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.-----------------Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:--------(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.--------(2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never “sees” the source dataset is useful.-----------------The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I’ve never seen networks pre-trained on CIFAR deployed anywhere, and it’s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)----------------Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.----------------===============----------------Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the ""stitched network"" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (""ResNet+VGG > ResNet"" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_522,https://openreview.net/forum?id=ryZqPN5xe,Beyond Fine Tuning: A Modular Approach To Learning On Small Data,"In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.  ","Beyond Fine Tuning: A Modular Approach To Learning On Small Data. This paper proposed to perform finetuning in an augmentation fashion by freezing the original network and adding a new model aside it. The idea itself is interesting and complements existing training and finetuning approaches, although I think there are a few baseline approaches that can be compared against, such as:----------------(1) Ensemble: in principle, the idea is similar to an ensembling approach where multiple networks are ensembled together to get a final prediction. The approach in Figure 1 should be compared with such ensemble baselines - taking multiple source domain predictors, possibly with the same modular setting as the proposed method, and compare the performance.----------------(2) comparison with late fusion: if we combine the pretrained network and a network finetuned from the pretrained one, and do a late fusion?----------------Basically, I think it is a valuable argument in section 3.2 (and Figure 4) that finetuning with a small amount of data may hurt the performance in general. This builds the ground for freezing a pretrained network and only augmenting it, not changing it. I agree with the authors on this argument, although currently other than Figure 4 there seem to be little empirical study that justifies it.----------------It is worth noting that Figure 3 seems to suggest that some of the module filters are either not converging or are learning unuseful features - like the first two filters in 3(a).----------------Overall I think it is an interesting idea and I would love to see it better developed, thus I am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.",6: Marginally above acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_523,https://openreview.net/forum?id=ryZqPN5xe,Beyond Fine Tuning: A Modular Approach To Learning On Small Data,"In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.  ","Beyond Fine Tuning: A Modular Approach To Learning On Small Data. This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are “stitched” to the old one at various levels in the hieararchy, rather that it just being a parallel “tower”. ----------------This work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.----------------The idea of learning a “residual” with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.----------------I’ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?----------------In Figure 5 it’d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!----------------Figure 5 again: what is an untrained model? It’s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than “Retrain Softmax”?----------------I think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.----------------The authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there’s no evidence for that in the paper (in that we don’t have any visualizations of what happens if there’s no L2 reg.).----------------One of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.----------------Disappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It’s really hard to situate this paper if we don’t actually know how it compares to previously published results.------------------------In general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_539,https://openreview.net/forum?id=HJgXCV9xx,Dialogue Learning With Human-in-the-loop,"An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes.  Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach.","Dialogue Learning With Human-in-the-loop. As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle.--------Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting.----------------several points were raised that were in turn addressed by the authors:--------1. formalisation of the task (learning dialogue) is not precise. when can we declare success? --------The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision.----------------2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning.--------The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges.----------------3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?))--------The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal.----------------4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments.--------The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing.----------------There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston'16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"pros:  - demonstration that using teacher's feedback to improve performance in a dialogue system can be made to work    in a real-world setting  - comprehensive experiments    cons:  - lack of technical novelty due to prior work  - not all agree with the RL vs not-RL (pre-built datasets) distinction suggested in the paper with respect to the previous work    Overall, the paper makes a number of practical contributions and evaluation, rather than theoretical novelty.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_540,https://openreview.net/forum?id=HJgXCV9xx,Dialogue Learning With Human-in-the-loop,"An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes.  Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach.","Dialogue Learning With Human-in-the-loop. This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model’s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher’s feedback to significantly improve performance.----------------Overall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.----------------My main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:----------------“(i) That earlier work did not use the natural reinforcement learning/online setting, but “cheated” with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.”----------------Point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that “the model also works if we collect the data online (i.e. the agent’s policy is used to collect data rather than a fixed policy beforehand)”. While this is a step in the right direction, I’m not sure if it’s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. ----------------EDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"pros:  - demonstration that using teacher's feedback to improve performance in a dialogue system can be made to work    in a real-world setting  - comprehensive experiments    cons:  - lack of technical novelty due to prior work  - not all agree with the RL vs not-RL (pre-built datasets) distinction suggested in the paper with respect to the previous work    Overall, the paper makes a number of practical contributions and evaluation, rather than theoretical novelty.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_541,https://openreview.net/forum?id=HJgXCV9xx,Dialogue Learning With Human-in-the-loop,"An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes.  Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach.","Dialogue Learning With Human-in-the-loop. SUMMARY: This paper describes a set of experiments evaluating techniques for--------training a dialogue agent via reinforcement learning. A--------standard memory network architecture is trained on both bAbI and a version of--------the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous--------experiments are performed comparing the behavior of different training--------algorithms under various experimental conditions.----------------STRENGTHS: The experimentation is comprehensive. I agree with the authors that--------these results provide additional useful insight into the performance of the--------model in the 2016 paper (henceforth W16).----------------WEAKNESSES: This is essentially an appendix to the earlier paper. There is no--------new machine learning content. Secondarily, the paper seems to confuse the--------distinction between ""training with an adaptive sampling procedure"" and ""training--------in interactive environments"" more generally. In particular, no comparisons are--------presented to the to the experiments with a static exploration policy presented--------in W16, when the two training can & should be evaluated side-by-side.--------The only meaningful changes between this work and W16 involve simple--------(and already well-studied) changes to the form of this exploration policy.----------------My primary concern remains about novelty: the extra data introduced here is--------welcome enough, but probably belongs in a *ACL short paper or a technical--------report. This work does not stand on its own, and an ICLR submission is not an--------appropriate vehicle for presenting it.----------------""REINFORCEMENT LEARNING""----------------[Update: concerns in this section have been addressed by the authors.]----------------This paper attempts to make a hard distinction between the reinforcement--------learning condition considered here and the (""non-RL"") condition considered in--------W16. I don't think this distinction is nearly as sharp as it's--------made out to be. ----------------As already noted in Weston 2016, the RBI objective is a special case of vanilla--------policy gradient with a zero baseline and off-policy samples. In this sense the--------version of RBI considered in this paper is the same as in W16, but with a--------different exploration policy; REINFORCE is the same objective with a nontrivial--------baseline. Similarly, the change in FP is only a change to the sampling policy.--------The fixed dataset / online learning distinction is not especially meaningful--------when the fixed dataset consists of endless synthetic data.----------------It should be noted that some variants of the exploration policy in W16 provide a--------stronger training signal than is available in the RL ""from scratch"" setting--------here: in particular, when  the training samples will feature much--------denser reward. However, if I correctly understand Figures 3 and 4 in this paper,--------the completely random initial policy achieves an average reward of ~0.3 on bAbI--------and ~0.1 on movies---as good or better than the other exploration policies in--------W16!----------------I think this paper would be a lot clearer if the delta from W16 were expressed--------directly in terms of their different exploration policies, rather than trying to--------cast all of the previous work as ""not RL"" when it can be straightforwardly--------accommodated in the RL framework.----------------I was quite confused by the fact that no direct comparisons are made to the--------training conditions in the earlier work. I think this is a symptom of the--------problem discussed above: once this paper adopts the position that this work is--------about RL and the previous work is not, it becomes possible to declare that the--------two training scenarios are incomparable. I really think this is a mistake---to--------the extent that the off-policy sample generators used in the previous paper are--------worse than chance, it is always possible to compare to them fairly here.--------Evaluating everything in the ""online"" setting and presenting side-by-side--------experiments would provide a much more informative picture of the comparative--------behavior of the various training objectives.----------------ON-POLICY VS OFF-POLICY----------------Vanilla policy gradient methods like the ones here typically can't use--------off-policy samples without a little extra hand-holding (importance sampling,--------trust region methods, etc.). They seem to work out of the box for a few of the--------experiments in this paper, which is an interesting result on its own. It would--------be nice to have some discussion of why that might be the case.----------------OTHER NOTES----------------- The claim that ""batch size is related to off-policy learning"" is a little--------odd. There are lots of on-policy algorithms that require the agent to collect a--------large batch of transitions from the current policy before performing an --------(on-policy) update.----------------- I think the experiments on fine-tuning to human workers are the most exciting--------part of this work, and I would have preferred to see these discussed (and--------explored with) in much more detail rather than being relegated to the--------penultimate paragraphs.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"pros:  - demonstration that using teacher's feedback to improve performance in a dialogue system can be made to work    in a real-world setting  - comprehensive experiments    cons:  - lack of technical novelty due to prior work  - not all agree with the RL vs not-RL (pre-built datasets) distinction suggested in the paper with respect to the previous work    Overall, the paper makes a number of practical contributions and evaluation, rather than theoretical novelty.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_545,https://openreview.net/forum?id=S13wCE9xx,Riemannian Optimization For Skip-gram Negative Sampling,"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in ""word2vec"" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.","Riemannian Optimization For Skip-gram Negative Sampling. The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. ----------------The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_546,https://openreview.net/forum?id=S13wCE9xx,Riemannian Optimization For Skip-gram Negative Sampling,"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in ""word2vec"" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.","Riemannian Optimization For Skip-gram Negative Sampling. Dear authors,----------------The authors' response clarified some of my confusion. But I still have the following question:------------------ The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?----------------As far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. ------------------ Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.----------------Overall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_547,https://openreview.net/forum?id=S13wCE9xx,Riemannian Optimization For Skip-gram Negative Sampling,"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in ""word2vec"" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.","Riemannian Optimization For Skip-gram Negative Sampling. This paper presents a principled optimization method for SGNS (word2vec).----------------While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_548,https://openreview.net/forum?id=ryWKREqxx,Emergent Predication Structure In Vector Representations Of Neural Readers,"Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of “predication structure” in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a “predicate vector” P and a “constant symbol vector” c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating “aggregation readers” such as the Attentive Reader and the Stanford Reader to “explicit reference readers” such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.","Emergent Predication Structure In Vector Representations Of Neural Readers. The paper aims to consolidate some recent literature in simple types of ""reading comprehension"" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into ""aggregation readers"" and ""explicit reference readers."" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.----------------I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. ----------------The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the ""explicit reference readers"" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for ""dramatic improvements in performance"" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.----------------I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic ""reading"" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_549,https://openreview.net/forum?id=ryWKREqxx,Emergent Predication Structure In Vector Representations Of Neural Readers,"Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of “predication structure” in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a “predicate vector” P and a “constant symbol vector” c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating “aggregation readers” such as the Attentive Reader and the Stanford Reader to “explicit reference readers” such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.","Emergent Predication Structure In Vector Representations Of Neural Readers. This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: ""aggregation readers"" and ""explicit reference models”. Overall the quality of writing is great and section 3 was especially nice to read. I’m also happy with the proposed rewording from ""logical structure"" to “predication"", and the clarification by the authors was detailed and helpful.----------------I think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN/DailyMail dataset (Chen et al. ACL’16) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.----------------Second, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic ""reading"" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_550,https://openreview.net/forum?id=ryWKREqxx,Emergent Predication Structure In Vector Representations Of Neural Readers,"Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of “predication structure” in the hidden state vectors of a class of neural readers including the Attentive Reader and Stanford Reader. We posits that the hidden state vectors can be viewed as (a representation of) a concatenation [P, c] of a “predicate vector” P and a “constant symbol vector” c and that the hidden state represents the atomic formula P(c). This predication structure plays a conceptual role in relating “aggregation readers” such as the Attentive Reader and the Stanford Reader to “explicit reference readers” such as the Attention-Sum Reader, the Gated-Attention Reader and the Attention-over-Attention Reader. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What dataset.","Emergent Predication Structure In Vector Representations Of Neural Readers. The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.----------------I like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.----------------However, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the “structures”. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. ----------------I am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?----------------Overall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic ""reading"" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_557,https://openreview.net/forum?id=BkjLkSqxg,Lipnet: End-to-end Sentence-level Lipreading,"Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).","Lipnet: End-to-end Sentence-level Lipreading. The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.----------------Overall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,우리의 관심사는 우리의 가장 큰 주요 카지노에서 당신의 행운과 성공을위한 당신입니다. 【https://custory.com/theking/】 더킹카지노 정말 고마워.,3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_558,https://openreview.net/forum?id=BkjLkSqxg,Lipnet: End-to-end Sentence-level Lipreading,"Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).","Lipnet: End-to-end Sentence-level Lipreading. - Proven again that end to end training with deep networks gives large--------gains over traditional hybrid systems with hand crafted features. The results --------are very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.----------------- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.----------------- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,우리의 관심사는 우리의 가장 큰 주요 카지노에서 당신의 행운과 성공을위한 당신입니다. 【https://custory.com/theking/】 더킹카지노 정말 고마워.,3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_559,https://openreview.net/forum?id=BkjLkSqxg,Lipnet: End-to-end Sentence-level Lipreading,"Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).","Lipnet: End-to-end Sentence-level Lipreading. UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  ----------------I appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.----------------I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  ----------------************************----------------ORIGINAL REVIEW:----------------The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:----------------- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.----------------- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.----------------- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.----------------- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the ""importance of spatiotemporal feature extraction"" as stated in the conclusion.----------------Some more minor comments, typos, etc.:----------------- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.--------- I did not quite follow the justification for upsampling.--------- what is meant by ""lip-rounding vowels""?  They seem to include almost all English vowels.--------- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.--------- ""Given that the speakers are British, the confusion between /aa/ and /ay/..."" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).--------- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.--------- ""lipreading actuations"":  I am not sure what ""actuations"" means in this context--------- ""palato-alvealoar"" --> ""palato-alveolar""--------- ""Articulatorily alveolar"" --> ""Alveolar""?",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,우리의 관심사는 우리의 가장 큰 주요 카지노에서 당신의 행운과 성공을위한 당신입니다. 【https://custory.com/theking/】 더킹카지노 정말 고마워.,3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_560,https://openreview.net/forum?id=HyCRyS9gx,Fast Adaptation In Generative Models With Generative Matching Networks,"Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. Both problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data. So far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.   In this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent.  Our experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.","Fast Adaptation In Generative Models With Generative Matching Networks. This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It’s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. ----------------The proposed idea seems reasonable, but I’m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can’t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.)----------------I’m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it’s not clear what this represents. ----------------In terms of experimental validation, there aren’t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit. ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_561,https://openreview.net/forum?id=HyCRyS9gx,Fast Adaptation In Generative Models With Generative Matching Networks,"Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. Both problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data. So far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.   In this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent.  Our experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.","Fast Adaptation In Generative Models With Generative Matching Networks. This paper proposes an interesting idea for rapidly adapting generative models in the low data regime. The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks. To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset. Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks). The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples.----------------I think that the method is very interesting, however the major issue for me with this paper is a lack of clarity. I outline more details below, but overall I found the paper somewhat difficult to follow. There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.----------------I was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline.----------------There is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, \phi, sim and R would be nice. Otherwise it is very unclear to me what’s going on.----------------Section 3.2: “only state of the recurrent controller was used for matching”, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings?----------------“since the result was fully contrastive we did not apply any further binarization” what does it mean for a result to be fully contrastive?----------------For clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent.----------------Figure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns.----------------Table 1: is the VAE a fair baseline? You mention that Ctest affects Pd() in the evaluation. The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric. Can the authors clarify this? It’s important that the comparison is apples-to-apples.----------------MNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models.----------------Further, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? What happens at T=30? 40?----------------As a minor grammatical issue, the paper is missing determiners in several sentences. At one point, the model is referred to as “she” instead of “it”. “On figure 3” should be changed to “in figure 3” in the experiments section.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_562,https://openreview.net/forum?id=HyCRyS9gx,Fast Adaptation In Generative Models With Generative Matching Networks,"Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. Both problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data. So far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.   In this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent.  Our experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.","Fast Adaptation In Generative Models With Generative Matching Networks. The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting ""summary"" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.--------Perhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.--------The experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_574,https://openreview.net/forum?id=HysBZSqlx,Playing Snes In The Retro Learning Environment,"Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carriedout in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment — RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.","Playing Snes In The Retro Learning Environment. The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".----------------These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.----------------That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! ----------------Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.----------------I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers: http://www.jmlr.org/mloss/)------------------- Post response:----------------Thank you for the clarifications. Ultimately I have not changed my opinion on the paper. Though I do think RLE could have a nice impact long-term, there is little new science in this paper, ad it's either too straight-forward (reward shaping, policy-shaping) or not quite developed enough (rivalry training).",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_575,https://openreview.net/forum?id=HysBZSqlx,Playing Snes In The Retro Learning Environment,"Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carriedout in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment — RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.","Playing Snes In The Retro Learning Environment. This paper introduces a new reinforcement learning environment called « The Retro Learning Environment”, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari’s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.----------------I like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.----------------Besides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution ""A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI"", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.----------------Overall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.----------------Other small comments:--------- There are lots of typos (way too many to mention them all)--------- It is said that Infinite Mario ""still serves as a benchmark platform"", however as far as I know it had to be shutdown due to Nintendo not being too happy about it--------- ""RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE"" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?--------- Why is there no DQN / DDDQN result on Super Mario?--------- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not--------- The Du et al reference seems incomplete",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_576,https://openreview.net/forum?id=HysBZSqlx,Playing Snes In The Retro Learning Environment,"Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carriedout in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment — RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.","Playing Snes In The Retro Learning Environment. This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.----------------Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more, and when?----------------“rivalry” training: this is one of the weaker components of the paper, and it should probably be emphasised less. On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself). To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don’t think that you really invented “a new method to train an agent by enabling it to train against several opponents” nor “a new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI”). Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.----------------Your definition of Q-function (“predicts the score at the end of the game given the current state and selected action”) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).----------------Minor:--------* Eq (1): the Q-net inside the max() is the target network, with different parameters theta’--------* the Du et al. reference is missing the year--------* some of the other references should point at the corresponding published papers instead of the arxiv versions","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a new set of environments, similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research, but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper, such as perhaps a journal or workshop.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_586,https://openreview.net/forum?id=BJluGHcee,Tensorial Mixture Models,"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure.  In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.  The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e.,  the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.","Tensorial Mixture Models. This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.----------------If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra.   The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models.   The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.  However, the paper can be improved in two aspects:   (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: https://arxiv.org/abs/1606.05340  (2) It is also desirable if more extensive experiments are performed.    Given the improvements outlined for the paper, it does not meet the bar for acceptance at ICLR. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_587,https://openreview.net/forum?id=BJluGHcee,Tensorial Mixture Models,"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure.  In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.  The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e.,  the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.","Tensorial Mixture Models. This paper uses Tensors to build generative models.  The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.  Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.  ----------------This approach seems quite elegant.  It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.----------------The experiments are on simple, synthetic examples of missing data.  This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.  One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.  Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?  In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.----------------I was a little confused about how the input of missing data is handled experimentally.  From the introductory discussion my impression was that the generative model was built over region patches in the image.  This led me to believe that they would marginalize over missing regions.  However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.  Why is it appropriate to marginalize over missing pixels?  Specifically,  in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.  How is this done when only a subset of a region is missing?  It also seems like the summation in the equation following Equation 6 could be quite large.  What is the run time of this? ----------------The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.  The motivation for the probabilistic model is mostly in terms of images.  But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.  This would be more convincing if there were experiments outside the image domain.----------------It was also not clear to me how, if at all, the proposed network makes use of translation invariance.  It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.   Is such invariance built into the authors’ network?  If not, why would we expect it to work well in challenging image domains?----------------As a minor point, the paper is not carefully proofread.  To just give a few examples from the first page or so:----------------“significantly lesser” -> “significantly less”----------------“the the”----------------“provenly” -> provably","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra.   The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models.   The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.  However, the paper can be improved in two aspects:   (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: https://arxiv.org/abs/1606.05340  (2) It is also desirable if more extensive experiments are performed.    Given the improvements outlined for the paper, it does not meet the bar for acceptance at ICLR. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_588,https://openreview.net/forum?id=BJluGHcee,Tensorial Mixture Models,"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure.  In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.  The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e.,  the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.","Tensorial Mixture Models. The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:--------(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. --------(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. --------(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. ----------------Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). --------(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \sum_{d1,\ldots,d_N} P(d_1,\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. --------(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. --------(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when ""sum of product"" operation is equal to ""product of sum"" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.----------------Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra.   The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models.   The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.  However, the paper can be improved in two aspects:   (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: https://arxiv.org/abs/1606.05340  (2) It is also desirable if more extensive experiments are performed.    Given the improvements outlined for the paper, it does not meet the bar for acceptance at ICLR. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_589,https://openreview.net/forum?id=SJ-uGHcee,Efficient Iterative Policy Optimization,"We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.","Efficient Iterative Policy Optimization. This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.----------------I'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.----------------The description of the experiments in Section VI is insufficient for reproducibility. Is ""The cart moved right"" supposed to be ""a positive force is applied to the cart""? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?----------------The footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. --------https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf--------I might be missing something basic here.----------------The control variates thing seems cool. I only read up on it now and I don't think I've seen it before in the RL literature. Seems like a powerful tool.----------------Section 6.2 has too much business jargon, I could barely read it.",3: Clear rejection,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The reviewers generally agreed that exploring policy search methods of this type is interesting, but the results presented in the paper are not at the standard required for publication. There are no comparisons of any sort, and the only task that is tested is trivially simple, so it's impossible to conclude anything about the effectiveness of the method. Despite the author promising to add additional experiments, nothing was added in the current draft. Besides this, reviewers raised concerns about the relevance of this approach to ICLR. The crucial point here is that it's unclear if the method will scale -- while there is nothing wrong in principle in proposing a general policy search method, there isn't really a compelling argument that can be made that it is suitable for learning representations if there is no plausible story for how it will scale to sufficiently complex policy classes that can actually learn representations.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,1.0
2017_590,https://openreview.net/forum?id=SJ-uGHcee,Efficient Iterative Policy Optimization,"We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.","Efficient Iterative Policy Optimization. The paper considers the problem of reinforcement learning where the number of policy updates is required to be low. The problem is well motivated and the author provides an interesting modification to the PoWER algorithm, along with variational bounds on the value function (lemmas 3,4) which are interesting in themselves. They also provide numerical results on the cartpole problem and a problem in online advertising with real data. Overall this is a strong, well-written paper. My main reservation is whether it is completely appropriate for ICLR, since the log-concavity assumption the model relies on appear to restrict to simpler models where representations will be not in fact be learned.----------------Other comments:--------- There is a general lack of baselines in the numerical experiment section. I acknowledge this is somewhat of an unusual setting, but even a simple, well-justified baseline would have been welcome. Since cartpole is a relatively simple problem and the advertising dataset is presumably private, perhaps a way to generate a synthetic advertising dataset would have been interesting.----------------- I was confused by the control variates as constant scalars - are they meant to be constant baselines? And if so, they appear to be treated as hyperparameters -  why are they not learned or estimated?----------------- There is an interesting section on constrained optimization, but as it is, feels a bit disconnected from the rest of the paper. It appears applicable to the problem of online advertising, but is not mentioned in the corresponding experimental section. Also might be worth adding a citation to the literature of constrained MDPs which develops similar lagrangian ideas.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The reviewers generally agreed that exploring policy search methods of this type is interesting, but the results presented in the paper are not at the standard required for publication. There are no comparisons of any sort, and the only task that is tested is trivially simple, so it's impossible to conclude anything about the effectiveness of the method. Despite the author promising to add additional experiments, nothing was added in the current draft. Besides this, reviewers raised concerns about the relevance of this approach to ICLR. The crucial point here is that it's unclear if the method will scale -- while there is nothing wrong in principle in proposing a general policy search method, there isn't really a compelling argument that can be made that it is suitable for learning representations if there is no plausible story for how it will scale to sufficiently complex policy classes that can actually learn representations.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,0.0
2017_591,https://openreview.net/forum?id=SJ-uGHcee,Efficient Iterative Policy Optimization,"We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates.","Efficient Iterative Policy Optimization. The paper presents an interesting modification to PoWER algorithm that is well motivated. The main limitation of this paper is the lack of comparison with other methods and on richer problems. The experiments haven't given confidence to show its claimed benefits, generality and scalability over prior methods. Giving this confidence doesn’t necessarily require running your method on all large-scale domains or doing exhaustic hyper-parameter search, but for example it could go beyond current domains. Cartpole only optimizes 5 parameters. Ad targeting task lacks comparison with alternative methods. Since this method is built on PoWER and closely connected with RWR, it is likely there are limits to performance which may become apparent when the method is tried on other domains and with other benchmark methods, e.g. even standard ones like importance sampling-based off-policy learning is known to suffer in high-dimensional or continuous action space; limits of RWR/PoWER-like methods based on their connection with entropy-regularized RL. https://arxiv.org/abs/1604.06778 may be a valuable starting point for comparison, e.g. it compares RWR with policy gradient methods, and it has open-sourced codes. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers generally agreed that exploring policy search methods of this type is interesting, but the results presented in the paper are not at the standard required for publication. There are no comparisons of any sort, and the only task that is tested is trivially simple, so it's impossible to conclude anything about the effectiveness of the method. Despite the author promising to add additional experiments, nothing was added in the current draft. Besides this, reviewers raised concerns about the relevance of this approach to ICLR. The crucial point here is that it's unclear if the method will scale -- while there is nothing wrong in principle in proposing a general policy search method, there isn't really a compelling argument that can be made that it is suitable for learning representations if there is no plausible story for how it will scale to sufficiently complex policy classes that can actually learn representations.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Reject,1.0
2017_598,https://openreview.net/forum?id=S1LVSrcge,Variable Computation In Recurrent Neural Networks,"Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.","Variable Computation In Recurrent Neural Networks. TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., ""variable computation"") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.----------------=== Gating Mechanism ===--------At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.----------------This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.----------------A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.----------------=== Variable Computation ===--------One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.----------------=== Evaluation ===--------This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.----------------The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.----------------One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.----------------=== Minor ===--------* Please add Equations numbers to the paper, hard to refer to in a review and discussion!----------------References--------Chung et al., ""Hierarchical Multiscale Recurrent Neural Networks,"" in 2016.--------Graves et al., ""Adaptive Computation Time for Recurrent Neural Networks,"" in 2016.--------Wu et al., ""On Multiplicative Integration with Recurrent Neural Networks,"" in 2016.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.    Pros:  - new idea  - convincing results in a head to head comparison between different set ups.    Cons:  - results are not nearly as good as the state of the art on the reported tasks.    The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_599,https://openreview.net/forum?id=S1LVSrcge,Variable Computation In Recurrent Neural Networks,"Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.","Variable Computation In Recurrent Neural Networks. This is high novelty work, and an enjoyable read.----------------My concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)----------------In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.---------------------------2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.    Pros:  - new idea  - convincing results in a head to head comparison between different set ups.    Cons:  - results are not nearly as good as the state of the art on the reported tasks.    The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_600,https://openreview.net/forum?id=S1LVSrcge,Variable Computation In Recurrent Neural Networks,"Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.","Variable Computation In Recurrent Neural Networks. This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.----------------The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It’s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I’m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I’ll come back to this point in the next paragraph because it’s mainly a clarity issue.----------------The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it’s not clear from the text how the hyperparameter \bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don’t consider this a very serious flaw because I’m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.----------------To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper’s strongest points. ----------------It’s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it’s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. ----------------Pros:--------Original clever idea.--------Nice interesting visualizations.--------Interesting experiments.----------------Cons:--------Some experimental details are not clear.--------I’m not convinced of the strength of the baseline.--------The paper shouldn’t claim actual computational savings without reporting wall-clock times.----------------Edit:--------I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. ----------------Edit: --------Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.    Pros:  - new idea  - convincing results in a head to head comparison between different set ups.    Cons:  - results are not nearly as good as the state of the art on the reported tasks.    The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_601,https://openreview.net/forum?id=Hyanrrqlg,Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks,"As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.","Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks. The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks.----------------The authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair.  ----------------Since hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck.  Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_602,https://openreview.net/forum?id=Hyanrrqlg,Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks,"As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.","Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks. The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.----------------On the positive side,--------+ The proposed ideas are novel and seem useful.--------+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.--------+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.--------On the negative side,--------- The computation cost seems worse than HashedNets and is not discussed.--------- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.----------------That said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.----------------More comments:--------- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.--------- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.--------- For completeness, please add U1 results to Table 1.--------- In Table 1, U4-G3 is listed twice with two different numbers.--------- Some sentences are not grammatically correct. Please improve the writing.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_603,https://openreview.net/forum?id=Hyanrrqlg,Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks,"As the complexity of deep neural networks (DNNs) trends to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on homologically functional hashing to compress DNNs, shortly named as HFH. For each  weight entry in a deep net, HFH uses multiple low-cost hash functions to fetch values in a compression space, and then employs a small reconstruction network to recover that entry. The compression space is homological because all layers fetch hashed values from it. The reconstruction network is plugged into the whole network and trained jointly. On several benchmark datasets, HFH demonstrates high compression ratios with little loss on prediction accuracy. Particularly, HFH includes the recently proposed HashedNets as a degenerated scenario and shows significantly improved performance. Moreover, the homological hashing essence facilitates us to efficiently figure out one single desired compression ratio, instead of exhaustive searching throughout a combinatory space configured by all layers.","Hfh: Homologically Functional Hashing For Compressing Deep Neural Networks. The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each ""virtual"" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers). These components are then passed through a small MLP to synthesize the final weight.----------------This is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets. However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches. The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits. While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on. The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided. The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed). ----------------In response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits. If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance. However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea.",4: Ok but not good enough - rejection,"1. Can you provide any concrete numbers as to how much slower this approach is during inference? It seems like it would be quite substantial, since the weight values must be computed on the fly on each pass for memory savings to be realized.
2. There are many other techniques in the literature without such speed tradeoffs, including several mentioned in the paper such as the pruning work of Han et al. HFH seems to provide worse accuracy at the same compression ratio than these methods, while also having significant performance implications. I think the fact that HFH improves on HashedNets is an interesting finding, but are there circumstances where using it would be a good idea in practice?","This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_619,https://openreview.net/forum?id=SJMGPrcle,Learning To Navigate In Complex Environments,"Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision.","Learning To Navigate In Complex Environments. This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.----------------The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:      Pros:  + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel  + Good results on a challenge task of maze navigation from visual data    Cons:  - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple ""this is what worked for this domain""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_620,https://openreview.net/forum?id=SJMGPrcle,Learning To Navigate In Complex Environments,"Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision.","Learning To Navigate In Complex Environments. I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. ----------------I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:      Pros:  + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel  + Good results on a challenge task of maze navigation from visual data    Cons:  - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple ""this is what worked for this domain""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_621,https://openreview.net/forum?id=SJMGPrcle,Learning To Navigate In Complex Environments,"Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision.","Learning To Navigate In Complex Environments. This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. --------The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.----------------Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.--------Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.------------------------While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence.----------------One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.----------------Another downside is that the authors dismiss navigation literature as ""not RL"". I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance, the task of predicting depth from images). The idea is a nice one, and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc, and could be made more formal, but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:      Pros:  + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel  + Good results on a challenge task of maze navigation from visual data    Cons:  - Methodology does seem a bit ad-hoc, it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple ""this is what worked for this domain""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_622,https://openreview.net/forum?id=r1VGvBcxl,Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu,"We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C.","Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu. The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors' own implementation of A3C as well as to published reference scores.----------------The paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. ----------------I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: https://github.com/muupan/async-rl--------One or two plots like that would complete this paper very nicely.-------------------------------------------------------------------I appreciate the additional experiments included in the revised version of the paper. The learning speed comparison makes the paper more complete and I’m slightly revising my score to reflect that. Having said that, there is still no clear demonstration that the higher throughput of GA3C leads to consistently faster learning. With the exception of Pong, the training curves in Figure 6 seem to significantly underperform the original A3C results or even the open source implementation of A3C on Breakout and Space Invaders (https://github.com/muupan/async-rl).",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper suggests modifications to A3C - mainly by adding a system of queues to batch data - to obtain higher learning throughput on a GPU. The paper gives a solid systems analysis of the algorithm and performance. Although the method does not necessarily lead to higher Atari performance, it is a valuable and relevant contribution to the field, since it provides a stable, fast, open-source implementation of the currently best-performing deep RL algorithm.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_623,https://openreview.net/forum?id=r1VGvBcxl,Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu,"We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C.","Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu. This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU. And they perform various analysis to show the gained speedup.----------------Thanks the authors for the replying to the questions and adjusting the paper to make it more clear.--------It's an interesting modification the the original algorithm. And section 5 does a through analysis of gpu utilization on different configurations.--------The main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities. Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains. I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper suggests modifications to A3C - mainly by adding a system of queues to batch data - to obtain higher learning throughput on a GPU. The paper gives a solid systems analysis of the algorithm and performance. Although the method does not necessarily lead to higher Atari performance, it is a valuable and relevant contribution to the field, since it provides a stable, fast, open-source implementation of the currently best-performing deep RL algorithm.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_624,https://openreview.net/forum?id=r1VGvBcxl,Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu,"We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C.","Reinforcement Learning Through Asynchronous Advantage Actor-critic On A Gpu. This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. ----------------One caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.----------------I applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.----------------A disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"This paper suggests modifications to A3C - mainly by adding a system of queues to batch data - to obtain higher learning throughput on a GPU. The paper gives a solid systems analysis of the algorithm and performance. Although the method does not necessarily lead to higher Atari performance, it is a valuable and relevant contribution to the field, since it provides a stable, fast, open-source implementation of the currently best-performing deep RL algorithm.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_628,https://openreview.net/forum?id=H1GEvHcee,Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm,"Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.","Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm. The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.---------------- It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.-------- 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.-------- 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.-------- 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_629,https://openreview.net/forum?id=H1GEvHcee,Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm,"Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.","Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm. Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.----------------Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.----------------Con: --------Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.----------------On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.----------------This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_630,https://openreview.net/forum?id=H1GEvHcee,Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm,"Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.","Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm. The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.----------------This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. ----------------Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.----------------PROS:--------Introduces an energy function having the leaky-relu as an activation function--------Introduces a novel sampling procedure based on annealing the leakiness parameter--------Similar sampling scheme shown to outperform AIS----------------CONS:--------Results are somewhat out of date--------Missing experiments on binary datasets (more comparable to prior RBM work)--------Missing PCD baseline--------Cost of projection method",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_631,https://openreview.net/forum?id=H1GEvHcee,Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm,"Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.","Annealing Gaussian Into Relu: A New Sampling Strategy For Leaky-relu Rbm. This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. ------------------------Main comments:----------------The proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.----------------The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1<v<a2. But here, there is no constraint on the visible v, i.e. v \in (-inf to +inf). The model just yields a marginal distribution which has a region-wise energy function, that is, for different regions, the energy functions are different. The authors should be aware of the claims of connection to truncated Gaussian.----------------That the authors claims that the model proposed by Nair & Hinton (2010) has no strict monotonicity and thus cannot use the Ravanbakhsh’s framework is incorrect. Nair & Hinton’s model use max(0, x+n) to introduce a ReLU-like nonlinearity. The output expectation is actually a strict monotonic increasing function of x.----------------Besides Nair & Hinton’s work, there is also another closely-related work ‘Unsupervised learning with truncated Gaussian graphical model’, which introduces ReLU into RBM using truncated normal.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_635,https://openreview.net/forum?id=Sy8gdB9xx,Understanding Deep Learning Requires Rethinking Generalization,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.  We interpret our experimental findings by comparison with traditional models.","Understanding Deep Learning Requires Rethinking Generalization. This paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. --------Extensive experiments support the main argument of the paper. ----------------Reflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking.----------------While this work doesn’t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models,--------it does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood.----------------In my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field.----------------Detailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?","10: Top 5% of accepted papers, seminal paper",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"I expected to like this paper, because I respect the authors, and many people have said good things about it. In addition, I enthusiastically support experimental work to improve our understanding of how deep nets work. I'm sorry to say I was very disappointed.  I agree with Pirmin Lemberger that the results in this paper are completely unsurprising. I'm surprised that the authors were surprised. I'm shocked that at least one reviewer thought this was ground breaking.  Suppose you replace every occurrence of ""neural net"" in this paper with ""decision tree"". Would you still be surprised? Of course trees would be able to memorize the data. Of course the trees would need to be deeper for random labels than for random pixels (because random pixels provide more information for memorizing the data; shuffling the original pixels is just another way of generating random pixels). Of course regularizing the depth of the trees would not make them generalize well. The only difference is that trees would perform very poorly on the original data too, because they lack the right representations to generalize well.  Regularization only prevents overfitting. It does not ensure that there are good hypotheses in the hypothesis space. Your experiments with randomly parameterized convolutional layers show that they DO help ensure that there are good hypotheses in H.   I note in Figure 1 that it takes 5000 steps to fit the original data, and more than 13000 steps to fit random labels. This directly contradicts your statement that training did not slow down substantially. Training time nearly tripled.  It has long been known that the most effective way to control hypothesis complexity in neural networks is early stopping. Back in the 90s, the standard procedure was to use the biggest hidden layer you could afford, and then do early stopping. (IIRC this appears in the ""Tricks of the trade"" book https://books.google.com/books?isbn=3540494308).   In your experiment, if we apply early stopping at 5000 steps, we will get a non-trivial Radamacher complexity for this data set. On random labels, the accuracy will be near zero. Did you really not notice this?  I think it is interesting that you did not need to change your hyperparameters to get the network to fit random labels. That suggests that the hyperparameters might depend more on the architecture and not so much on the data. Have people seen this in other cases?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Oral),0.0
2017_636,https://openreview.net/forum?id=Sy8gdB9xx,Understanding Deep Learning Requires Rethinking Generalization,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.  We interpret our experimental findings by comparison with traditional models.","Understanding Deep Learning Requires Rethinking Generalization. This paper presents a set of experiments where via clever use of randomization and noise addition authors demonstrate the enormous modeling (memorization) power of the deep neural networks with large enough capacity.  Yet these same models have very good generalization behavior even when all obvious explicit or implicit regularizers are removed.  These observations are used to argue that classical theory (VC dimension, Rademacher complexity, uniform stability) is not able to explain the generalization behavior of deep neural networks, necessitating novel theory.----------------This is a very interesting and thought provoking body of work and I am in complete accord with the observations and conclusions of the paper.  The classical generalization theory is indeed often at a loss with complex enough model families.  As the authors point out, once model families reach a point where they have capacity to memorize train sets, the classical theory does not yield useful results that could give insight into generalization behavior of these models, leaving one to empirical studies and observations.----------------A minor clarification comment: On page 2, “ … true labels were replaced by random labels.”  Please state that random labels came from the same set as the true labels, to clarify the experiment.","9: Top 15% of accepted papers, strong accept",3: The reviewer is fairly confident that the evaluation is correct,"I expected to like this paper, because I respect the authors, and many people have said good things about it. In addition, I enthusiastically support experimental work to improve our understanding of how deep nets work. I'm sorry to say I was very disappointed.  I agree with Pirmin Lemberger that the results in this paper are completely unsurprising. I'm surprised that the authors were surprised. I'm shocked that at least one reviewer thought this was ground breaking.  Suppose you replace every occurrence of ""neural net"" in this paper with ""decision tree"". Would you still be surprised? Of course trees would be able to memorize the data. Of course the trees would need to be deeper for random labels than for random pixels (because random pixels provide more information for memorizing the data; shuffling the original pixels is just another way of generating random pixels). Of course regularizing the depth of the trees would not make them generalize well. The only difference is that trees would perform very poorly on the original data too, because they lack the right representations to generalize well.  Regularization only prevents overfitting. It does not ensure that there are good hypotheses in the hypothesis space. Your experiments with randomly parameterized convolutional layers show that they DO help ensure that there are good hypotheses in H.   I note in Figure 1 that it takes 5000 steps to fit the original data, and more than 13000 steps to fit random labels. This directly contradicts your statement that training did not slow down substantially. Training time nearly tripled.  It has long been known that the most effective way to control hypothesis complexity in neural networks is early stopping. Back in the 90s, the standard procedure was to use the biggest hidden layer you could afford, and then do early stopping. (IIRC this appears in the ""Tricks of the trade"" book https://books.google.com/books?isbn=3540494308).   In your experiment, if we apply early stopping at 5000 steps, we will get a non-trivial Radamacher complexity for this data set. On random labels, the accuracy will be near zero. Did you really not notice this?  I think it is interesting that you did not need to change your hyperparameters to get the network to fit random labels. That suggests that the hyperparameters might depend more on the architecture and not so much on the data. Have people seen this in other cases?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Oral),1.0
2017_637,https://openreview.net/forum?id=Sy8gdB9xx,Understanding Deep Learning Requires Rethinking Generalization,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.  We interpret our experimental findings by comparison with traditional models.","Understanding Deep Learning Requires Rethinking Generalization. the authors of this work shed light on the generalization properties of deep neural networks. Specifically, the consider various regularization methods (data augmentation, weight decay, and dropout). They also show that quality of the labels, namely label noise also significantly affects the generalization ability of the network.----------------There are a number of experimental results, most of which are intuitive. Here are some specific questions that were not addressed in the paper:--------1. Given two different DNN architectures with the same number of parameters, why do certain architectures generalize better than others? In other words, is it enough to consider only the size (# of parameters) of the network and the size of the input (number of samples and their dimensionality), to be able to reason about the generalization properties of a given network?----------------2. Does it make sense to study the stability of predictions given added dropout during inference?----------------Finally, provided a number of experiments and results, the authors do not draw a conclusion or offer a strong insight into what is going on with generalization in DNNs or how to proceed forward. ","10: Top 5% of accepted papers, seminal paper",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"I expected to like this paper, because I respect the authors, and many people have said good things about it. In addition, I enthusiastically support experimental work to improve our understanding of how deep nets work. I'm sorry to say I was very disappointed.  I agree with Pirmin Lemberger that the results in this paper are completely unsurprising. I'm surprised that the authors were surprised. I'm shocked that at least one reviewer thought this was ground breaking.  Suppose you replace every occurrence of ""neural net"" in this paper with ""decision tree"". Would you still be surprised? Of course trees would be able to memorize the data. Of course the trees would need to be deeper for random labels than for random pixels (because random pixels provide more information for memorizing the data; shuffling the original pixels is just another way of generating random pixels). Of course regularizing the depth of the trees would not make them generalize well. The only difference is that trees would perform very poorly on the original data too, because they lack the right representations to generalize well.  Regularization only prevents overfitting. It does not ensure that there are good hypotheses in the hypothesis space. Your experiments with randomly parameterized convolutional layers show that they DO help ensure that there are good hypotheses in H.   I note in Figure 1 that it takes 5000 steps to fit the original data, and more than 13000 steps to fit random labels. This directly contradicts your statement that training did not slow down substantially. Training time nearly tripled.  It has long been known that the most effective way to control hypothesis complexity in neural networks is early stopping. Back in the 90s, the standard procedure was to use the biggest hidden layer you could afford, and then do early stopping. (IIRC this appears in the ""Tricks of the trade"" book https://books.google.com/books?isbn=3540494308).   In your experiment, if we apply early stopping at 5000 steps, we will get a non-trivial Radamacher complexity for this data set. On random labels, the accuracy will be near zero. Did you really not notice this?  I think it is interesting that you did not need to change your hyperparameters to get the network to fit random labels. That suggests that the hyperparameters might depend more on the architecture and not so much on the data. Have people seen this in other cases?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Oral),0.0
2017_641,https://openreview.net/forum?id=rJq_YBqxx,Deep Character-level Neural Machine Translation By Learning Morphology,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.","Deep Character-level Neural Machine Translation By Learning Morphology. * Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.------------------------* Review:--------     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. --------     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?--------     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.--------                   --------* Some Requests:-------- -Can you add the size of the models to the Table 1? --------- Can you add some of the failure cases of your model, where the model failed to translate correctly?----------------* An Overview of the Review:----------------Pros:--------    - The paper is well written--------    - Extensive analysis of the model on various language pairs--------    - Convincing experimental results.    --------    --------Cons:--------    - The model is complicated.--------    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.--------    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.----------------[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work.     Pros:  - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.    Mixed:   - Some found the paper clear, praising it as a ""well-written paper"", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved  - Reviewers were also split on results. Some found the results quite ""compelling"" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work  - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR    Cons:  - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering.   - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_642,https://openreview.net/forum?id=rJq_YBqxx,Deep Character-level Neural Machine Translation By Learning Morphology,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.","Deep Character-level Neural Machine Translation By Learning Morphology. Update after reading the authors' responses & the paper revision dated Dec 21:--------I have removed the comment ""insufficient comparison to past work"" in the title & update the score from 3 -> 5.--------The main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of ""whether to continue from character-level states or using word-level states"". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.-------------------------------------This is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). ----------------Moreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.----------------One minor comment: annotate h_t in Figure 1.----------------[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation--------with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work.     Pros:  - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.    Mixed:   - Some found the paper clear, praising it as a ""well-written paper"", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved  - Reviewers were also split on results. Some found the results quite ""compelling"" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work  - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR    Cons:  - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering.   - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_643,https://openreview.net/forum?id=rJq_YBqxx,Deep Character-level Neural Machine Translation By Learning Morphology,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.","Deep Character-level Neural Machine Translation By Learning Morphology. The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks.  The quantitative results are quite good and the qualitative results are quite encouraging.----------------First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it’s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.----------------Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model.----------------On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5.----------------To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work.     Pros:  - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.    Mixed:   - Some found the paper clear, praising it as a ""well-written paper"", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved  - Reviewers were also split on results. Some found the results quite ""compelling"" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work  - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR    Cons:  - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering.   - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_653,https://openreview.net/forum?id=rJM69B5xx,Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension,"Deep learning has proven useful on many NLP tasks including reading comprehension. However it requires a lot of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model isn’t shown any training examples from the target domain however the results are promising if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.","Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension. First I would like to apologize for the delay in reviewing.----------------summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. ----------------Here is what I understand are their several experiments to transfer learning, but I am not 100% sure.--------1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)--------2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).--------3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)----------------I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?----------------Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.----------------Minor: unexplained acronyms: GRU, BT, CBT.--------benfits p. 2--------subsubset p. 6",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_654,https://openreview.net/forum?id=rJM69B5xx,Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension,"Deep learning has proven useful on many NLP tasks including reading comprehension. However it requires a lot of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model isn’t shown any training examples from the target domain however the results are promising if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.","Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension. This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.----------------This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.----------------Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. ----------------The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?----------------Unfortunately, there is not much to take-away from this paper.-------- ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_655,https://openreview.net/forum?id=rJM69B5xx,Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension,"Deep learning has proven useful on many NLP tasks including reading comprehension. However it requires a lot of training data which are not available in some domains of application. Hence we examine the possibility of using data-rich domains to pre-train models and then apply them in domains where training data are harder to get. Specifically, we train a neural-network-based model on two context-question-answer datasets, the BookTest and CNN/Daily Mail, and we monitor transfer to subsets of bAbI, a set of artificial tasks designed to test specific reasoning abilities, and of SQuAD, a question-answering dataset which is much closer to real-world applications. Our experiments show very limited transfer if the model isn’t shown any training examples from the target domain however the results are promising if the model is shown at least a few target-domain examples. Furthermore we show that the effect of pre-training is not limited to word embeddings.","Finding A Jack-of-all-trades: An Examination Of Semi-supervised Learning In Reading Comprehension. This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.----------------The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].----------------More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  ----------------[1] http://www.msmarco.org/dataset.aspx--------[2] https://datasets.maluuba.com/NewsQA--------[3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_656,https://openreview.net/forum?id=BkSqjHqxg,Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model,"In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs — as represented by their random walks — that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.","Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model. This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets.----------------Strength:--------1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. --------2, The paper is well organized.----------------Weakness:--------1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. --------2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings.----------------Detailed Questions:--------1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake.--------2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_657,https://openreview.net/forum?id=BkSqjHqxg,Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model,"In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs — as represented by their random walks — that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.","Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model. This paper studies the graph embedding problem by using the encoder-decoder method. The experimental study on real network data sets show the features extracted by the proposed model is good for classification.----------------Strong points of this paper:--------  1. The idea of using the methods from natural language processing to graph mining is quite interesting.--------  2. The organization of the paper is clear----------------Weak points of this paper:--------  1. Comparisons with state-of-art methods (Graph Kernels) is missing. --------  2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing. --------  3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.)--------  4. Presentation of the paper is weak. There are lots of typos and unclear statements. --------  5. The author mentioned about the graph kernel things, but in the experiment they didn't compare them. Also, only compare the classification accuracy by using the proposed method is not enough.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_658,https://openreview.net/forum?id=BkSqjHqxg,Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model,"In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs — as represented by their random walks — that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.","Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model. Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.----------------Paper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)----------------Unfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.",6: Marginally above acceptance threshold,1: The reviewer's evaluation is an educated guess,"The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_659,https://openreview.net/forum?id=BkSqjHqxg,Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model,"In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs — as represented by their random walks — that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.","Skip-graph: Learning Graph Embeddings With An Encoder-decoder Model. The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.----------------In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. ----------------An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))----------------To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.  ","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_669,https://openreview.net/forum?id=BkdpaH9ll,Boosting Image Captioning With Attributes,"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.","Boosting Image Captioning With Attributes. CONTRIBUTIONS--------This paper extends end-to-end CNN+RNN image captioning model with auxiliary attribute predictions. It proposes five variants of network architectures, which take the attribute/image features in alternating orders or at every timestamp. The attributes (i.e., 1,000 most common words on COCO) are obtained by attribute classifiers trained by a multiple instance learning approach. The experiment results indicated that having these attributes as input improves captioning performance on standard metrics, including BLEU, METEOR, ROUGE-L, CIDEr-D. ----------------NOVELTY + SIGNIFICANCE--------All five variants of the network architectures, shown in Fig. 1, have followed a standard seq-to-seq LSTM model. The differences between these variants come from two aspects: 1. the order of image/attribute inputs; 2. whether to input attribute/image features at each time step. No architectural changes have been added to the proposed model over a standard seq-to-seq model. The proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas. In particular, Fang et al. 2015 and You et al. 2016 have both used attributes for image captioning. This work has used the same multiple instance learning procedure as Fang et al. 2015 to train visual detectors for common words and used detector outputs as conditional inputs to a language model. In addition, the idea of using image feature as input to every RNN timestamp has been widely explored, for instance, in Donahue et al. 2015. The authors did not offer a clear explanation about the technical contribution of this work over these existing approaches.----------------CLARITY--------First, it is not clear to me which image features have been used by the baseline methods. As the baselines may rely on different image representations, the experiments would not offer a completely fair comparison. For example, the results of the attention-based models in Table 1 are directly copied from Xu et al., 2015, which were reported with Oxford VGG features, instead of GoogLeNet used by LSTM-A. Even if the baselines do not use the same types of features, it should at least be explicitly mentioned. Besides, the fact that the results of Table 1 and Table 2 are reported with different features (GoogLeNet v.s. ResNet) are not described clearly in the paper.----------------“We select 1,000 most common words on COCO…” How would this approach guarantee that the selected attributes have clear semantic meanings, as many words among the top 1000 would be stop words, abstract nouns, non-visual verbs, etc.? It would be interesting to perform some quantitative analysis to see whether these 1000-dimensional attribute vectors actually carry semantic meanings, or merely capture biases of word distributions from the ground-truth captions. One possible way to justify the importance of semantic attributes is to experiment with ground-truth attributes or attribute classifiers trained on annotated sources such as COCO-Attribute (Patterson et al. 2016) or Visual Genome (Krishna et al. 2016).----------------EXPERIMENTS--------It would be interesting to analyze how the behavior of the seq-to-seq model changes with respect to the additional attribute inputs. My hypothesis is that the model’s word choices will shift towards words with high scores. This experiments will help us understand how the model can take advantage of the auxiliary attribute inputs.----------------Since the attributes are selected as the most frequent words from COCO, it is likely for the model to overfit the metrics, such as BLEU, that rely on word matching. On the other hand, it has been shown that these automatic caption evaluation metrics do not necessarily correlate with human judgment. Therefore, I think it is necessary to conduct a human study to convince the readers the quality improvement of the proposed model is not caused by overfitting to metrics.----------------SUMMARY--------This paper demonstrates that image captioning can be improved by having attributes as auxiliary inputs. However, the model has minor novelty given existing work that has explored similar ideas. Besides, more analysis is necessary to demonstrate the semantic meanings of attributes. A human study is recommended to justify the actual performance improvement. Given these points to be improved, I would recommend rejecting this paper in its current form.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper investigates several ways of conditioning an image captioning model on image attributes. While the results are good, the novelty is too limited for the paper to be accepted.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_670,https://openreview.net/forum?id=BkdpaH9ll,Boosting Image Captioning With Attributes,"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.","Boosting Image Captioning With Attributes. The authors take a standard image captioning system and, in addition to the image, also condition the caption on a fixed vector of attributes. These attributes are the top 1,000 most common words trained with MIL as seen in Fang et al. 2015. The authors investigate 5 ways of plugging in the attributes vector for each image. More or less all of them turn out to work comparably (24 - 25.2 METEOR), but also a good amount better than a standard image captioning model (25.2 > 23.1 METEOR). At the time this approach was state of the art on the MS COCO leaderboard.----------------I am conflicted judging these kinds of application-heavy papers. It is clear that the technical execution is done relatively well, but there is little to take away or learn. I find it interesting and slightly surprising that you can boost image captioning results by appending these automatically-extracted attributes, but on the topic of how many people would find this relevant or how much additional work it can inspire I am not so sure.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper investigates several ways of conditioning an image captioning model on image attributes. While the results are good, the novelty is too limited for the paper to be accepted.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_671,https://openreview.net/forum?id=BkdpaH9ll,Boosting Image Captioning With Attributes,"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in \citep{Karpathy:CVPR15} when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.","Boosting Image Captioning With Attributes. The paper evaluates different variants to include attributes for caption generation. Attributes are automatically learned from descriptions as in [Fang et al., 2015].  ------------------------Strength:--------1. The paper discusses and evaluates different variants to incorporate attribute and image representation in a recurrent network.--------2. The paper reports improvements over state-of-the-art on the large-scale MS COCO image caption dataset.----------------Weaknesses:--------1. The technical novelty of the paper is limited; it is mainly a comparison of different variants to include attributes.--------2. While the exact way how attributes are used is different to prior work, the presented variants are not especially exiting.--------3. The reported metrics are known to not always correlate very well with human judgments.--------3.1. It would be good to additionally also report the SPICE (Anderson et al ECCV 2016) metric, which has shown good correlation w.r.t. human judgments.--------3.2. In contrast to the arguments of the authors during the discussion period, I believe a human evaluation is feasible at least on a subset of the test set. In my experience, most authors will share their generated sentences if they did not publish them. Also, a comparison between the different variants should be possible.--------4. Qualitative results:--------4.1. How do the different variants A1 to A5 compare? Is there a difference in the sentence between the different approaches?------------------------Other (minor/discussion points)--------- Page 8: “is benefited” -> benefits------------------------Summary:--------While the paper presents a valuable experimental evaluation with convincing results, the contribution w.r.t. to novelty and approach is limited.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper investigates several ways of conditioning an image captioning model on image attributes. While the results are good, the novelty is too limited for the paper to be accepted.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_690,https://openreview.net/forum?id=B1mAJI9gl,Towards Understanding The Invertibility Of Convolutional Neural Networks,"Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.","Towards Understanding The Invertibility Of Convolutional Neural Networks. The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.----------------In my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.----------------A deep neural network is fundamentally different from a single layer---it is the ""deep"" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.----------------For any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The hat{x} is reconstructed by hat{z}, and hat{z] is reconstructed from Wx. Here the x should be unknown, but the author seems to treated as knwon. The x goes through the convolution and activation to get h, then the maximal pooling is performed over h, i.e., max_pool(h). Here the problem is the x is unknown. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_691,https://openreview.net/forum?id=B1mAJI9gl,Towards Understanding The Invertibility Of Convolutional Neural Networks,"Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.","Towards Understanding The Invertibility Of Convolutional Neural Networks. The authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.----------------This is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.----------------A few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.----------------The relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.----------------Although it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).----------------Finally, the filter coherence measure must be defined either mathematically or with a proper reference.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The hat{x} is reconstructed by hat{z}, and hat{z] is reconstructed from Wx. Here the x should be unknown, but the author seems to treated as knwon. The x goes through the convolution and activation to get h, then the maximal pooling is performed over h, i.e., max_pool(h). Here the problem is the x is unknown. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_692,https://openreview.net/forum?id=B1mAJI9gl,Towards Understanding The Invertibility Of Convolutional Neural Networks,"Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.","Towards Understanding The Invertibility Of Convolutional Neural Networks. Summary of the paper----------------The paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.-------- ----------------Clarity:----------------- The paper is confusing wrt to standard notations in deep learning.----------------Comments:----------------The paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:----------------1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf).----------------2- The pooling operation is modified to be shrinkage operator, that keeps the maximum value in a block  and sets to the zero other values, hence the dimensionality of the pooled representation is the same of the un-pooled representations. Note that in that cases the locations of where the maximum happens is known. This is not the case in the ’standard’ max pooling definition. IHT does not map to a forward of a CNN as described in the paper. (see next point)----------------3- It maybe that the notations  used in the paper are implying some confusions wrt to the standard notations in deep learning. ----------------  - Let z be the standard  pooled representation of dimension ‘k’. --------  - Define U as the unpooling  operation U(z, locations of maximum) :=  M(Wx,k).--------Hence your model of inversion is assuming the knowledge of the ’standard’ pooling representation and the switches (max locations). Referring to M as a pooling operation is misleading and confusing, it is the ‘standard’ unpooling operation.-------- - Under this notations W^{\top} U(z, locations of maximum) is a sensible reconstruction algorithm , with the simplification of ignoring the RELU.--------- Under these notations,  IHT is similar to a backward of the encoding neural network not  a forward. It is known if you take the derivative with respect to the input in a neural network, you get the transpose convolution also known as ‘deconvolution’ (which is not a correct naming) , hence this IHT iteration is nothing else then the well known transposed convolution.----------------4- I suggest the authors to rewrite the paper taking into account standard definitions and notation in deep learning as discussed in point 3, and to clearly state that the recovery is done under the knowledge of the switches. ----------------=====--------After reading the authors rebuttal and the revisions , the reviewer maintains his main concerns with the paper. Many improvements are still needed for the paper to be ready to be published in ICLR, at the notation , presentation and the theory levels. --------===== ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The hat{x} is reconstructed by hat{z}, and hat{z] is reconstructed from Wx. Here the x should be unknown, but the author seems to treated as knwon. The x goes through the convolution and activation to get h, then the maximal pooling is performed over h, i.e., max_pool(h). Here the problem is the x is unknown. ",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_702,https://openreview.net/forum?id=BycCx8qex,Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks,"In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\em encoder} for downstream tasks and as a {\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.","Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks. The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. ----------------The paper contains two major parts: DRAGNN and demonstrations of its usages. ----------------Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.----------------In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability ""incorporate dynamic recurrent connections through the definition of the transition system"" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple ""software engineering with no inherent ""free things"" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are ""reimplementations of things in the literature"". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_703,https://openreview.net/forum?id=BycCx8qex,Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks,"In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\em encoder} for downstream tasks and as a {\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.","Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks. The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google’s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. ----------------The most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). ----------------The “raison d’etre,” in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I’d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. ----------------Overall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability ""incorporate dynamic recurrent connections through the definition of the transition system"" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple ""software engineering with no inherent ""free things"" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are ""reimplementations of things in the literature"". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_704,https://openreview.net/forum?id=BycCx8qex,Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks,"In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\em encoder} for downstream tasks and as a {\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.","Dragnn: A Transition-based Framework For Dynamically Connected Neural Networks. Overall, this is a nice paper. Developing a unifying framework for these newer--------neural models is a worthwhile endeavor.----------------However, it's unclear if the DRAGNN framework (in its current form) is a--------significant standalone contribution. The main idea is straightforward: use a--------transition system to unroll a computation graph. When you implement models in--------this way you can reuse code because modules can be mixed and matched. This is--------nice, but (in my opinion) is just good software engineering, not machine --------learning research.----------------Moreover, there appears to be little incentive to use DRAGNN, as there are no--------'free things' (benefits) that you get by using the framework. For example:----------------- If you write your neuralnet in an automatic differentiation library (e.g.,--------  tensorflow or dynet) you get gradients for 'free'.----------------- In the VW framework, there are efficiency tricks that 'the credit assignment--------  compiler' provides for you, which would be tedious to implement on your--------  own. There is also a variety of algorithms for training the model in a--------  principled way (i.e., without exposure bias).----------------I don't feel that my question about the limitations of the framework has been--------satisfactorily addressed. Let me ask it in a different way: Can you give me--------examples of a few models that I can't (nicely) express in the DRAGNN framework?--------What if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or--------http://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the--------dynamic programming components as transition units and (importantly) would it be--------efficient?---------------- disagree that the VW framework is orthogonal, it is a *competing* way to--------implement recurrent models. The main different to me appears to be that VW's--------imperative framework is more general, but less modular.----------------The experimental contribution seems useful as does the emphasis on how easy it--------is to incorporate multi-task learning.----------------Minor:----------------- It would be useful to see actual code snippets (possibly in an--------  appendix). Otherwise, its unclear how modular DRAGNN really are.----------------- The introduction states that (unlike seq2seq+attention) inference remains--------  linear. Is this *necessarily* the case? Users define a transition system that--------  is quadratic, just let attention be over all previous states. I recommend that--------  authors rephrase statement more carefully.----------------- It seems strange to use A() as in ""actions"", then use d as ""decision"" for its--------  elements.----------------- I recommend adding i as an argument to the definition of the recurrence--------  function r(s) to make it clear that it's the subset of previous states at time--------  i, otherwise it looks like an undefined variable. A nice terse option is to--------  write r(s_i).----------------- Real numbers should be \mathbb{R} not \mathcal{R}.----------------- It's more conventional to use t for a time-step instead of i.----------------- Example 2: ""52 feature embeddings"" -> did you mean ""52-DIMENSIONAL feature--------  embeddings""?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability ""incorporate dynamic recurrent connections through the definition of the transition system"" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple ""software engineering with no inherent ""free things"" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are ""reimplementations of things in the literature"". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_705,https://openreview.net/forum?id=r1aGWUqgg,Unsupervised Learning Of State Representations For Multiple Tasks,"We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.","Unsupervised Learning Of State Representations For Multiple Tasks. This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.----------------Positives:--------+ Gating to enable learning a joint representation--------+ Multi-task learning extended from a single task in prior work--------+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)----------------Negatives:--------- Parameters choice is arbitrary (w parameters)--------- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks--------- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.----------------I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_706,https://openreview.net/forum?id=r1aGWUqgg,Unsupervised Learning Of State Representations For Multiple Tasks,"We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.","Unsupervised Learning Of State Representations For Multiple Tasks. This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.----------------The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.----------------The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.----------------The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.----------------In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.----------------Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.----------------In summary, here are the pros and cons of this paper:--------Cons--------- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task--------- Only one experimental set-up that evaluates learned policy with multi-task state representation--------- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems--------Pros: --------- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches--------- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful--------- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches----------------Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.------------------------Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:----------------Approach:--------Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.----------------Experiments:--------One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).----------------Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.----------------If there are aliasing issues with the images, why not just use higher resolution images?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_707,https://openreview.net/forum?id=r1aGWUqgg,Unsupervised Learning Of State Representations For Multiple Tasks,"We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.","Unsupervised Learning Of State Representations For Multiple Tasks. The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.----------------there were several unclear issues:----------------1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?--------The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.----------------2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).--------The explanation of the authors did provide more details and more explicit information. ----------------3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.--------The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.----------------In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_714,https://openreview.net/forum?id=Sy2fzU9gl,Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.","Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework. The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question.----------------The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will  testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. ----------------This of course does not handle complex factors like ""type of legs"" but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!)----------------By complete luck, the authors may have hit upon something else of fundamental importance: the letter ""beta"" for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative ""energy"" and ""entropy"" and is  partially addressed in section 2.9 of reference [1]. The correct definition of ""generative temperature"" is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the ""correct"" physical model instead.----------------A question to the authors: creating a new metric (""disentanglement"") as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along  different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])?----------------[1] https://arxiv.org/pdf/1508.06585v5.pdf--------[2] https://arxiv.org/pdf/1511.02841v3.pdf","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_715,https://openreview.net/forum?id=Sy2fzU9gl,Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.","Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework. The paper proposes beta-VAE which strengthen the KL divergence between the recognition model and the prior to limit the capacity of latent variables while sacrificing the reconstruction error. This allows the VAE model to learn more disentangled representation. ----------------The main concern is that the paper didn't present any quantitative result on log likelihood estimation. On the quality of generated samples, although the beta-VAE learns disentangled representation, the generated samples are not as realistic as those based on generative adversarial network, e.g., InfoGAN. Beta-VAE learns some interpretable factors of variation, but it still remains unclear why it is a better (or more efficient) representation than that of standard VAE.----------------In experiment, what is the criteria for cross-validation on hyperparameter \beta?----------------There also exists other ways to limit the capacity of the model. The simplest way is to reduce the latent variable dimension. I am wondering how the proposed beta-VAE is a better model than the VAE with reduced, or optimal latent variable dimension.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_716,https://openreview.net/forum?id=Sy2fzU9gl,Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.","Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework. Summary--------===----------------This paper presents Beta-VAE, an augmented Variational Auto-Encoder which--------learns disentangled representations. The VAE objective is derived--------as an approximate relaxation of a constrained optimization problem where--------the constraint matches the latent code of the encoder to a prior.--------When KKT multiplier beta on this constraint is set to 1 the result is the--------original VAE objective, but when beta > 1 we obtain Beta-VAE, which simply--------increases the penalty on the KL divergence term. This encourages the model to--------learn a more efficient representation because the capacity of the latent--------representation is more limited by beta. The distribution of the latent--------representation is rewarded more when factors are independent because--------the prior (an isotropic Gaussian) encourages independent factors, so the--------representation should also be disentangled.----------------A new metric is proposed to evaluate the degree of disentanglement. Given--------a setting in which some disentangled latent factors are known, many examples--------are generated which differ in all of these factors except one. These examples--------are encoded into the learned latent representation and a simple classifier--------is used to predict which latent factor was kept constant. If the learned--------representation does not disentangle the constant factor then the classifier--------will more easily confuse factors and its accuracy will be lower. This--------accuracy is the final number reported.----------------A synthetic dataset of 2D shapes with known latent factors is created to--------test the proposed metric and Beta-VAE outperforms a number of baselines--------(notably InfoGAN and the semi-supervised DC-IGN).----------------Qualitative results show that Beta-VAE learns disentangled factors--------on the 3D chairs dataset, a dataset of 3D faces, and the celebA dataset--------of face images. The effect of varying Beta is also evaluated using the proposed--------metric and the latent factors learned on the 2D shapes dataset are explored--------in detail.------------------------Strengths--------===--------* Beta-VAE is simple and effective.----------------* The proposed metric is a novel way of testing whether ground truth factors--------of variation have been identified.----------------* There is extensive comparison to relevant baselines.------------------------Weaknesses--------===----------------* Section 3 describes the proposed disentanglement metric, however I feel--------I need to read the caption of the associated figure (I thank for adding--------that) and Appendix 4 to understand the metric intuitively or in detail.--------It would be easier to read this section if a clear intuition preceeded--------a detailed description and I think more space should be devoted to this--------in the paper.----------------* Appendix 4: Why was the bottom 50% of the resulting scores discarded?----------------* As indicated in pre-review comments, the disentanglement metric is similar--------to a measure of correlation between latent features. Could the proposed metric--------be compared to a direct measure of cross-correlation between latent factors--------estimated over the 2D shapes dataset?------------------------* The end of section 4.2 observes that high beta values result in low--------disentanglement, which suggests the most efficient representation is not--------disentangled. This seems to disagree with the intuition from the approach--------section that more efficient representations should be disentangled. It would--------be nice to see discussion of potential reasons for this disagreement.----------------* The writing is somewhat dense.------------------------Overall Evaluation--------===--------The core idea is novel, simple and extensive tests show that it is effective.--------The proposed evaluation metric is novel might come into broader use.--------The main downside to the current version of this paper is the presentation,--------which provides sufficient detail but could be more clear.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_717,https://openreview.net/forum?id=Sy2fzU9gl,Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.","Beta-vae: Learning Basic Visual Concepts With A Constrained Variational Framework. This paper proposes the beta-VAE, which is a reasonable but also straightforward generalization of the standard VAE. In particular, a weighting factor beta is added for the KL-divergence term to balance the likelihood and KL-divergence. Experimental results show that tuning this weighting factor is important for learning disentangled representations. A linear-classifier based protocol is proposed for measuring the quality of disentanglement. Impressive illustrations on manipulating latent variables are shown in the paper. ----------------Learning disentangled representations without supervision is an important topic. Showing the effectiveness of VAE for this task is interesting. Generalizing VAE with a weighting factor is straightforward (though reformulating VAE is also interesting), the main contribution of this paper is on the empirical side. ----------------The proposed protocol for measuring disentangling quality is reasonable. Establishing protocol is one important methodology contribution of this paper, but the presentation of Section 3 is still not good. Little motivation is provided at the beginning of Section 3. Figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation). Moreover, 1) z_diff appeared without any definition in the main text. 2) Use “decoding” for x~Sim(v,w) may make people confuse the ground truth sampling procedure w ith the trained decoder. ----------------The illustrative figures on traversing the disentangled factor are impressive, though image generation quality is not as good as InfoGAN (not the main point of this paper). However, 1) it will be helpful to discuss if the good disentangling quality only attribute to the beta factor and VAE framework. For example, the training data in this paper seems to be densely sampled for the visualized factors. Does the sampling density play a critical role? 2) Not too many qualitative results are provided for each experiment? Adding more figures (e.g., in appendix) to cover more factors and seeding images can strength the conclusions drawn in this paper. 3) Another detailed question related to the generalizability of the model: are the seeding image for visualizing faces from unseen subjects or subjects in the training set? (maybe I missed something here.)----------------Quantitative results are only presented for the synthesized 2D shape. What hinders this paper from reporting quantitative numbers on real data (e.g., the 2D and 3D face data)? One possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some well-defined factor to measure the quantitative performance. ----------------Quantitative performance is only measured by the proposed protocol. Since the effectiveness of the protocol is something the paper need to justify, reporting quantitative results using simpler protocol is helpful both for demonstrating the disentangling quality and for justifying the proposed protocol (consistency with other measurement). A simple experiment is facial identity recognition and pose estimation using disentangled features on a standard test set (like in Reed et al, ICML 2014). ----------------In Figure 6 (left), why ICA is worse than PCA for disentanglement? Is it due to the limitation of the ICA algorithm or some other reasons? ----------------In Figure 6 (right), what is “factor change accuracy”? According to Appendix A.4 (which is not referred to in the main text), it is the “Disentanglement metric score”. Is that right?--------If so Figure 6 (right) shows the reconstruction results for the best disentanglement metric score. Then, 1) how about random generation or traversing along a disentangled factor? 2) more importantly, how is the reconstruction/generation results when the disentanglement metric score is suboptimal. ----------------Overall, the results presented in this paper are very interesting, but there are many details to be clarified. Moreover, more quantitative results are also needed. I hope at least some of the above concerns can be addressed. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_728,https://openreview.net/forum?id=HkCjNI5ex,Regularizing Neural Networks By Penalizing Confident Output Distributions,"We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.","Regularizing Neural Networks By Penalizing Confident Output Distributions. The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks.----------------Comments:--------The paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable.--------The error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax?--------Although the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing.--------This might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition.--------More motivation is necessary for the proposed smoothing.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agreed that the idea proposed in this paper is sensible and possibly very useful, and that the experiments are thorough with good results. However, they share strong doubts regarding the novelty of the proposed approach. Hopefully the discussion will help the authors refine this work. With a more thorough discussion of related ideas, such as entropy maximization, within the machine learning literature and a careful placement of this idea within that context, this could be a strong submission to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_729,https://openreview.net/forum?id=HkCjNI5ex,Regularizing Neural Networks By Penalizing Confident Output Distributions,"We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.","Regularizing Neural Networks By Penalizing Confident Output Distributions. Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions.  This avoids overfitting.----------------I generally like this idea.  Regularizing the behavior of the model often makes more sense than regularizing its parameters.  After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior.  So one might be able to choose a more sensible prior over the behavior.  In other words, prefer parameters not because they are individually close to 0 but because they jointly lead to a distribution that is plausible or low-risk a priori.----------------Pro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5).----------------Pro: It's possible that this hasn't been well-explored yet in neural networks (not sure).----------------Pro: The experimental results look good.  So maybe everyone should use this kind of regularizer. ----------------Con: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning.  There are many, many papers that include a scaled entropy term in the optimization objective!  It's not just for reinforcement learning.  Please see the long list of connections in my pre-review questions / comments.  ----------------Con: Experimental results should always be accompanied by significance tests and error analysis.  Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?  Are the improvements robust across many different training sets?  What errors does your model fix, and what errors does it introduce?  ----------------Summary recommendation: Revise and resubmit.  ICLR has lots of submissions.  I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it.  Otherwise, there's a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agreed that the idea proposed in this paper is sensible and possibly very useful, and that the experiments are thorough with good results. However, they share strong doubts regarding the novelty of the proposed approach. Hopefully the discussion will help the authors refine this work. With a more thorough discussion of related ideas, such as entropy maximization, within the machine learning literature and a careful placement of this idea within that context, this could be a strong submission to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_730,https://openreview.net/forum?id=HkCjNI5ex,Regularizing Neural Networks By Penalizing Confident Output Distributions,"We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.","Regularizing Neural Networks By Penalizing Confident Output Distributions. The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation ""unigram"" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? ----------------The idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. ----------------The justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.----------------I could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? ----------------The strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. ----------------At present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. ----------------Typo:--------In related work: ""Penalizing entropy"" - you mean penalizing low entropy",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agreed that the idea proposed in this paper is sensible and possibly very useful, and that the experiments are thorough with good results. However, they share strong doubts regarding the novelty of the proposed approach. Hopefully the discussion will help the authors refine this work. With a more thorough discussion of related ideas, such as entropy maximization, within the machine learning literature and a careful placement of this idea within that context, this could be a strong submission to a future conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_761,https://openreview.net/forum?id=H1hoFU9xe,Generative Adversarial Networks For Image Steganography,"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in  steganographic applications.","Generative Adversarial Networks For Image Steganography. This paper proposes an interesting application of the GAN framework in steganography domain. In addition to the normal GAN discriminator, there is a steganalyser discriminator that receives the negative examples from the generator and positive examples from the generator images that contain a hidden payload. As a result, the generator, not only learn to generate realistic images by fooling the discriminator of the GAN, but also learn to be a secure container by fooling steganalyser discriminator. The method is tested by training an independent steganalyser S* on real images and generated images.----------------Given that in the ICLR community, not many people are familiar with the literature of steganography, I think this paper should have provided more context about how exactly this method can be used in practice, what are the related works on setganalysis-secure message embedding and probably a more thorough sets of experiments on more than one dataset.----------------The proposed SGAN framework (Figure 2) does make sense to me, and I think it is very general and can have more applications other than the steganography domain. But it is not clear to me why fooling the steganalyser discriminator S, necessarily mean that we can fool an independent discriminator S*?----------------Also I find it surprising that a different seed value, can make such a huge difference in the accuracy.----------------In short, the ideas of this paper are interesting and potentially useful, but I think the presentation of this paper should be improved so that it becomes more suitable for the ICLR and machine learning community.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper examines an application of that deviates from the usual applications presented at ICLR. The idea seems very interesting to the reviewers, but a number of reviewers had trouble really understanding why the proposed SGAN would be attractive for this problem, and this problem setup with the SGAN in general. Clearer concrete 'use case scenarios' and experimentation that helps clarify the precise application setting and the advantages of the SGAN formulation would help make this work more impactful on the community. Given the quality of other paper submitted to ICLR this year the reviewer scores are just short of the threshold for acceptance",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_762,https://openreview.net/forum?id=H1hoFU9xe,Generative Adversarial Networks For Image Steganography,"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in  steganographic applications.","Generative Adversarial Networks For Image Steganography. I found this paper very original and thought-provoking, but also a bit difficult to understand. It is very exciting to see a practical use case for image-generating GANs, with potentially meaningful benchmarks aside from subjective realism.----------------I found eq. 4 interesting because it introduces a potentially non-differentiable black-box function Stego(...) into the training of (S, G). Do you in fact backprop through the Stego function?----------------- For the train/test split, why is the SGAN trained on all 200k images? Would it not be cleaner to use the same splits for training SGAN as for ""steganalysis purposes""? Could this account for the sensitivity to random seed shown in table 2?--------- Sec. 5.3: ""Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm."". This experiment showed that SGAN can fool HUGO, but I do not see how it was ""tuned"" to deceive HUGO, or how it could be tuned in general for a particular steganalyzer.----------------Although S* seems to be fooled by the proposed method, in general for image generation the discriminator D is almost never fooled. I.e. contemporary GANs never converge to actually fooling the discriminator, even if they produce samples that sometimes fool humans. What if I created an additional steganalyzer S**(x) = S*(x) * D(x)? This I think would be extremely difficult to fool reliably because it requires realistic image generation.----------------After reading the paper several times, it is still a bit unclear to me how or why precisely one would use a trained SGAN. I think the paper could be greatly improved by detailing, step by step, the workflow of how a hypothetical user would use a trained SGAN. This description should be aimed at a reader who knows nothing or very little about steganography (e.g. most of ICLR attendees).",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper examines an application of that deviates from the usual applications presented at ICLR. The idea seems very interesting to the reviewers, but a number of reviewers had trouble really understanding why the proposed SGAN would be attractive for this problem, and this problem setup with the SGAN in general. Clearer concrete 'use case scenarios' and experimentation that helps clarify the precise application setting and the advantages of the SGAN formulation would help make this work more impactful on the community. Given the quality of other paper submitted to ICLR this year the reviewer scores are just short of the threshold for acceptance",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_763,https://openreview.net/forum?id=H1hoFU9xe,Generative Adversarial Networks For Image Steganography,"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in  steganographic applications.","Generative Adversarial Networks For Image Steganography. I reviewed the manuscript as of December 6th.----------------Summary:--------The authors build upon generative adversarial networks for the purpose of steganalysis -- i.e. detecting hidden messages in a payload. The authors describe a new model architecture in which a new element, a 'steganalyser' is added a training objective to the GAN model.----------------Major Comments:--------The authors introduce an interesting new direction for applying generative networks. That said, I think the premise of the paper could stand some additional exposition. How exactly would a SGAN method be employed? This is not clear from the paper. Why does the model require a generative model? Steganalysis by itself seems like a classification problem (i.e. a binary decision if there a hidden message?) Would you envision that a user has a message to send and does not care about the image (container) that it is being sent with? Or does the user have an image and the network generates a synthetic version of the image as a container and then hide the message in the container? Or is the SGAN somehow trained as a method for detecting hidden codes performed by any algorithm in an image? Explicitly describing the use-case would help with interpreting the results in the paper.----------------Additionally, the experiments and analysis in this paper is quite light as the authors only report a few steganalysis performance numbers in the tables (Table 1,2,3). A more extensive analysis seems warranted to explore the parameter space and provide a quantitative comparison with other methods discussed (e.g. HUGO, WOW, LSB, etc.) When is it appropriate to use this method over the others? Why does the seed effect the quality of results? Does a fixed seed correspond realistic scenario for employing this method?----------------Minor comments:--------- Is Figure 1 necessary?--------- Why does the seed value effect the quality of the predictive performance of the model?",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"This paper examines an application of that deviates from the usual applications presented at ICLR. The idea seems very interesting to the reviewers, but a number of reviewers had trouble really understanding why the proposed SGAN would be attractive for this problem, and this problem setup with the SGAN in general. Clearer concrete 'use case scenarios' and experimentation that helps clarify the precise application setting and the advantages of the SGAN formulation would help make this work more impactful on the community. Given the quality of other paper submitted to ICLR this year the reviewer scores are just short of the threshold for acceptance",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_767,https://openreview.net/forum?id=BkSmc8qll,Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes,"In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.","Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes. The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.----------------Pros:--------+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers--------+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions----------------Cons:--------- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?--------- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_768,https://openreview.net/forum?id=BkSmc8qll,Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes,"In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.","Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes. This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.----------------The model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).----------------The model is evaluated on a set of toy problems (the “babi task”) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  ----------------In terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., “softplus”), overloading notations (w_t, b…), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?…).----------------Overall it is very hard to put together all the pieces of this model(s), there is no code available and I’m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_769,https://openreview.net/forum?id=BkSmc8qll,Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes,"In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.",Dynamic Neural Turing Machine With Continuous And Discrete Addressing Schemes. The paper extends the NTM by a trainable memory addressing scheme.--------The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.----------------Pros:--------* Extension to NTM with trainable addressing.--------* Experiments with discrete addressing.--------* Experiments on bAbI QA tasks.----------------Cons:--------* Big gap to MemN2N and DMN+ in performance.--------* Code not available.--------* There could be more experiments on other real-world tasks.,"7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_776,https://openreview.net/forum?id=r1WUqIceg,Improving Stochastic Gradient Descent With Feedback,"In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.","Improving Stochastic Gradient Descent With Feedback. The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training. The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems.----------------I have the following concerns about the paper----------------- The proposed method is VARIANT to arbitrary shifts and scaling to the cost function.  ----------------- A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of d_t. I suspect 1/d_t just shrinks as an exponential decay from Figure 2.----------------- Three additional hyper-parameters: k, K, \beta_3.----------------Overall, I think the method has its fundamental flew and the paper offers very limited novelty. There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method. Furthermore, it is hard for me to follow Section 3.2. The writing quality and clarity of the method section can be further improved.   -------- ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_777,https://openreview.net/forum?id=r1WUqIceg,Improving Stochastic Gradient Descent With Feedback,"In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.","Improving Stochastic Gradient Descent With Feedback. The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve. Originality is somehow limited but the method appears to have a positive effect on neural network training. The paper is well written and illustrations are appropriate.----------------Pros:----------------- probably a more sophisticated scheduling technique than a simple decay term--------- reasonable results on the CIFAR dataset (although with comparably small neural network)----------------Cons:----------------- effect of momentum term would be of interest--------- the Adam reference doesn't point to the conference publications but only to arxiv--------- comparison to Adam not entirely conclusive",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_778,https://openreview.net/forum?id=r1WUqIceg,Improving Stochastic Gradient Descent With Feedback,"In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.","Improving Stochastic Gradient Descent With Feedback. As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, --------but --------1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)--------2) I don't buy ""Eve always converges"" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. ----------------To my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_791,https://openreview.net/forum?id=BJC_jUqxe,A Structured Self-attentive Sentence Embedding,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","A Structured Self-attentive Sentence Embedding. I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I'd like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I'd like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I'd also like to see performances on the dev sets or learning curves.----------------In the conclusion, the authors remark that ""attention mechanism reliefs the burden of LSTM"". If the 2D representations are effective in that aspect, I'd expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs  will be helpful.----------------Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM.----------------* Minor comments:--------Typos: netowkrs, toghter, performd--------Missing year for the citation of (Margarit & Subramaniam)--------In figure 3, attention plotswith and without penalization look similar.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"A solid paper about sentence representation learning using ""internal attention"" (representing sentences as a self-attention matrix rather than a vector). The idea is novel, well presented, and has potential applications to a variety of sequence-based problems. The main criticism in reviews was about minor points needing clarification, but I am suitably satisfied that these have been acknowledged and incorporated into the current revision. Reviewer 2 is concerned that the experiments do not fully support the claim that this model will improve end task performance over more standard attention mechanisms. Glancing over the paper, and in light of other reviews and discussion, I am not convinced that this objection should halt acceptance, but invite the authors to take this suggestion onboard for future work (or in the final version of their paper). This paper should be accepted to the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_792,https://openreview.net/forum?id=BJC_jUqxe,A Structured Self-attentive Sentence Embedding,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","A Structured Self-attentive Sentence Embedding. This paper proposes a method for representing sentences as a 2d matrix by utilizing a self-attentive mechanism on the hidden states of a bi-directional LSTM encoder. This work differs from prior work mainly in the 2d structure of embedding, which the authors use to produce heat-map visualizations of input sentences and to generate good performance on several downstream tasks.----------------There is a substantial amount of prior work which the authors do not appropriately address, some of which is listed in previous comments. The main novelty of this work is in the 2d structure of embeddings, and as such, I would have liked to see this structure investigated in much more depth. Specifically, a couple important relevant experiments would have been:----------------* How do the performance and visualizations change as the number of attention vectors (r) varies?--------* For a fixed parameter budget, how important is using multiple attention vectors versus, say, using a larger hidden state or embedding size?----------------I would recommend changing some of the presentation in the penalization term section. Specifically, the statement that ""the best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors"" runs somewhat counter to the authors' comments about this topic below.----------------In Fig. (2), I did not find the visualizations to provide particularly compelling evidence that the multiple attention vectors were doing much of interest beyond a single attention vector, even with penalization. To me this seems like a necessary component to support the main claims of this paper.----------------Overall, while I found the architecture interesting, I am not convinced that the model's main innovation -- the 2d structure of the embedding matrix -- is actually doing anything important or meaningful beyond what is being accomplished by similar attentive embedding models already present in the literature. Further experiments demonstrating this effect would be necessary for me to give this paper my full endorsement.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"A solid paper about sentence representation learning using ""internal attention"" (representing sentences as a self-attention matrix rather than a vector). The idea is novel, well presented, and has potential applications to a variety of sequence-based problems. The main criticism in reviews was about minor points needing clarification, but I am suitably satisfied that these have been acknowledged and incorporated into the current revision. Reviewer 2 is concerned that the experiments do not fully support the claim that this model will improve end task performance over more standard attention mechanisms. Glancing over the paper, and in light of other reviews and discussion, I am not convinced that this objection should halt acceptance, but invite the authors to take this suggestion onboard for future work (or in the final version of their paper). This paper should be accepted to the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_793,https://openreview.net/forum?id=BJC_jUqxe,A Structured Self-attentive Sentence Embedding,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","A Structured Self-attentive Sentence Embedding. This paper introduces a sentence encoding model (for use within larger text understanding models) that can extract a matrix-valued sentence representation by way of within-sentence attention. The new model lends itself to (slightly) more informative visualizations than could be gotten otherwise, and beats reasonable baselines on three datasets.----------------The paper is reasonably clear, I see no major technical issues, and the proposed model is novel and effective. It could plausibly be relevant to sequence modeling tasks beyond NLP. I recommend acceptance.----------------There is one fairly serious writing issue that I'd like to see fixed, though: The abstract, introduction, and related work sections are all heavily skewed towards unsupervised learning. The paper doesn't appear to be doing unsupervised learning, and the ideas are no more nor less suited to unsupervised learning than any other mainstream ideas in the sentence encoding literature.----------------Details:--------- You should be clearer about how you expect these embeddings to be used, since that will be of certain interest to anyone attempting to use the results of this work. In particular, how you should convert the matrix representation into a vector for downstream tasks that require one. Some of the content of your reply to my comment could be reasonably added to the paper.--------- A graphical representation of the structure of the model would be helpful.--------- The LSTMN (Cheng et al., EMNLP '16) is similar enough to this work that an explicit comparison would be helpful. Again, incorporating your reply to my comment into the paper would be more than adequate. --------- Jiwei Li et al. (Visualizing and Understanding Neural Models in NLP, NAACL '15) present an alternative way of visualizing the influence of words on sentence encodings without using cross-sentence attention. A brief explicit comparison would be nice here.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"A solid paper about sentence representation learning using ""internal attention"" (representing sentences as a self-attention matrix rather than a vector). The idea is novel, well presented, and has potential applications to a variety of sequence-based problems. The main criticism in reviews was about minor points needing clarification, but I am suitably satisfied that these have been acknowledged and incorporated into the current revision. Reviewer 2 is concerned that the experiments do not fully support the claim that this model will improve end task performance over more standard attention mechanisms. Glancing over the paper, and in light of other reviews and discussion, I am not convinced that this objection should halt acceptance, but invite the authors to take this suggestion onboard for future work (or in the final version of their paper). This paper should be accepted to the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_794,https://openreview.net/forum?id=r1Bjj8qge,Encoding And Decoding Representations With Sum- And Max-product Networks,"Sum-Product networks (SPNs) are expressive deep architectures for representing probability distributions, yet allowing exact and efficient inference. SPNs have been successfully applied in several domains, however always as black-box distribution estimators. In this paper, we argue that due to their recursive definition, SPNs can also be naturally employed as hierarchical feature extractors and thus for unsupervised representation learning. Moreover, when converted into Max-Product Networks (MPNs), it is possible to decode such representations back into the original input space. In this way, MPNs can be interpreted as a kind of generative autoencoder, even if they were never trained to reconstruct the input data. We show how these learned representations, if visualized, indeed correspond to ""meaningful parts"" of the training data. They also yield a large improvement when used in structured prediction tasks. As shown in extensive experiments, SPN and MPN encoding and decoding schemes prove very competitive  against the ones employing RBMs and other stacked autoencoder architectures.","Encoding And Decoding Representations With Sum- And Max-product Networks. This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks.  The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings.----------------Pros:---------The idea is interesting and interpretable models/representations is an important topic.---------Generating embeddings to interpret SPNs is a novel idea.---------The experiments are interesting but could be extended.----------------Cons:---------The author's contribution isn't fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? -----------------A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade.----------------I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.    A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.    The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.    The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_795,https://openreview.net/forum?id=r1Bjj8qge,Encoding And Decoding Representations With Sum- And Max-product Networks,"Sum-Product networks (SPNs) are expressive deep architectures for representing probability distributions, yet allowing exact and efficient inference. SPNs have been successfully applied in several domains, however always as black-box distribution estimators. In this paper, we argue that due to their recursive definition, SPNs can also be naturally employed as hierarchical feature extractors and thus for unsupervised representation learning. Moreover, when converted into Max-Product Networks (MPNs), it is possible to decode such representations back into the original input space. In this way, MPNs can be interpreted as a kind of generative autoencoder, even if they were never trained to reconstruct the input data. We show how these learned representations, if visualized, indeed correspond to ""meaningful parts"" of the training data. They also yield a large improvement when used in structured prediction tasks. As shown in extensive experiments, SPN and MPN encoding and decoding schemes prove very competitive  against the ones employing RBMs and other stacked autoencoder architectures.",Encoding And Decoding Representations With Sum- And Max-product Networks. The paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL. Well - this is an interesting perspective and could be (potentially) worth a paper. ----------------However----------------- the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now. The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far. ----------------- the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight. I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so----------------So in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference. ----------------As for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper. ,3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.    A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.    The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.    The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_796,https://openreview.net/forum?id=r1Bjj8qge,Encoding And Decoding Representations With Sum- And Max-product Networks,"Sum-Product networks (SPNs) are expressive deep architectures for representing probability distributions, yet allowing exact and efficient inference. SPNs have been successfully applied in several domains, however always as black-box distribution estimators. In this paper, we argue that due to their recursive definition, SPNs can also be naturally employed as hierarchical feature extractors and thus for unsupervised representation learning. Moreover, when converted into Max-Product Networks (MPNs), it is possible to decode such representations back into the original input space. In this way, MPNs can be interpreted as a kind of generative autoencoder, even if they were never trained to reconstruct the input data. We show how these learned representations, if visualized, indeed correspond to ""meaningful parts"" of the training data. They also yield a large improvement when used in structured prediction tasks. As shown in extensive experiments, SPN and MPN encoding and decoding schemes prove very competitive  against the ones employing RBMs and other stacked autoencoder architectures.","Encoding And Decoding Representations With Sum- And Max-product Networks. The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.----------------This paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:----------------1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.--------2. They propose how to decode MPN's with partial data.--------3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.--------4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.----------------My main concerns with this paper are as follows:----------------- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. ----------------- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?----------------- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? ----------------- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.----------------I *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:----------------Input | Predicted Output | Decoder | Hamming | Exact Match--------------------X | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)--------SPN E_X | P(Y) | n/a | xx.xx | xx.xx --------X | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)----------------Does a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.    A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.    The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.    The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_803,https://openreview.net/forum?id=SJc1hL5ee,Fasttext.zip: Compressing Text Classification Models,"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.","Fasttext.zip: Compressing Text Classification Models. The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.----------------The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.----------------The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models?----------------Given the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR.----------------More comments:--------- The title does not make it clear that the paper focuses on wide and shallow text classification models. Please revise the title.--------- The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper. Please make the paper self-contained and include any supplementary material in the appendix.--------- In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.--------- The paper argues the wide and shallow models are the state of the art in small datasets. Is this really correct? What about transfer learning?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_804,https://openreview.net/forum?id=SJc1hL5ee,Fasttext.zip: Compressing Text Classification Models,"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.","Fasttext.zip: Compressing Text Classification Models. The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.----------------As such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective. However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_805,https://openreview.net/forum?id=SJc1hL5ee,Fasttext.zip: Compressing Text Classification Models,"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.","Fasttext.zip: Compressing Text Classification Models. This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.----------------This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.----------------The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:--------  - a straightforward variant of PQ for unnormalized vectors,--------  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,--------  - hashing tricks and bloom filter are simply borrowed from previous papers.----------------These techniques are quite generic and could as well be used in other works. ------------------------Here are some minor problems with the paper:----------------  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).--------  --------  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.--------  ----------------Overall this looks like a solid work, but with potentially limited impact research-wise.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_806,https://openreview.net/forum?id=ryPx38qge,A Hybrid Network: Scattering And Convnet,"This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.","A Hybrid Network: Scattering And Convnet. In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.----------------I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.----------------Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.----------------If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.----------------Summary: ----------------+ An interesting approach is presented that might be useful for real-world limited data scenarios.--------+ Limited data results look promising.--------- Adversarial examples are not investigated in the experimental section.--------- No realistic small-data problem is addressed.----------------Minor:--------- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.--------- Some typos: tacke, developping, learni.----------------[1] https://arxiv.org/abs/1610.00768v3--------[2] https://arxiv.org/abs/1511.04599","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_807,https://openreview.net/forum?id=ryPx38qge,A Hybrid Network: Scattering And Convnet,"This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.","A Hybrid Network: Scattering And Convnet. The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime.----------------I have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization.----------------For the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion.----------------The authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. ----------------In conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper.----------------Minor comments:---------section 3.1.2: “learni”",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_808,https://openreview.net/forum?id=ryPx38qge,A Hybrid Network: Scattering And Convnet,"This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.","A Hybrid Network: Scattering And Convnet. Thanks a lot for your detailed response and clarifications.----------------The paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines.----------------I find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point.----------------The paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. ----------------The author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_809,https://openreview.net/forum?id=By5e2L9gl,Trusting Svm For Piecewise Linear Cnns,"We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.","Trusting Svm For Piecewise Linear Cnns. This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. ----------------Pros:----------------- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.--------- The paper is well-written and easy to follow. ----------------Cons:----------------- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. -------- --------- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, ----------------Other comments:----------------- If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM.     Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.    Thus, I recommend this paper be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_810,https://openreview.net/forum?id=By5e2L9gl,Trusting Svm For Piecewise Linear Cnns,"We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.","Trusting Svm For Piecewise Linear Cnns. A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.----------------Summary:--------———--------I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.----------------Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.--------Clarity: Some of the derivations and intuitions could be explained in more detail.--------Originality: The suggested idea is reasonable albeit heuristics are required.--------Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.----------------Details:--------————--------1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.----------------2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016; which should probably be mentioned.----------------3. I think the comparison between backprop and the discussed CCCP approach is not really appropriate. Note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. This means that backprop combined with Armijo line-search would for example result in convergence guarantees. Hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next’ is according to my opinion superficial.----------------4. More justification regarding the argument that search for the step-size is a disadvantage seems necessary. There is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. In contrast the proposed CCCP procedure may converge pre-maturely to a local optimum close to the initialization. Since mini-batches are used no guarantees are available in any case. Hence I think additional evidence for the fact that such convergence guarantees don’t result in premature stopping seems necessary.----------------5. The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. Note that you can easily construct examples where Eq. (9) and Eq. (7) differ significantly.----------------6. I personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. E.g., LW-SVM always improves over the solution of the SGD algorithm. If the authors were to use backtracking line search they could also improve upon LW-SVM.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM.     Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.    Thus, I recommend this paper be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_811,https://openreview.net/forum?id=By5e2L9gl,Trusting Svm For Piecewise Linear Cnns,"We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.","Trusting Svm For Piecewise Linear Cnns. This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.----------------Overall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.----------------Of course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).----------------The experiment is a bit weak.--------1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.----------------2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM.     Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.    Thus, I recommend this paper be accepted.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_821,https://openreview.net/forum?id=Bk_zTU5eg,Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners),"Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and large mini-batches (given each instance processes equal number of training samples), while providing experimental evidence for the same. Going further to distributed settings, we show that an analogous effect holds with popular Asynchronous Gradient Descent (\asgd): there can be a large difference between convergence guarantees with increasing number of learners given that the cumulative number of training samples processed remains the same. Thus there is an inherent (and similar) inefficiency introduced in the convergence behavior when we attempt to take advantage of parallelism, either by increasing mini-batch size or by increase the number of learners.",Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners). This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller number of learners. This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware. This paper looks good overall and makes some connection between algorithm design and hardware properties.----------------My main concern is that Lemma 1 looks incorrect to me. The factor D_f / S should be D_f/ (S*M) for me. Please clarify this and check the subsequent theorem.,6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work addresses the question of whether mini-batching improves the convergence of stochastic gradient methods, in terms of the number of examples, in the general non-asymptotic/non-convex setting of Ghadimi and Lan. Similar results are already known (at least as folk theory) in simpler regimes, but this result is novel. The reviewers are mixed in their opinions of the paper, but AnonReviewer4 brings up a point that deserves further attention: the analysis assumes that the same step-size is used for different mini-batch sizes. Because of this assumption, the conclusion of the paper is misleading. One of the effects of increasing the mini-batch is to decrease the variance of the estimator, which allows a larger step-size. With this larger step-size the net effect of increasing the mini-batch can actually be positive even in terms of the number of examples. Fixing the step-size removes the chance for the mini-batch strategy to do better in theory, and there is indeed a large amount of empirical evidence that mini-batches can lead to practical improvements. I do think the authors should continue this work, but I think accepting the current version is problematic because of the current conclusion.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_822,https://openreview.net/forum?id=Bk_zTU5eg,Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners),"Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and large mini-batches (given each instance processes equal number of training samples), while providing experimental evidence for the same. Going further to distributed settings, we show that an analogous effect holds with popular Asynchronous Gradient Descent (\asgd): there can be a large difference between convergence guarantees with increasing number of learners given that the cumulative number of training samples processed remains the same. Thus there is an inherent (and similar) inefficiency introduced in the convergence behavior when we attempt to take advantage of parallelism, either by increasing mini-batch size or by increase the number of learners.","Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners). This paper shows that when a larger mini-batch is used (in the serial setting), the number of samples needed to be processed for the same convergence guarantee is larger. A similar behavior is discussed for using multiple learners in asynchronous SGD. This behavior has been known in convex optimization (e.g., ""Better Mini-Batch Algorithms via Accelerated Gradient Methods"", NIPS 2011). There, the convergence is of the form O(1/\sqrt{bT}+1/T), and so using bT samples only lead to \sqrt{b} time improvement. This paper extends a similar result to the nonconvex case (but the underlying mathematics is mainly from [Ghadimi & Lan, 2013]). However, this behavior is also known and indeed has been summarized in the deep learning textbook (chapter 8). Hence, the novelty is limited.----------------The theoretical results in this paper suggest that it is best not to use mini-batch. However, using mini-batch is often beneficial in practice. As discussed in the deep learning textbook, using mini-batch allows using a larger learning rate (note that this paper assumes the same learning rate for both mini-batch sizes). Moreover, multicore architectures allows parallel execution of mini-batch almost for free. Hence, the practical significance of the results is also limited.----------------Other comments:--------- Equation (4): since the same number of samples (S) is processed and S=MK, where M is the mini-batch size and K is the number of mini-batches processed (as mentioned in the first paragraph of section 2), when two different mini-batch sizes are considered (M_l and M_h), their K's should differ. However, the same K is used on the LHS of (4).----------------- Figures 1 and 2: As convergence speed is of main interest, why not show the training objective instead?",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work addresses the question of whether mini-batching improves the convergence of stochastic gradient methods, in terms of the number of examples, in the general non-asymptotic/non-convex setting of Ghadimi and Lan. Similar results are already known (at least as folk theory) in simpler regimes, but this result is novel. The reviewers are mixed in their opinions of the paper, but AnonReviewer4 brings up a point that deserves further attention: the analysis assumes that the same step-size is used for different mini-batch sizes. Because of this assumption, the conclusion of the paper is misleading. One of the effects of increasing the mini-batch is to decrease the variance of the estimator, which allows a larger step-size. With this larger step-size the net effect of increasing the mini-batch can actually be positive even in terms of the number of examples. Fixing the step-size removes the chance for the mini-batch strategy to do better in theory, and there is indeed a large amount of empirical evidence that mini-batches can lead to practical improvements. I do think the authors should continue this work, but I think accepting the current version is problematic because of the current conclusion.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_823,https://openreview.net/forum?id=Bk_zTU5eg,Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners),"Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and large mini-batches (given each instance processes equal number of training samples), while providing experimental evidence for the same. Going further to distributed settings, we show that an analogous effect holds with popular Asynchronous Gradient Descent (\asgd): there can be a large difference between convergence guarantees with increasing number of learners given that the cumulative number of training samples processed remains the same. Thus there is an inherent (and similar) inefficiency introduced in the convergence behavior when we attempt to take advantage of parallelism, either by increasing mini-batch size or by increase the number of learners.","Inefficiency Of Stochastic Gradient Descent With Larger Mini-batches (and More Learners). This paper addresses the problem of the influence of mini-batch size on the SGD convergence in a general non-convex setting. The results are then translated to analyze the influence of the number of learners on ASGD. I find the problem addressed in the paper relevant and the theoretical part clearly written. The experimental evaluation is somehow limited though. I would like to see experiments on more data sets and more architectures, as well as richer evaluation, e.g. N=16 is a fairly small experiment. It would also enhance the paper if the experiments were showing a similar behavior of other popular methods like momentum SGD or maybe EASGD (the latter in distributed setting). I understand the last evaluation does not directly lie in the scope of the paper, though adding these few experiments do not require much additional work and should be done.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The work addresses the question of whether mini-batching improves the convergence of stochastic gradient methods, in terms of the number of examples, in the general non-asymptotic/non-convex setting of Ghadimi and Lan. Similar results are already known (at least as folk theory) in simpler regimes, but this result is novel. The reviewers are mixed in their opinions of the paper, but AnonReviewer4 brings up a point that deserves further attention: the analysis assumes that the same step-size is used for different mini-batch sizes. Because of this assumption, the conclusion of the paper is misleading. One of the effects of increasing the mini-batch is to decrease the variance of the estimator, which allows a larger step-size. With this larger step-size the net effect of increasing the mini-batch can actually be positive even in terms of the number of examples. Fixing the step-size removes the chance for the mini-batch strategy to do better in theory, and there is indeed a large amount of empirical evidence that mini-batches can lead to practical improvements. I do think the authors should continue this work, but I think accepting the current version is problematic because of the current conclusion.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_827,https://openreview.net/forum?id=rye9LT8cee,Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks,"The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ","Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks. The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks.  The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos).----------------Pros: --------1) Put an old algorithm to good use in a new setting--------2) The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention).  This contributes to the efficient trainability of the model--------3) Seems to dovetail nicely with other results to encourage sparsity--that is, it can be used simultaneously--and is quite generalizable.----------------Cons:--------1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity--some sort of comparison involving training time would be great, and is currently lacking. EDIT: Authors addressed this by addition of results in Appendix B.--------2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment.  Is this a general feature?  Is this a statistical fluke? etc.  Even if the answer is ""it is not obvious, and determining why goes outside the scope of this work"", I would like to know it!  EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A.--------3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field.  I don't think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper.  EDIT: Authors addressed this by followup to question and additional text in the paper.----------------Additional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7.  The core of this paper is quite solid, it just needs a little bit more polishing.  ------------------------EDIT: Score has been updated.  ----------------Note: the authors probably meant ""In order to verify"" in the first sentence of Appendix A.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"This paper studies a sparsification method for pre-trained CNNs based on the alternating direction method of multipliers.   The reviewers agreed that the paper is well-written and easy to follow. The authors were responsive during the rebuttal phase and addressed most of the reviewers questions.     The reviewers, however, disagree with the significance of this work. Whereas R4 is happy to see an established method (ADMM) applied to a nowadays very popular architecture, R1-R3 were skeptical about the scope of the experiments and the usefulness of the sparse approximation.     Pros:  - Simple algorithmic description using well-known ADMM method.   - Consistent performance gains in small and mid-scale object classification problems.     Cons:  - Lack of significance in light of current literature on the topic.   - Lack of numerical experiments on large-scale classification problems and/or other tasks.  - Lack of clarity when reporting speedup gains.     Based on these assessments, the AC recommends rejection. Since this decision is not 100% aligned   with the reviewers, let me expand on the reasons why I recommend rejection.     This paper presents a sound algorithm that sparsifies the weights of a trained-CNN, and it shows that   the resulting pruned network works well, better than the original one. A priori, this is a solid result.   My main problem is that this contribution has to be taken in the context of the already large body of   literature addressing this same question, starting from the seminal 'Predicting Parameters in Deep Learning', by Misha Denil et al., NIPS 2013, which is surprisingly ignored in the bibliography here; and onwards with 'Exploiting linear structure within convolutional networks for efficient evaluation', Denton et al, NIPS'14 (also ignored) and 'Speeding up convolutional neural networks with low rank expansions', Jadergerg et al., '14. These and many other works culminated in two recent papers, 'Deep Compression', by Han et al, ICLR'16 and 'Learning Structured Sparsity in Deep Neural Networks' by Wen et al, NIPS'16.  Several reviewers pointed out that comparisons with Wen et al were needed; the authors of the present submission now do cite this work, but do not compare their results quantitatively and do not present the reader with compelling reasons to choose their acceleration instead of Wen et al.'s. Moreover, they mention in the rebuttal that the paper was published after the submission, but the work was available on the arxiv well before the ICLR deadline (august 2016). Wen's paper presents (i) source code, (ii) experiments on imagenet, (iii) 3x to 5x speedups in both CPU and GPU, and (iv) accuracy improvements on Cifar of ~1.5%. As far as I can see, the present submission presents none of these. Further compression factors were obtained in 'Deep Compression', also in large-scale models.     In light of these results, the authors should present more compelling evidence (in this case, since this is an experimental paper, empirical evidence in the form of higher accuracy and/or larger speedups/gains) as to why should a practitioner use their model, rather than simply presenting the differences between the approaches.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_828,https://openreview.net/forum?id=rye9LT8cee,Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks,"The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ","Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks. This paper presents a framework to use sparsity to reduce the parameters and computations of a pre-trained CNN. --------The results are only reported for small datasets and networks, while it is now imperative to be able to report results on larger datasets and production-size networks.--------The biggest problem with this paper is that it does not report numbers of inference time and gains, which is very important in production. And similarly for disk pace. Parameters reduction is useful only if it leads to a large decrease in space or inference time.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper studies a sparsification method for pre-trained CNNs based on the alternating direction method of multipliers.   The reviewers agreed that the paper is well-written and easy to follow. The authors were responsive during the rebuttal phase and addressed most of the reviewers questions.     The reviewers, however, disagree with the significance of this work. Whereas R4 is happy to see an established method (ADMM) applied to a nowadays very popular architecture, R1-R3 were skeptical about the scope of the experiments and the usefulness of the sparse approximation.     Pros:  - Simple algorithmic description using well-known ADMM method.   - Consistent performance gains in small and mid-scale object classification problems.     Cons:  - Lack of significance in light of current literature on the topic.   - Lack of numerical experiments on large-scale classification problems and/or other tasks.  - Lack of clarity when reporting speedup gains.     Based on these assessments, the AC recommends rejection. Since this decision is not 100% aligned   with the reviewers, let me expand on the reasons why I recommend rejection.     This paper presents a sound algorithm that sparsifies the weights of a trained-CNN, and it shows that   the resulting pruned network works well, better than the original one. A priori, this is a solid result.   My main problem is that this contribution has to be taken in the context of the already large body of   literature addressing this same question, starting from the seminal 'Predicting Parameters in Deep Learning', by Misha Denil et al., NIPS 2013, which is surprisingly ignored in the bibliography here; and onwards with 'Exploiting linear structure within convolutional networks for efficient evaluation', Denton et al, NIPS'14 (also ignored) and 'Speeding up convolutional neural networks with low rank expansions', Jadergerg et al., '14. These and many other works culminated in two recent papers, 'Deep Compression', by Han et al, ICLR'16 and 'Learning Structured Sparsity in Deep Neural Networks' by Wen et al, NIPS'16.  Several reviewers pointed out that comparisons with Wen et al were needed; the authors of the present submission now do cite this work, but do not compare their results quantitatively and do not present the reader with compelling reasons to choose their acceleration instead of Wen et al.'s. Moreover, they mention in the rebuttal that the paper was published after the submission, but the work was available on the arxiv well before the ICLR deadline (august 2016). Wen's paper presents (i) source code, (ii) experiments on imagenet, (iii) 3x to 5x speedups in both CPU and GPU, and (iv) accuracy improvements on Cifar of ~1.5%. As far as I can see, the present submission presents none of these. Further compression factors were obtained in 'Deep Compression', also in large-scale models.     In light of these results, the authors should present more compelling evidence (in this case, since this is an experimental paper, empirical evidence in the form of higher accuracy and/or larger speedups/gains) as to why should a practitioner use their model, rather than simply presenting the differences between the approaches.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_829,https://openreview.net/forum?id=rye9LT8cee,Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks,"The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ","Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks. This paper reduces the computational complexity of CNNs by minimizing the recognition loss and a sparsity-promoting penalty term together. The authors use ADMM, which is widely used in optimization problems but not used with CNNs before.--------The paper has a good motivation and well written. The experiments show that the proposed approach increases the sparsity in a network as well as increases the performance.----------------As the authors stated, ADMM approach is not guaranteed to converge in non-convex problems. The authors used pre-trained networks to mitigate the problem of trapping into a local optimum. However, the datasets that are used are very small. It would be good to investigate how the proposed approach works on bigger datasets such as ImageNet.----------------Authors should compare their results with previous studies that use pruning or sparsity regularizers (Liu et al. (2015); Han et al. (2015); Collins & Kohli (2014)).----------------In the discussion section, authors stated that the proposed approach is efficient in training because of the separability property. Could you elaborate on that? Lets say this work uses two phases; phase 1 is pre-training a network, phase 2 is using sparsity and performance promoting steps.  Phase 2 also includes fine-tuning the network based on the new sparse structure. How long does phase 2 take compare to phase 1? How many epochs is needed to fine-tune the network?",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper studies a sparsification method for pre-trained CNNs based on the alternating direction method of multipliers.   The reviewers agreed that the paper is well-written and easy to follow. The authors were responsive during the rebuttal phase and addressed most of the reviewers questions.     The reviewers, however, disagree with the significance of this work. Whereas R4 is happy to see an established method (ADMM) applied to a nowadays very popular architecture, R1-R3 were skeptical about the scope of the experiments and the usefulness of the sparse approximation.     Pros:  - Simple algorithmic description using well-known ADMM method.   - Consistent performance gains in small and mid-scale object classification problems.     Cons:  - Lack of significance in light of current literature on the topic.   - Lack of numerical experiments on large-scale classification problems and/or other tasks.  - Lack of clarity when reporting speedup gains.     Based on these assessments, the AC recommends rejection. Since this decision is not 100% aligned   with the reviewers, let me expand on the reasons why I recommend rejection.     This paper presents a sound algorithm that sparsifies the weights of a trained-CNN, and it shows that   the resulting pruned network works well, better than the original one. A priori, this is a solid result.   My main problem is that this contribution has to be taken in the context of the already large body of   literature addressing this same question, starting from the seminal 'Predicting Parameters in Deep Learning', by Misha Denil et al., NIPS 2013, which is surprisingly ignored in the bibliography here; and onwards with 'Exploiting linear structure within convolutional networks for efficient evaluation', Denton et al, NIPS'14 (also ignored) and 'Speeding up convolutional neural networks with low rank expansions', Jadergerg et al., '14. These and many other works culminated in two recent papers, 'Deep Compression', by Han et al, ICLR'16 and 'Learning Structured Sparsity in Deep Neural Networks' by Wen et al, NIPS'16.  Several reviewers pointed out that comparisons with Wen et al were needed; the authors of the present submission now do cite this work, but do not compare their results quantitatively and do not present the reader with compelling reasons to choose their acceleration instead of Wen et al.'s. Moreover, they mention in the rebuttal that the paper was published after the submission, but the work was available on the arxiv well before the ICLR deadline (august 2016). Wen's paper presents (i) source code, (ii) experiments on imagenet, (iii) 3x to 5x speedups in both CPU and GPU, and (iv) accuracy improvements on Cifar of ~1.5%. As far as I can see, the present submission presents none of these. Further compression factors were obtained in 'Deep Compression', also in large-scale models.     In light of these results, the authors should present more compelling evidence (in this case, since this is an experimental paper, empirical evidence in the form of higher accuracy and/or larger speedups/gains) as to why should a practitioner use their model, rather than simply presenting the differences between the approaches.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_830,https://openreview.net/forum?id=rye9LT8cee,Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks,"The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. ","Alternating Direction Method Of Multipliers For Sparse Convolutional Neural Networks. The paper presents a method for sparsifying the weights of the convolutional--------filters and fully-connected layers of a CNN without loss of performance. Sparsification is achieved by using augmenting the CNN objective function with a regularization term promoting sparisty on groups of weights. The authors use ADMM for solving this optimization task, which allows decoupling of the two terms. The method alternates between promoting the sparsity of the network and optimizing the recognition performance----------------The method is technically sound and clearly explained. The paper is well organised and the ideas presented in a structured manner. I believe that sometimes the wording could be improved. ----------------The proposed method is simple and effective. Using it in combination with other CNN compression techniques such as quantization/encoding is a promising direction for future research. The experimental evaluation is convincing in the sense that the method seems to work well. The authors do not use state-of-the-art CNNs architectures, but I don't see this a requirement to deliver the message. ----------------On the other hand, the proposed method is closely related to recent works (as shown in the references posted in the public comment titled ""Comparison with structurally-sparse DNNs using group Lasso), that should be cited and compared against (at least at a conceptual level). I will of course consider the authors rebuttal on this matter.----------------It would be very important for the authors to comment on the differences between these works and the proposed approach. It seems that both of these references use sparsity regularized training for neural networks, with a very similar formulation. ----------------The authors choose to optimize the proposed objective function using ADMM. It is not clear to me, why this approach should be more effective than proximal gradient descent methods. Could you please elaborate on this? ADMM is more demanding in terms of memory (2 copies of the parameters need to be stored). ----------------The claimed contributions (Section 5) seem a bit misleading in my opinion. Using sparsity promoting regularization in the parameters of the model has been used by many works, in particular in the dictionary learning literature but also in the neural network community (as stated above).  Claims 1,2 and 3, are well understood properties of L1-type regularizers. As written in the current version of the manuscript, it seems that these are claimed contributions of this particular work. ----------------A discussion on why sparsity sometimes helps improve performance could be interesting. ----------------In the experimental section, the authors mainly concentrate on comparing accuracy vs parameter reduction. While this is naturally very relevant property to report, it would be also interesting to show more results in terms of speed-up. which should also be improved by the proposed approach.----------------Other minor issues:--------- In (3): I think a ""^2"" is missing in the augmented term (the rightmost term).----------------- The authors could cite the approach by Han et all for compressing DNNS:--------Han, ICLR 2016 ""Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding""",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper studies a sparsification method for pre-trained CNNs based on the alternating direction method of multipliers.   The reviewers agreed that the paper is well-written and easy to follow. The authors were responsive during the rebuttal phase and addressed most of the reviewers questions.     The reviewers, however, disagree with the significance of this work. Whereas R4 is happy to see an established method (ADMM) applied to a nowadays very popular architecture, R1-R3 were skeptical about the scope of the experiments and the usefulness of the sparse approximation.     Pros:  - Simple algorithmic description using well-known ADMM method.   - Consistent performance gains in small and mid-scale object classification problems.     Cons:  - Lack of significance in light of current literature on the topic.   - Lack of numerical experiments on large-scale classification problems and/or other tasks.  - Lack of clarity when reporting speedup gains.     Based on these assessments, the AC recommends rejection. Since this decision is not 100% aligned   with the reviewers, let me expand on the reasons why I recommend rejection.     This paper presents a sound algorithm that sparsifies the weights of a trained-CNN, and it shows that   the resulting pruned network works well, better than the original one. A priori, this is a solid result.   My main problem is that this contribution has to be taken in the context of the already large body of   literature addressing this same question, starting from the seminal 'Predicting Parameters in Deep Learning', by Misha Denil et al., NIPS 2013, which is surprisingly ignored in the bibliography here; and onwards with 'Exploiting linear structure within convolutional networks for efficient evaluation', Denton et al, NIPS'14 (also ignored) and 'Speeding up convolutional neural networks with low rank expansions', Jadergerg et al., '14. These and many other works culminated in two recent papers, 'Deep Compression', by Han et al, ICLR'16 and 'Learning Structured Sparsity in Deep Neural Networks' by Wen et al, NIPS'16.  Several reviewers pointed out that comparisons with Wen et al were needed; the authors of the present submission now do cite this work, but do not compare their results quantitatively and do not present the reader with compelling reasons to choose their acceleration instead of Wen et al.'s. Moreover, they mention in the rebuttal that the paper was published after the submission, but the work was available on the arxiv well before the ICLR deadline (august 2016). Wen's paper presents (i) source code, (ii) experiments on imagenet, (iii) 3x to 5x speedups in both CPU and GPU, and (iv) accuracy improvements on Cifar of ~1.5%. As far as I can see, the present submission presents none of these. Further compression factors were obtained in 'Deep Compression', also in large-scale models.     In light of these results, the authors should present more compelling evidence (in this case, since this is an experimental paper, empirical evidence in the form of higher accuracy and/or larger speedups/gains) as to why should a practitioner use their model, rather than simply presenting the differences between the approaches.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_837,https://openreview.net/forum?id=BkJsCIcgl,The Predictron: End-to-end Learning And Planning,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple ""imagined"" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths.  The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function, thereby focusing the model upon the aspects of the environment most relevant to planning. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.","The Predictron: End-to-end Learning And Planning. The paper proposes an approach to learning models that are good for planning problems, using deep netowork architectures. The key idea is to ensure that models are self-consistent and accurately predict the future. The problem of learning good planning models (as opposed to simply good predictive models is really crucial and attempts so far have failed. This paper is conceptually interesting and provides a valuable perspective on how to achieve this goal. Its incorporation of key RL concepts (like discounting and eligibility traces) and the flexibility to learn these is very appealing. Hence, I think it should be accepted. This being said, I think the paper does not quite live up to its claims. Here are some aspects that need to be addressed (in order of importance):--------1. Relationship to past work: the proposed representation seems essentially a non-linear implementation of the Horde architecture. It is also very similar in spirit to predictive state representations. Yet these connections are almost not discussed at all. The related work paragraph is very brief and needs expansion to situate the work in the context of other predictive modelling attempts that both were designed to be used for planning and (in the case of PSRs) were in fact successsfully used in planning tasks. Some newer work on learning action-conditional models in Atari games are also not discussed. Situating the paper better in the context of existing model learning would also help understand easier both the motivations and the novel contributions of the work (otherwise, the reader is left to try and elucidate this for themselves, and may come to the wrong conclusion).--------2. The paper needs to provide some insight about the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and ""bulky"" (very high number of parameters). Is this really necessary in all tasks? Can one implement the basic ideas outside of the particular architecture proposed? Can we use feedforward approximations or is the recurrent part somehow necessary? At the very least the paper should expand the discussion on this topic, if not provide some empirical evidence.--------3. The experiments are very restricted in their setup: iid data drawn from fixed distributions, correct targets. So, the proposed approach seems like an overkill for these particular tasks. There is an indirect attempt to provide evidence the learned models would be useful for planning, but no direct measurement to support this'd claim (no use of the models in planning). Compared to the original Horde paper, fewer predictions are learned, and these are more similar to each other. While I sympathize with the desire to go in steps, I think the paper stops short of where it should. At the very least, doing prediction in the context of an actual RL prediction task, with non-iid inputs, should be included in the paper. This should only require minor modifications to the experiments (same task, just different data). Ideally, in the case of the mazes, the learned models should be used in some form of simplified planning to learn paths. This would align the experiments much better with the claims in the presentation of the architecture.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"There is potential here for a great paper, unfortunately in its current form there is too deep of a disconnect between the framing and promise of the presentation, and the empirical validation actually delivered by the experiments.  The choice of the experimental setting (iid input from fixed distributions in a pattern recognition setting where targets are between 0 and 1) is too narrow to allow for convincing validation of the central hypothesis of the paper, namely, that the architecture (a recurrent convnet with sigmoid gating) can be useful for problems involving planning. Instead, it simply shows that the architecture is better at outputting the targets than other deep architectures without sigmoid gating.  I strongly encourage the authors to add more ambitious experiments to keep the empirical arm more in step with the stated promises of the set-up.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_838,https://openreview.net/forum?id=BkJsCIcgl,The Predictron: End-to-end Learning And Planning,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple ""imagined"" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths.  The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function, thereby focusing the model upon the aspects of the environment most relevant to planning. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.","The Predictron: End-to-end Learning And Planning. I think there may be a nice paper to made from this, but as it is, it should not be accepted.   The authors describe a new architecture for regression, inspired by techniques for estimating the value function of an Markov reward process.   The connection is interesting, and there is certainly merit in the idea.   However, the writing is confusing,  and as far as I can tell, the experiments and discussion are inadequate.  It is quite possible that I am misunderstanding some things, so I am not putting high confidence.----------------Because of all the discussion of MRP's and the background that inspired the model, it is difficult to see that the authors are in a pure, i.i.d. regression setting, where they sample inputs i.i.d. (with deterministic outputs given the input) from a distribution, and try to match  a parameterized function to the input output pairs.    ----------------Because they are in this setting, there is a lot lacking from the experiments.  For example, they report l2 loss on the maze problem; but not ""percent correct""; indeed, it looks like the deep net with skips goes to about .001 average l2 loss on the 0-1 output maze problem.   This is an issue because because it suggests that by simply thresholding the outputs, you could get nearly perfect results, which would point to a model specification error of the baseline.  Are there sigmoids at the end of the baseline plain deep network?  Note that the proposed models do have sigmoids in the outputs in the multiplicative weightings.     ----------------How do the number of parameters of the proposed network compare to the baselines?   Is the better performance (and again, better is really marginal if I am understanding the way loss is measured) simply an issue of modeling power (perhaps because of the multiplicative connections of the proposed model vs. the baseline)?  Because the input is taken i.i.d and the test distribution exactly matches the train, this is an important part of the discussion.  Moreover, there do not seem to be experiments where the size of the training set is fixed- the axis in the graphs is number of samples seen, which is tied to the number of optimization steps.  Thus there is no testing of over-fitting.----------------Why not try the model on more standard regression problems (as at heart, the paper seems to be about a new convnet architecture for regression)?   Show imagenet or cifar accuracies, for example.  If  the proposed model does worse there, try to explain/understand what it is about the reported tasks that favor the proposed model?----------------------------------------**********************************************************************************--------edited with increased confidence in post review discussions--------**********************************************************************************",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is potential here for a great paper, unfortunately in its current form there is too deep of a disconnect between the framing and promise of the presentation, and the empirical validation actually delivered by the experiments.  The choice of the experimental setting (iid input from fixed distributions in a pattern recognition setting where targets are between 0 and 1) is too narrow to allow for convincing validation of the central hypothesis of the paper, namely, that the architecture (a recurrent convnet with sigmoid gating) can be useful for problems involving planning. Instead, it simply shows that the architecture is better at outputting the targets than other deep architectures without sigmoid gating.  I strongly encourage the authors to add more ambitious experiments to keep the empirical arm more in step with the stated promises of the set-up.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_839,https://openreview.net/forum?id=BkJsCIcgl,The Predictron: End-to-end Learning And Planning,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple ""imagined"" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths.  The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function, thereby focusing the model upon the aspects of the environment most relevant to planning. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.","The Predictron: End-to-end Learning And Planning. This work proposes a computational structure of function approximator with a strong prior: it is optimized to act as an abstract MRP, capable of learning its own internal state, model, and notion of time-step. Thanks to the incorporation of a \lambda-return style return estimation, it can effectively adapt its own ""thinking-depth"" on the current input, thus performing some sort of soft iterative inference.----------------Such a prior, maintained by strong regularization, helps perform better than similar baselines or some prediction tasks that require some form of sequential reasoning.----------------The proposed idea is novel, and a very interesting take on forcing internal models upon function approximators which begs for future work. The experimental methodology is complete, showcases the potential of the approach, and nicely analyses the iterative/adaptative thinking depth learned by the model.----------------As pointed out by my previous comments, the paper reads well but utilizes language that may confuse a reader unfamiliar with the subject. I think some rewording could be done without having much impact on the depth of the paper. In particular, introducing the method as a regularized model pushed to act like an MRP, rather than an actual MRP performing some abstract reasoning, may help confused readers such as myself.","9: Top 15% of accepted papers, strong accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","There is potential here for a great paper, unfortunately in its current form there is too deep of a disconnect between the framing and promise of the presentation, and the empirical validation actually delivered by the experiments.  The choice of the experimental setting (iid input from fixed distributions in a pattern recognition setting where targets are between 0 and 1) is too narrow to allow for convincing validation of the central hypothesis of the paper, namely, that the architecture (a recurrent convnet with sigmoid gating) can be useful for problems involving planning. Instead, it simply shows that the architecture is better at outputting the targets than other deep architectures without sigmoid gating.  I strongly encourage the authors to add more ambitious experiments to keep the empirical arm more in step with the stated promises of the set-up.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_843,https://openreview.net/forum?id=rJxDkvqee,Multi-view Recurrent Neural Acoustic Word Embeddings,"Recent work has begun exploring neural acoustic word embeddings–fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.","Multi-view Recurrent Neural Acoustic Word Embeddings. Pros:--------  Interesting training criterion.--------Cons:--------  Missing proper ASR technique based baselines.----------------Comments:--------  The dataset is quite small.--------  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.--------  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection--------  performance of out-of-vocabulary words.--------  It would be interesting to show scatter plots for embedding vs. orthographic distances.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_844,https://openreview.net/forum?id=rJxDkvqee,Multi-view Recurrent Neural Acoustic Word Embeddings,"Recent work has begun exploring neural acoustic word embeddings–fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.",Multi-view Recurrent Neural Acoustic Word Embeddings. This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.----------------There's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.---------------- the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.---------------- it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand ,6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_845,https://openreview.net/forum?id=rJxDkvqee,Multi-view Recurrent Neural Acoustic Word Embeddings,"Recent work has begun exploring neural acoustic word embeddings–fixed dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.","Multi-view Recurrent Neural Acoustic Word Embeddings. this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.----------------although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:--------- investigating the use of fairly known architecture on a new domain.--------- providing novel objectives specific to the domain--------- setting up new benchmarks designed for evaluating multi-view models----------------I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_846,https://openreview.net/forum?id=Bkfwyw5xg,Investigating Different Context Types And Representations For Learning Word Embeddings,"The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.",Investigating Different Context Types And Representations For Learning Word Embeddings. This paper analyzes dependency trees vs standard window contexts for word vector learning.--------While that's a good goal I believe the paper falls short of a thorough analysis of the subject matter.--------It does not analyze Glove like objective functions which often work better than the algorithms used here.--------It doesn't compare in absolute terms to other published vectors or models.----------------It fails to gain any particularly interesting insights that will modify other people's work.--------It fails to push the state of the art or make available new resources for people.,4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Reviewers agree that the findings are not clear enough to be of interest, though the effort to do a controlled study is appreciated.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_847,https://openreview.net/forum?id=Bkfwyw5xg,Investigating Different Context Types And Representations For Learning Word Embeddings,"The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.","Investigating Different Context Types And Representations For Learning Word Embeddings. This paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks.----------------I am ambivalent about this paper. On one hand, it continues an important line of work in decoupling various parameters from the embedding algorithms (this time focusing on context); on the other hand, I am not sure I understand what the conclusion from these experiments is. There does not appear to be a significant and consistent advantage to any one context type. Why is this? Are the benchmarks sensitive enough to detect these differences, if they exist?----------------While I am OK with this paper being accepted, I would rather see a more elaborate version of it, which tries to answer these more fundamental questions.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers agree that the findings are not clear enough to be of interest, though the effort to do a controlled study is appreciated.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_848,https://openreview.net/forum?id=Bkfwyw5xg,Investigating Different Context Types And Representations For Learning Word Embeddings,"The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their context. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and representations for learning word embeddings. We conduct comprehensive experiments to evaluate their effectiveness under 4 tasks (21 datasets), which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.","Investigating Different Context Types And Representations For Learning Word Embeddings. This paper investigates the issue of whether and how to use syntactic dependencies in unsupervised word representation learning models like CBOW or Skip-Gram, with a focus one the issue of bound (word+dependency type, 'She-nsubj') vs. unbound (word alone, 'She') representations for context at training time. The empirical results are extremely mixed, and no specific novel method consistently outperforms existing methods.----------------The paper is systematic and I have no major concerns about its soundness. However, I don't think that this paper is of broad interest to the ICLR community. The paper is focused on a fairly narrow detail of representation learning that is entirely specific to NLP, and its results are primarily negative. A short paper at an ACL conference would be a more reasonable target.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"Reviewers agree that the findings are not clear enough to be of interest, though the effort to do a controlled study is appreciated.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_852,https://openreview.net/forum?id=BJ--gPcxl,Semi-supervised Learning With Context-conditional Generative Adversarial Networks,"We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.","Semi-supervised Learning With Context-conditional Generative Adversarial Networks. This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.--------They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.--------Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_853,https://openreview.net/forum?id=BJ--gPcxl,Semi-supervised Learning With Context-conditional Generative Adversarial Networks,"We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.","Semi-supervised Learning With Context-conditional Generative Adversarial Networks. This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.----------------The core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.----------------I think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_854,https://openreview.net/forum?id=BJ--gPcxl,Semi-supervised Learning With Context-conditional Generative Adversarial Networks,"We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.","Semi-supervised Learning With Context-conditional Generative Adversarial Networks. After rebuttal:----------------Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:--------- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""--------- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""----------------These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.--------------------------------Initial review:----------------The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.----------------The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.----------------1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.----------------2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There has been prior work on semi-supervised GAN, though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers, as the approach seems more incremental. Furthermore, it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_858,https://openreview.net/forum?id=HkSOlP9lg,Recurrent Inference Machines For Solving Inverse Problems,"Inverse problems are typically solved by first defining a model and then choosing an inference procedure. With this separation of modeling from inference, inverse problems can be framed in a modular way. For example, variational inference can be applied to a broad class of models. The modularity, however, typically goes away after model parameters have been trained under a chosen inference procedure. During training, model and inference often interact in a way that the model parameters will ultimately be adapted to the chosen inference procedure, posing the two components inseparable after training. But if model and inference become inseperable after training, why separate them in the first place?  We propose a novel learning framework which abandons the dichotomy between model and inference. Instead, we introduce Recurrent Inference Machines (RIM), a class of recurrent neural networks (RNN) that directly learn to solve inverse problems.  We demonstrate the effectiveness of RIMs in experiments on various image reconstruction tasks. We show empirically that RIMs exhibit the desirable convergence behavior of classical inference procedures, and that they can outperform state-of- the-art methods when trained on specialized inference tasks.  Our approach bridges the gap between inverse problems and deep learning, providing a framework for fast progression in the field of inverse problems.","Recurrent Inference Machines For Solving Inverse Problems. This paper proposes the RIMs that unrolls variational inference procedure. ----------------The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments.----------------While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. ----------------However I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters.  ----------------Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2].----------------Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents an approach for learning both a model and inference procedure at the same time using RNNs. The reviewers agree that the idea is interesting, but discussion and considering the responses to the reviews, still felt that more is needed to motivate and contextualise the work, and to make the methods presented more convincing. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_859,https://openreview.net/forum?id=HkSOlP9lg,Recurrent Inference Machines For Solving Inverse Problems,"Inverse problems are typically solved by first defining a model and then choosing an inference procedure. With this separation of modeling from inference, inverse problems can be framed in a modular way. For example, variational inference can be applied to a broad class of models. The modularity, however, typically goes away after model parameters have been trained under a chosen inference procedure. During training, model and inference often interact in a way that the model parameters will ultimately be adapted to the chosen inference procedure, posing the two components inseparable after training. But if model and inference become inseperable after training, why separate them in the first place?  We propose a novel learning framework which abandons the dichotomy between model and inference. Instead, we introduce Recurrent Inference Machines (RIM), a class of recurrent neural networks (RNN) that directly learn to solve inverse problems.  We demonstrate the effectiveness of RIMs in experiments on various image reconstruction tasks. We show empirically that RIMs exhibit the desirable convergence behavior of classical inference procedures, and that they can outperform state-of- the-art methods when trained on specialized inference tasks.  Our approach bridges the gap between inverse problems and deep learning, providing a framework for fast progression in the field of inverse problems.","Recurrent Inference Machines For Solving Inverse Problems. Unfortunately, even after reading the authors' response to my pre-review question, I feel this paper in its current form lacks sufficient novelty to be accepted to ICLR.----------------Fundamentally, the paper suggests that traditional iterative algorithms for specific class of problems (ill-posed image inverse problems) can be replaced by discriminatively trained recurrent networks. As R3 also notes, un-rolled networks for iterative inference aren't new: they've been used to replace CRF-type inference, and _also_ to solve image inverse problems (my refs [1-3]). Therefore, I'd argue that the fundamental idea proposed by the paper isn't new---it is just that the paper seeks to 'formalize' it as an approach for inverse problems (although, there is nothing specific about the analysis that ties it to inverse problems: the paper only shows that the RIM can express gradient descent over prior + likelihood objective).----------------I also did not find the claims about benefits over prior approaches very compelling. The comment about parameter sharing works both ways---it is possible that untying the parameters leads to better performance over a fewer number of 'iterations', and given that the 'training set' is synthetically generated, learning a larger number of parameters doesn't seem to be an issue. Also, I'd argue that sharing the parameters is the 'obvious' approach, and the prior methods choose to not tie the parameters to get better accuracy.----------------The same holds for being able to handle different noise levels / scale sizes. A single model can always be trained to handle multiple forms of degradation---its just that its likely to do better when it's trained for specific degradation model/level. But more importantly, there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture. (Moreover, this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end).----------------It is possible that the proposed method does offer practical benefits beyond prior work---but these benefits don't come from the idea of simply unrolling iterations, which is not novel. I would strongly recommend that the authors consider a significant re-write of the paper---with a detailed discussion of prior work mentioned in the comments that highlights, with experiments, the specific aspects of their recurrent architecture that enables better recovery for inverse problems. I would also suggest that to claim the mantle of 'solving inverse problems', the paper consider a broader set of inverse tasks---in-painting, deconvolution, different noise models, and possibly working with multiple observations (like for HDR).",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents an approach for learning both a model and inference procedure at the same time using RNNs. The reviewers agree that the idea is interesting, but discussion and considering the responses to the reviews, still felt that more is needed to motivate and contextualise the work, and to make the methods presented more convincing. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_860,https://openreview.net/forum?id=HkSOlP9lg,Recurrent Inference Machines For Solving Inverse Problems,"Inverse problems are typically solved by first defining a model and then choosing an inference procedure. With this separation of modeling from inference, inverse problems can be framed in a modular way. For example, variational inference can be applied to a broad class of models. The modularity, however, typically goes away after model parameters have been trained under a chosen inference procedure. During training, model and inference often interact in a way that the model parameters will ultimately be adapted to the chosen inference procedure, posing the two components inseparable after training. But if model and inference become inseperable after training, why separate them in the first place?  We propose a novel learning framework which abandons the dichotomy between model and inference. Instead, we introduce Recurrent Inference Machines (RIM), a class of recurrent neural networks (RNN) that directly learn to solve inverse problems.  We demonstrate the effectiveness of RIMs in experiments on various image reconstruction tasks. We show empirically that RIMs exhibit the desirable convergence behavior of classical inference procedures, and that they can outperform state-of- the-art methods when trained on specialized inference tasks.  Our approach bridges the gap between inverse problems and deep learning, providing a framework for fast progression in the field of inverse problems.","Recurrent Inference Machines For Solving Inverse Problems. This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems.--------The proposed method is interesting and results are quite good. The paper is also nicely presented. ----------------I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising. What are the filters like? Is it particularly sensitive to different structures in images? edges? Also, what is the state in the recurrent unit used for? when are the gates open etc.----------------Nevertheless, I think this is nice work which should be accepted.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"This paper presents an approach for learning both a model and inference procedure at the same time using RNNs. The reviewers agree that the idea is interesting, but discussion and considering the responses to the reviews, still felt that more is needed to motivate and contextualise the work, and to make the methods presented more convincing. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_861,https://openreview.net/forum?id=B1s6xvqlx,Recurrent Environment Simulators,"Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.","Recurrent Environment Simulators. [UPDATE]--------After going through the response from the author and the revision, I increased my review score for two reasons.--------1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.--------This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.--------It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).----------------2. The revised paper contains more comprehensive results than before.--------The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.----------------- Summary--------This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.----------------- Novelty--------The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. ----------------- Experiment--------The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.----------------- Clarity--------The paper is well-written and easy to follow.----------------- Overall --------Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.----------------[Reference]--------Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Quality, Clarity:    The paper is well written. Further revisions have been made upon the original.    Originality, Significance:   The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. This is done using a mix of (a) architectural modifications; (b) jumpy predictions; and (c) particular training schemes. The experimental validation is extensive, now including additional comparisons suggested by reviewers.  There is not complete consensus about the significance of the contributions, with one reviewer seeking additional technical novelty. Overall, the paper appears to provide interesting and very soundly-evaluated results, which likely promises to be the new standard for this type of prediction problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_862,https://openreview.net/forum?id=B1s6xvqlx,Recurrent Environment Simulators,"Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.","Recurrent Environment Simulators. The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. The paper claims three main contributions: --------1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t--------2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames)--------3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)----------------1. modification to model architecture--------+ The motivation seems good that in past work (Oh et al.) the action at t-1 influences x_t, but not the state h_t of the LSTM. This could be fixed by making the LSTM state h_t dependent on a_{t-1}--------- However, this is of minor technical novelty. Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t. This could be done without modifying the LSTM architecture as stated in the paper. While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1. Also, a stronger motivation is required to support the current formulation.--------- Further, the benefits of this change in architecture is not well analyzed in experiments. Fig. 5(a) provides the difference between Oh et al. (with traditional LSTM) and current method. However, the performance difference is composed of 2 components (difference in training scheme and architecture). This contribution of the architecture to the performance is not clear from this experiment. The authors did claim in the pre-review phase that Fig. 12 (a) shows the difference in performance only due to architecture for ""Seaquest"". However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig. 5 (a) (~90). It is difficult to judge the significance of the architecture modification from this result for one game.----------------2. Exploring the idea of jumpy predictions:--------+ As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.--------+ The results in Fig. 5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games.--------- However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.--------- While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games. Also, the novelty in dropping intermediate frames for speedup is marginal.----------------3. Exploring different training schemes--------+ This is perhaps the most interesting observation presented in the paper. The authors present the difference in performance for different training schemes in Fig. 2(a). The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames.--------+ The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al.--------- While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.----------------Clarity of presentation:--------- The exact experimental setup is not clearly stated for some of the results. For instance, the paper does not say that Fig. 2(a) uses the same architecture as Oh et al. However, this is stated in the response to reviewer questions.--------- Fig. 4 is difficult to interpret. The qualitative difference between Oh et al. and current method could be highlighted explicitly. --------- Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.----------------Overall, the paper presents some interesting experimental observations. However, the technical novelty and contribution of the proposed architecture and training scheme is not clear.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Quality, Clarity:    The paper is well written. Further revisions have been made upon the original.    Originality, Significance:   The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. This is done using a mix of (a) architectural modifications; (b) jumpy predictions; and (c) particular training schemes. The experimental validation is extensive, now including additional comparisons suggested by reviewers.  There is not complete consensus about the significance of the contributions, with one reviewer seeking additional technical novelty. Overall, the paper appears to provide interesting and very soundly-evaluated results, which likely promises to be the new standard for this type of prediction problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_863,https://openreview.net/forum?id=B1s6xvqlx,Recurrent Environment Simulators,"Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.","Recurrent Environment Simulators. The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.--------The original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,--------and their inability to jump directly to a future prediction without iterating through all intermediate states.--------The authors have provided an extensive experimental evaluation on several benchmarks with promising results.--------In general the paper is well written and quite clear in its explanations.--------Demonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.----------------# Minor comments:--------`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.----------------# Typos--------Section 3.1 - `this configuration is all experiments'","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Quality, Clarity:    The paper is well written. Further revisions have been made upon the original.    Originality, Significance:   The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. This is done using a mix of (a) architectural modifications; (b) jumpy predictions; and (c) particular training schemes. The experimental validation is extensive, now including additional comparisons suggested by reviewers.  There is not complete consensus about the significance of the contributions, with one reviewer seeking additional technical novelty. Overall, the paper appears to provide interesting and very soundly-evaluated results, which likely promises to be the new standard for this type of prediction problem.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_864,https://openreview.net/forum?id=H1zJ-v5xl,Quasi-recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","Quasi-recurrent Neural Networks. This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in--------sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition--------matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). ----------------As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.----------------The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. ----------------While I feel that the contribution is somewhat incremental, I recommend acceptance. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.    Pros:  - Fast model  - Good results    Cons:  - Because of its strong relationship to other models, the novelty is incremental.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_865,https://openreview.net/forum?id=H1zJ-v5xl,Quasi-recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","Quasi-recurrent Neural Networks. This paper introduces a novel RNN architecture named QRNN.----------------QNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.--------Consequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.----------------Various extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.----------------Authors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). ----------------Overall the paper is an enjoyable read and the proposed approach is interesting,--------Pros:--------- Address an important problem--------- Nice empirical evaluation showing the benefit of their approach--------- Demonstrate up to 16x speed-up relatively to a LSTM--------Cons:--------- Somewhat incremental novelty compared to (Balduzizi et al., 2016)----------------Few specific questions:--------- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  --------- How does the i-fo-ifo pooling perform comparatively? --------- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.    Pros:  - Fast model  - Good results    Cons:  - Because of its strong relationship to other models, the novelty is incremental.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_866,https://openreview.net/forum?id=H1zJ-v5xl,Quasi-recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","Quasi-recurrent Neural Networks. The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.------------------------Major comment--------=============--------QRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?------------------------Minor comments--------==============--------1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. ----------------2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. ----------------3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.----------------4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.----------------Sentiment classification----------------------------------------5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.----------------6. What was the convolutional filter size?----------------7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?----------------8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors’ claim that neurons are interpretable even more convincing.------------------------Language modeling---------------------------------9. What was the size of the training, test, and validation set?----------------10. What was the convolutional filter size, denoted as ‘k’?----------------11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?----------------12. The authors should show learning curves for a models with and without zone-out.-------- --------Translation---------------------------13. What was the size of the training, test, and validation set?----------------14. How does translation performance depend on k?------------------------ -------- ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.    Pros:  - Fast model  - Good results    Cons:  - Because of its strong relationship to other models, the novelty is incremental.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_867,https://openreview.net/forum?id=H1zJ-v5xl,Quasi-recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","Quasi-recurrent Neural Networks. This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.----------------- I would encourage the authors to improve the explanation of the model. --------- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. --------- Otherwise the experiments seem adequate and I enjoyed this paper.----------------This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.    Pros:  - Fast model  - Good results    Cons:  - Because of its strong relationship to other models, the novelty is incremental.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_881,https://openreview.net/forum?id=HycUbvcge,Deep Generalized Canonical Correlation Analysis,"We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.","Deep Generalized Canonical Correlation Analysis. The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. ----------------In general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. ----------------It would be interesting to add or discuss the following issues:----------------- what is the complexity of the proposed method, esp. the representation learning part?--------- would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel.--------- the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. --------- the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This is largely a clearly written paper that proposes a nonlinear generalization of a generalized CCA approach for multi-view learning. In terms of technical novelty, the generalization follows rather straightforwardly. Reviewers have expressed the need to clarify relationship and provide comparisons to existing proposals for combining deep learning with CCA. As such the paper has been evaluated to be borderline. The proposed method appears to yield significant gains on a speech dataset, though comparisons on other datasets appear to be less conclusive. Some basic baselines as missing, e.g., concatenating views and running a deep model, or using much older nonlinear extensions of CCA such as kernel CCA (e.g. accelerated via random features, and combined with deep representations underneath).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_882,https://openreview.net/forum?id=HycUbvcge,Deep Generalized Canonical Correlation Analysis,"We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.","Deep Generalized Canonical Correlation Analysis. This paper proposes a deep extension of generalized CCA. The main contribution of the paper is deriving the gradient update for the GCCA objective.----------------I disagree with the claim that “this is the first Multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views”.  [R1] proposes a Multiview representation learning method which is both non-linear and capable of handling more than 2 views. This is very much relevant to what authors are proposing. The objective function proposed in [R1] maximizes the correlation between views and minimizes the self and cross reconstruction errors. This is intuitively similar to nonlinear version of PCA+CCA for multiple views. Comparing these 2 methods is crucial to prove the usefulness of DGCCA and the paper is incomplete without this comparison. Authors should also change their strong claim.----------------Related work section is minimal. There are significant advances in 2-view non-linear representation learning which are worth mentioning. ----------------References:--------[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. HLT-NAACL 2016: 171-181",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This is largely a clearly written paper that proposes a nonlinear generalization of a generalized CCA approach for multi-view learning. In terms of technical novelty, the generalization follows rather straightforwardly. Reviewers have expressed the need to clarify relationship and provide comparisons to existing proposals for combining deep learning with CCA. As such the paper has been evaluated to be borderline. The proposed method appears to yield significant gains on a speech dataset, though comparisons on other datasets appear to be less conclusive. Some basic baselines as missing, e.g., concatenating views and running a deep model, or using much older nonlinear extensions of CCA such as kernel CCA (e.g. accelerated via random features, and combined with deep representations underneath).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_883,https://openreview.net/forum?id=HycUbvcge,Deep Generalized Canonical Correlation Analysis,"We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.","Deep Generalized Canonical Correlation Analysis. The proposed method is simple and elegant; it builds upon the huge success of gradient based optimization for deep non-linear function approximators and combines it with established (linear) many-view CCA methods. A major contribution of this paper is the derivation of the gradients with respect to the non-linear encoding networks which project the different views into a common space. The derivation seems correct. In general this approach seems very interesting and I could imagine that it might be applicable to many other similarly structured problems.--------The paper is well written; but it could be enhanced with an explicit description of the complete algorithm which also highlights how the joint embeddings G and U are updated. -------- --------I don’t have prior experience with CCA-style many-view techniques and it is therefore hard for me to judge the practical/empirical progress presented here. But the experiments seem reasonable convincing; although generally only performed on small and medium sized datasets.--------  --------Detailed comments: ----------------The colours or the sign of the x-axis in figure 3b seem to be flipped compared to figure 4.--------  --------It would be nice to additionally see a continuous (rainbow-coloured) version for Figures 2, 3 and 4 to better identify neighbouring datapoints; but more importantly: I’d like to see how the average reconstruction error between the individual network outputs and the learned representation develop during training.  Is the mismatch between different views on a validation/test-set a useful metric for cross validation? In general, it seems the method is sensitive to regularization and hyperparameter selection  (because it has many more parameters compared to GCCA and different regularization parameters have been chosen for different views) and I wonder if there is a clear metric to optimize these.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"This is largely a clearly written paper that proposes a nonlinear generalization of a generalized CCA approach for multi-view learning. In terms of technical novelty, the generalization follows rather straightforwardly. Reviewers have expressed the need to clarify relationship and provide comparisons to existing proposals for combining deep learning with CCA. As such the paper has been evaluated to be borderline. The proposed method appears to yield significant gains on a speech dataset, though comparisons on other datasets appear to be less conclusive. Some basic baselines as missing, e.g., concatenating views and running a deep model, or using much older nonlinear extensions of CCA such as kernel CCA (e.g. accelerated via random features, and combined with deep representations underneath).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_887,https://openreview.net/forum?id=HJV1zP5xg,Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models,"Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead com- pared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.",Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models. The paper addresses an important problem - namely on how to improve diversity in responses. It is applaudable that the authors show results on several tasks showing the applicability across different problems. ----------------In my view there are two weaknesses at this point----------------1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper----------------2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted. Having said this the gist of the proposed solution seems interesting but somewhat premature. ,4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_888,https://openreview.net/forum?id=HJV1zP5xg,Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models,"Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead com- pared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.","Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models. [ Summary ]----------------This paper presents a new modified beam search algorithm that promotes diverse beam candidates. It is a well known problem —with both RNNs and also non-neural language models— that beam search tends to generate beam candidates that are very similar with each other, which can cause two separate but related problems: (1) search error: beam search may not be able to discover a globally optimal solution as they can easily fall out of the beam early on, (2) simple, common, non-diverse output: the resulting output text tends to be generic and common.----------------This paper aims to address the second problem (2) by modifying the search objective function itself so that there is a distinct term that scores diversity among the beam candidates. In other words, the goal of the presented algorithm is not to reduce the search error of the original objective function. In contrast, stack decoding and future cost estimation, common practices in phrase-based SMT, aim to address the search error problem.----------------[ Merits ]----------------I think the Diverse Beam Search (DBS) algorithm proposed by the authors has some merits. It may be useful when we cannot rely on traditional beam search on the original objective function either because the trained model is not strong enough, or because of the search error, or because the objective itself does not align with the goal of the application.----------------[ Weaknesses ]----------------It is however not entirely clear how the proposed method compares against more traditional approaches like stack decoding and future cost estimation, on tasks like machine translation, as the authors compare their algorithm mainly against L&J’s diverse LM models and simple beam search.----------------In fact, modification to the objective function has been applied even in the neural MT context. For example, see equation (14) in page 12 of the following paper:----------------""Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"" (https://arxiv.org/pdf/1609.08144v2.pdf)----------------where the attention coverage term serves a role similar to stack decoding (though unlike stack decoding, the objective term is entirely re-defined, more similarly to DBS proposed in this work), and the length penalty may have an effect that indirectly promotes more informative (thus more likely diverse) responses.----------------Comparison against these existing algorithms would make the proposed work more complete.----------------Also, I have a mixed feeling about computing and reporting only *oracle* BLUE, CIDEr, METEOR, etc. Especially given how these oracle scores are very close to each other, and that developing a high performing ranking has not been addressed in this work (and that doing so must be not all that trivial), I’m somewhat skeptical how much of DBS results make a practical difference.----------------------------------------**** [Update after the author responses] ****----------------The authors addressed some of my concerns by adding a new baseline comparison against Wu et al. 2016. Thus I will raise my score to 6. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_889,https://openreview.net/forum?id=HJV1zP5xg,Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models,"Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top B candidates. This tends to result in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. We observe that our method not only improved diversity but also finds better top 1 solutions by controlling for the exploration and exploitation of the search space. Moreover, these gains are achieved with minimal computational or memory overhead com- pared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation, conversation and visual question generation using both standard quantitative metrics and qualitative human studies. We find that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.","Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models. This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.----------------On the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.----------------The novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.----------------The diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.----------------The experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).---------------------------------------update: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_893,https://openreview.net/forum?id=HyecJGP5ge,Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World,"In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model’s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the “neuronal birth” is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. “Neuronal death” is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.","Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World. The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. ----------------The online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments.----------------The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. ----------------Still: this is a nice work!   ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).  The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:  - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity  - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the ""state-of-the-art"" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.  In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.  Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, https://arxiv.org/abs/1511.05077), and augmenting neural networks with extra parameters while retaining previous learning (Progressive Neural Networks, Rusu et al 2016, https://arxiv.org/pdf/1606.04671v3.pdf).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_894,https://openreview.net/forum?id=HyecJGP5ge,Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World,"In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model’s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the “neuronal birth” is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. “Neuronal death” is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.","Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World. The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution.----------------I have a few points to make:----------------1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much. ----------------2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc)----------------In a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain.----------------The paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).  The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:  - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity  - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the ""state-of-the-art"" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.  In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.  Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, https://arxiv.org/abs/1511.05077), and augmenting neural networks with extra parameters while retaining previous learning (Progressive Neural Networks, Rusu et al 2016, https://arxiv.org/pdf/1606.04671v3.pdf).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_895,https://openreview.net/forum?id=HyecJGP5ge,Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World,"In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model’s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the “neuronal birth” is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. “Neuronal death” is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.","Neurogenesis-inspired Dictionary Learning: Online Model Adaption In A Changing World. I'd like to thank the authors for their detailed response and clarifications.----------------This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. ----------------The paper has two main innovations over the baseline approach (Mairal et al): (i) “neuronal birth” which represents an adaptive way of increasing the number of atoms in the dictionary (ii) ""neuronal death"", which corresponds to removing “useless” dictionary atoms.----------------Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.----------------I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.--------The paper is very well written and easy to follow.----------------On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the ""level"" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.----------------The authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,----------------Ramirez, Ignacio, and Guillermo Sapiro. ""An MDL framework for sparse coding and dictionary learning."" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).  The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:  - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity  - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the ""state-of-the-art"" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.  In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.  Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, https://arxiv.org/abs/1511.05077), and augmenting neural networks with extra parameters while retaining previous learning (Progressive Neural Networks, Rusu et al 2016, https://arxiv.org/pdf/1606.04671v3.pdf).",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_896,https://openreview.net/forum?id=BJ_MGwqlg,Rethinking Numerical Representations For Deep Neural Networks,"With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.","Rethinking Numerical Representations For Deep Neural Networks. The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy.----------------The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below.----------------1- The paper is not clear that it is only focusing on neural network inference. Please include the word ""inference"" in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different.----------------2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference.----------------3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper.----------------4- The whole discussion about ""efficient customized precision search"" seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples.----------------5- Nvidia's Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited.----------------More comments:----------------- Parts of the paper discussing ""efficient customized precision search"" are not clear to me.----------------- As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_897,https://openreview.net/forum?id=BJ_MGwqlg,Rethinking Numerical Representations For Deep Neural Networks,"With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.","Rethinking Numerical Representations For Deep Neural Networks. This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference. Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy.----------------Questions:----------------1. Does the custom floating point number representation take into account support for de-normal numbers? --------2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory----------------Comments:----------------1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats. --------2. In my opinion, the claim that switching to custom floating point  lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit’s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims. --------3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited.--------4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_898,https://openreview.net/forum?id=BJ_MGwqlg,Rethinking Numerical Representations For Deep Neural Networks,"With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.","Rethinking Numerical Representations For Deep Neural Networks. The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. ----------------The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?----------------The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.----------------Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. ----------------I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.",5: Marginally below acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_918,https://openreview.net/forum?id=SJgWQPcxl,Multi-view Generative Adversarial Networks,"Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.","Multi-view Generative Adversarial Networks. The paper proposed conditional biGAN and its extension to multi-view biGAN. The main idea of conditional biGAN is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (\tilde{x}) and the output (y), respectively, in addition to standard biGAN formulation.----------------The description on MV-GAN require more revision. Specifically, the definition on aggregating model, \Phi, mapping v and a variable s should be clarified. Looking at Equation (8), I can't find a term that constrains the output domain of function v to be the same as a data domain. ----------------Experimental results are not convincing. In most generation results, the observation is not very well preserved. For example, in Figure 6 the second row, background changes significantly from the observation. We also observe such behavior in digit generation example. Preserving attributes like gender is interesting but doesn't seem to be a strong indication that the model learn to correlate observation and the output through latent variable. ",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"The presented approach builds heavily on recent work, but does provide some novelty. The presentation is generally all right, but there are parts of the manuscript that the reviewers feel needed/needs work. All reviewers note that the evaluation and experimental work could be improved.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_919,https://openreview.net/forum?id=SJgWQPcxl,Multi-view Generative Adversarial Networks,"Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.","Multi-view Generative Adversarial Networks. This paper builds in the bidirectional GAN (BiGAN) to obtain an extension which can handle multiple views of data. To this end, the authors extend the BiGAN for multiple view aggregation. This is an easy task and just requires the introduction of the additional distribution and accompanying discriminator. The main challenging and novel part is in regularizing the model to avoid instabilities. The authors propose a novel KL divergence based constraint.----------------As mentioned above, the approach builds quite heavily on previous ones but it has enough novel elements, in particular the constraint for regularization. This constraint is a reasonable assumption an in practice seems to work well. One downside is that it comes with a parameter \lambda which controls its strength and which is not obvious how to find efficiently. For example, for the MNIST data it takes a very small value, 10^-5 and for the CELEBA it's 10^-3. It could be that the results are not very sensitive to this value, but there's no discussion concerning this aspect. ----------------Apart from the regularization term, the rest of the model construction is well motivated, I agree with the authors that a multi-view approach employing GANs is an interesting topic to consider. ----------------Presentation is in general good although at parts readability is hindered. I feel that the notation is unnecessarily complicated, and some parts of the text too. Furthermore, it wasn't immediately obvious to me what is considered as \tilde{x} and what is y in the experiments. ----------------Additionally, most of the discussion on the well-studied area of multi-view learning (intro and sec. 6) omits reference to important prior work which is not based on neural networks. Indeed, some of the issues mentioned as common in today's methods (discrete outputs only, no density estimation...) do not actually exist in many non-neural network approaches. There are too many works to suggest including in the discussion, but I guess the most relevant ones are in the field of probabilistic and non-linear multi-view learning, which is also what MV-BiGAN is doing.----------------The experiments presented in the paper are nice illustrations but unfortunately insufficient. Firstly, they only cover the task of generating (small) images and correspond to non-real world settings (I'd actually consider all of the experiments as toy experiments). Furthermore, the most difficult of these experiments (sec. 5.3) is quite unconvincing. If Fig. 6 shows some of the best results that can be achieved, then this is rather disappointing. Important details are also missing: what is exactly the attribute vector used? How many instances exist?----------------The two other experiments on 5.1 and 5.2 are well executed. It's important that the authors show a comparison with \lambda=0. ----------------However, beyond showing the validity of the KL term, one can't conclude much about the overall merit of the method as a multi-view learning approach, given the above experiments. ----------------Overall this was a nice paper to read, but seems somehow incomplete.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The presented approach builds heavily on recent work, but does provide some novelty. The presentation is generally all right, but there are parts of the manuscript that the reviewers feel needed/needs work. All reviewers note that the evaluation and experimental work could be improved.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_920,https://openreview.net/forum?id=SJgWQPcxl,Multi-view Generative Adversarial Networks,"Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.","Multi-view Generative Adversarial Networks. This paper presents extensions of bidirectional generative adversarial networks to the conditional setting and multi-view setting. The methods are well-motivated, the mathematical derivations appear to be correct, and the presentation is clear enough to me. ----------------I would suggest this paper to be accepted. However, I find it somewhat limited to only present results for generation tasks. I think a main advantage of using bi-GAN (rather than the standard GAN) is the additional inference model that can learn useful features. I am curious about how good the features are for some other supervised (or semi-supervised) learning tasks and what have they really learned.----------------I also find it interesting that the counterpart of these models under the VAE framework have also been proposed--------- Kihyuk Sohn and Honglak Lee and Xinchen Yan. Learning Structured Output Representation using Deep Conditional Generative Models. NIPS 2015.  (*** contitional VAE ***)--------- Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep Variational Canonical Correlation Analysis. In submission to ICLR 2017. (*** sort of multi-view VAE ***)----------------It would be nice to have discussions and comparisons in future work. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The presented approach builds heavily on recent work, but does provide some novelty. The presentation is generally all right, but there are parts of the manuscript that the reviewers feel needed/needs work. All reviewers note that the evaluation and experimental work could be improved.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_921,https://openreview.net/forum?id=SkC_7v5gx,The Power Of Sparsity In Convolutional Neural Networks,"Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.","The Power Of Sparsity In Convolutional Neural Networks. The paper is about channel sparsity in Convolution layer.--------The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity.  The paper contains detailed literature review.--------In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices.--------The main point is that the paper needs more detailed investigation on different dropout schedule.--------As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_922,https://openreview.net/forum?id=SkC_7v5gx,The Power Of Sparsity In Convolutional Neural Networks,"Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.","The Power Of Sparsity In Convolutional Neural Networks. This paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.  Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.----------------Unfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.  In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.  The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.----------------The section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.  The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.----------------Overall, the method and evaluations show that the basic approach has promise.  However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.  Without this last step, it still seems incomplete to me for a conference paper.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_923,https://openreview.net/forum?id=SkC_7v5gx,The Power Of Sparsity In Convolutional Neural Networks,"Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.","The Power Of Sparsity In Convolutional Neural Networks. The paper experiments with channel to channel sparse neural networks.--------The paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.--------The results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. --------This paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_927,https://openreview.net/forum?id=Hy6b4Pqee,Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","Deep Probabilistic Programming. The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. ----------------However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF.----------------Presentation:--------* Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter.--------* I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example.--------It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. ----------------Experiments:--------* Why is the run time not reported in table 1?--------* What are the ""difficulties around convergence"" encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? --------* Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. --------* How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler?--------* It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models.--------* What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example?----------------Minor comments:--------* Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I'm not sure why alpha=-1 was chosen here. --------* How do you handle discrete distributions (eg Figure 5)?--------* x_real is not defined in Figure 7.--------* I would suggest highlighting M in Figure 8.--------* Comma instead of period after ""rized), In"" on page 8.----------------In conclusion I would say that the software developments presented here are quite exciting, and I'm glad the authors are pushing towards practical and accessible ""inference for all"". In its current form though I am forced to give the submission itself a score of 5.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewerÕs points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_928,https://openreview.net/forum?id=Hy6b4Pqee,Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","Deep Probabilistic Programming. The paper introduces Edward, a probabilistic programming language--------built over TensorFlow and Python, and supporting a broad range of most--------popular contemporary methods in probabilistic machine learning.------------------------Quality:----------------The Edward library provides an extremely impressive collection of--------modern probabilistic inference methods in an easily usable form.--------The paper provides a brief review of the most important techniques--------especially from a representation learning perspective, combined with--------two experiments on implementing various modern variational inference--------methods and GPU-accelerated HMC.----------------The first experiment (variational inference) would be more valuable if--------there was a clear link to complete code to reproduce the results--------provided. The HMC experiment looks OK, except the characterising Stan--------as a hand-optimised implementation seems unfair as the code is clearly--------not hand-optimised for this specific model and hardware configuration.--------I do not think anyone doubts the quality of your implementation, so--------please do not ruin the picture by unsubstantiated sensationalist--------claims. Instead of current drama, I would suggest comparing--------head-to-head against Stan on single core and separately reporting the--------extra speedups you gain from parallelisation and GPU. These numbers--------would also help the readers to estimate the performance of the method--------for other hardware configurations.------------------------Clarity:----------------The paper is in general clearly written and easy to read. The numerous--------code examples are helpful, but also difficult as it is sometimes--------unclear what is missing. It would be very helpful if the authors could--------provide and clearly link to a machine-readable companion (a Jupyter--------notebook would be great, but even text or HTML would be easier to--------copy-paste from than a pdf like the paper) with complete runnable code--------for all the examples.------------------------Originality:----------------The Edward library is clearly a unique collection of probabilistic--------inference methods. In terms of the paper, the main threat to novelty--------comes from previous publications of the same group. The main paper--------refers to Tran et al. (2016a) which covers a lot of similar material,--------although from a different perspective. It is unclear if the other--------paper has been published or submitted somewhere and if so, where.------------------------Significance:----------------It seems very likely Edward will have a profound impact on the field--------of Bayesian machine learning and deep learning.------------------------Other comments:----------------In Sec. 2 you draw a clear distinction between specialised languages--------(including Stan) and Turing-complete languages such as Edward. This--------seems unfair as I believe Stan is also Turing complete. Additionally--------no proof is provided to support the Turing-completeness of Edward.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewerÕs points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_929,https://openreview.net/forum?id=Hy6b4Pqee,Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","Deep Probabilistic Programming. Thank you for an interesting read.----------------I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.----------------The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewerÕs points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_945,https://openreview.net/forum?id=r1BJLw9ex,Adjusting For Dropout Variance In Batch Normalization And Weight Initialization,"We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.","Adjusting For Dropout Variance In Batch Normalization And Weight Initialization. This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.----------------I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.----------------The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.----------------Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. ----------------In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.----------------There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.----------------None of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.----------------Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  ----------------This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_946,https://openreview.net/forum?id=r1BJLw9ex,Adjusting For Dropout Variance In Batch Normalization And Weight Initialization,"We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.","Adjusting For Dropout Variance In Batch Normalization And Weight Initialization. The main observation made in the paper is that the use of dropout increases the variance of neurons. Correcting for this increase in variance, in the parameter initialization, and in the test-time statistics of batch normalization, improves performance, as is shown reasonably convincingly in the experiments.----------------This observation is important, as it applies to many of the models used in the literature. It's not extremely novel (it's been observed in the literature before that our simple dropout approximations at test time do not achieve the accuracy obtained by full Monte Carlo dropout)----------------The paper could use more experimental validation. Specifically:----------------- I'm guessing the correction for dropout variance at test time is not only specific to batch normalization: Standard dropout, in networks without batch normalization, corrects only for the mean at test time (by dividing activations by one minus the dropout probability). This work suggests it would be beneficial to also correct for the variance. Has this been tested?-----------------  How does the dropout variance correction compare to using Monte Carlo dropout at test time? (i.e. just averaging over a large number of random dropout masks)","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_947,https://openreview.net/forum?id=r1BJLw9ex,Adjusting For Dropout Variance In Batch Normalization And Weight Initialization,"We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.","Adjusting For Dropout Variance In Batch Normalization And Weight Initialization. The paper presents an approach for compensating the input/activation variance introduced by dropout in a network. Additionally, a practical inference trick of re-estimating the batch normalization parameters with dropout turned off before testing. ----------------The authors very well show how dropout influences the input/activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing. It is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches. The limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach. Perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach.----------------Authors might consider adding some validation for considering the backpropagation variance. On multiple occasions comparison is drawn against batch normalization which I believe does much more than a weight initialization technique. The presented approach is a good initialization technique just not sure if its better than existing ones.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_957,https://openreview.net/forum?id=B1E7Pwqgl,Cooperative Training Of Descriptor And Generator Networks,"This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a  top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other’s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.","Cooperative Training Of Descriptor And Generator Networks. The authors proposes an interesting idea of connecting the energy-based model (descriptor) and --------the generator network to help each other. The samples from the generator are used as the initialization --------of the descriptor inference. And the revised samples from the descriptor is in turn used to update--------the generator as the target image. ----------------The proposed idea is interesting. However, I think the main flaw is that the advantages of having that --------architecture are not convincingly demonstrated in the experiments. For example, readers will expect --------quantative analysis on how initializing with the samples from the generator helps? Also, the only --------quantative experiment on the reconstruction is also compared to quite old models. Considering that --------the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison --------to that model. ----------------** Minor--------- I'm wondering if the analysis on the convergence is sound when considering the fact that samples --------from SGLD are biased samples (with fixed step size). --------- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.    (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_958,https://openreview.net/forum?id=B1E7Pwqgl,Cooperative Training Of Descriptor And Generator Networks,"This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a  top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other’s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.","Cooperative Training Of Descriptor And Generator Networks. This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”.----------------The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.----------------In a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\tilde{Y}, \hat{X}) instead of ({\tilde{Y}, \tilde{X}) ? Run comparative experiments.----------------The paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.----------------PROS:--------+ Interesting and novel idea--------CONS:--------- Improper experimental protocols--------- Missing baselines--------- Missing diagnostic experiments----------------[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.    (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_959,https://openreview.net/forum?id=B1E7Pwqgl,Cooperative Training Of Descriptor And Generator Networks,"This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a  top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other’s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.","Cooperative Training Of Descriptor And Generator Networks. This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.----------------This is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:--------- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.--------- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.----------------Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.----------------Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.    (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_963,https://openreview.net/forum?id=rJiNwv9gg,Lossy Image Compression With Compressive Autoencoders,"We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.","Lossy Image Compression With Compressive Autoencoders. This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.----------------Pros:--------+ Very clear paper. It should be possible to replicate these results should one be inclined to do so.--------+ The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.----------------Cons:--------- The training procedure seems clunky. It requires multiple training stages, freezing weights, etc.--------- The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.  The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_964,https://openreview.net/forum?id=rJiNwv9gg,Lossy Image Compression With Compressive Autoencoders,"We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.","Lossy Image Compression With Compressive Autoencoders. This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.----------------Pros:--------+ The paper is clear and well-written.--------+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.--------+ The proposed approaches to quantization and rate estimation are sensible and well-justified.----------------Cons:--------- The experimental baselines do not appear to be entirely complete.----------------The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.----------------I have no further specific comments at this time as they were answered sufficiently in the pre-review questions.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.  The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_965,https://openreview.net/forum?id=rJiNwv9gg,Lossy Image Compression With Compressive Autoencoders,"We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.","Lossy Image Compression With Compressive Autoencoders. The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.----------------Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons-------------------1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?----------------2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?----------------3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.  The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_978,https://openreview.net/forum?id=rJJRDvcex,Layer Recurrent Neural Networks,"In this paper, we propose a Layer-RNN (L-RNN) module that is able to learn contextual information adaptively using within-layer recurrence. Our contributions are three-fold:  (i) we propose a hybrid neural network architecture that interleaves traditional convolutional layers with L-RNN module for learning long- range dependencies at multiple levels;  (ii) we show that a L-RNN module can be seamlessly inserted into any convolutional layer of a pre-trained CNN, and the entire network then fine-tuned, leading to a boost in performance;  (iii) we report experiments on the CIFAR-10 classification task, showing that a network with interleaved convolutional layers and L-RNN modules, achieves comparable results (5.39% top1 error) using only 15 layers and fewer parameters to ResNet-164 (5.46%); and on the PASCAL VOC2012 semantic segmentation task, we show that the performance of a pre-trained FCN network can be boosted by 5% (mean IOU) by simply inserting Layer-RNNs.","Layer Recurrent Neural Networks. Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.------------------------Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image. Promising results on classification and semantic labeling are reported.------------------------Review summary:--------The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim. Overall it is not a earth shattering paper, but a good piece of incremental science.------------------------Pros:--------* Clear description--------* Well built experiments--------* Simple yet effective idea--------* No overclaiming--------* Detailed comparison with related work architectures------------------------Cons:--------* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016)--------* Results are good, but do not improve over state of the art------------------------Quality: the ideas are sound, experiments well built and analysed.------------------------Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below)------------------------Originality: minor, this is a different combination of ideas well known.------------------------Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification).------------------------Specific comments:--------* Section 2.2 “we introduction more nonlinearities (through the convolutional layers and ...”. Convolutional layers are linear operators.--------* Section 2.2, why exactly RNN cannot have pooling operators ? I do not see what would impede it.--------* Section 3 “into the computational block”, which block ? Seems like a typo, please rephrase.--------* Figure 2b and 2c not present ? Please fix figure or references to it.--------* Maybe add a short description of GRU in the appendix, for completeness ?--------* Section 5.1, last sentence. Not sure what is meant. The convolutions + relu and pooling in ResNet do provide non-linearities “between layers” too. Please clarify--------* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table ? If there is a human in the loop, what is the variance in results between “two human schedulers” ?--------* Section 5.2.1, last sentence; “we certainly have  a strong baseline”; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results. So no, 64.4 is not “certainly strong”. Please tune down the statement.--------* Section 5.2.3 Modules -> modules--------* The results ignore any mention of increased memory usage or computation cost. This is not a small detail. Please add a discussion on the topic.--------* Section 6 “adding multi-scale spatial” -> “adding spatial” (there is nothing inherently “multi” in the RNN)--------* Section 6 Furthermoe -> Furthermore--------* Appendix C, redundant with Figure 5 ?","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_979,https://openreview.net/forum?id=rJJRDvcex,Layer Recurrent Neural Networks,"In this paper, we propose a Layer-RNN (L-RNN) module that is able to learn contextual information adaptively using within-layer recurrence. Our contributions are three-fold:  (i) we propose a hybrid neural network architecture that interleaves traditional convolutional layers with L-RNN module for learning long- range dependencies at multiple levels;  (ii) we show that a L-RNN module can be seamlessly inserted into any convolutional layer of a pre-trained CNN, and the entire network then fine-tuned, leading to a boost in performance;  (iii) we report experiments on the CIFAR-10 classification task, showing that a network with interleaved convolutional layers and L-RNN modules, achieves comparable results (5.39% top1 error) using only 15 layers and fewer parameters to ResNet-164 (5.46%); and on the PASCAL VOC2012 semantic segmentation task, we show that the performance of a pre-trained FCN network can be boosted by 5% (mean IOU) by simply inserting Layer-RNNs.","Layer Recurrent Neural Networks. The paper proposes a method of integrating recurrent layers within larger, potentially pre-trained, convolutional networks. The objective is to combine the feature extraction abilities of CNNs with the ability of RNNs to gather global context information.--------The authors validate their idea on two tasks, image classification (on CIFAR-10) and semantic segmentation (on PASCAL VOC12).----------------On the positive side, the paper is clear and well-written (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch. The evaluation is also systematic, providing a clear ablation study. ----------------On the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit. --------Regarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel. --------This contribution ("" we use RNNs within layers"") is repeatedly mentioned in the paper (including intro &  conclusion), but in my understanding was part of Bell et al, modulo minor changes. ----------------Regarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept. ----------------Furthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16)--------report  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). So. --------The authors answer: ""Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.""----------------So, we agree that WRN do not need recurrence - and can still do better. --------The point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being ""yes, if you want to keep your network shallow"".  I do not necessarily see why one would want to keep one's network shallow.----------------Probably an evaluation on imagenet would bring some more insight about the merit of this layer. ------------------------Regarding semantic segmentation, one of my questions has been:--------""Is the boost you are obtaining due to something special to the recurrent layer, or is simply because one is adding extra parameters on top of a pre-trained network? (I admit I may have missed some details of your experimental evaluation)""--------The answer was:--------""...For PASCAL segmentation, we add the L-RNN into a pre-trained network (this adds recurrence parameters), and again show that this boosts performance - more so than adding the same number of parameters as extra CNN layers - as it is able to model long-range dependences""--------I could not find one such experiment in the paper ('more so than adding the same number of parameters as extra CNN layers'); I understand that you have 2048 x 2048 connections for the recurrence, it would be interesting to see what you get by spreading them over (non-recurrent) residual layers.--------Clearly, this is not going to be my criterion for rejection/acceptance, since one can easily make it fail - but I was mostly asking for some sanity check ----------------Furthermore, it is a bit misleading to put in Table 3 FCN-8s and FCN8s-LRNN, since this gives the impression that the LRNN gives a  boost by 10%. In practice the ""FCN8s"" prefix of ""FCN8s-LRNN"" is that of the authors, and not of Long et al (as indicated in Table 2, 8s original is quite worse than 8s here). ----------------Another thing that is not clear to me is where the boost comes from in Table 2; the authors mention that ""when inserting the L-RNN after pool 3 and pool4 in FCN-8s, the L-RNN is able to learn contextual information over a much larger range than the receptive field of pure local convolutions. ""--------This is potentially true, but I do not see why this was not also the case for FCN-32s (this is more a property of the recurrence rather than the 8/32 factor, right?)----------------A few additional points: --------It seems like Fig 2b and Fig2c never made it into the pdf. ----------------Figure 4 is unstructured and throws some 30 boxes to the reader - I would be surprised if anyone is able to get some information out of this (why not have a table?) ----------------Appendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)--------What is the performance if you apply a standard training schedule? (e.g. step). --------Appendix C: ""maps .. is"" -> ""maps ... are""",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_980,https://openreview.net/forum?id=rJJRDvcex,Layer Recurrent Neural Networks,"In this paper, we propose a Layer-RNN (L-RNN) module that is able to learn contextual information adaptively using within-layer recurrence. Our contributions are three-fold:  (i) we propose a hybrid neural network architecture that interleaves traditional convolutional layers with L-RNN module for learning long- range dependencies at multiple levels;  (ii) we show that a L-RNN module can be seamlessly inserted into any convolutional layer of a pre-trained CNN, and the entire network then fine-tuned, leading to a boost in performance;  (iii) we report experiments on the CIFAR-10 classification task, showing that a network with interleaved convolutional layers and L-RNN modules, achieves comparable results (5.39% top1 error) using only 15 layers and fewer parameters to ResNet-164 (5.46%); and on the PASCAL VOC2012 semantic segmentation task, we show that the performance of a pre-trained FCN network can be boosted by 5% (mean IOU) by simply inserting Layer-RNNs.","Layer Recurrent Neural Networks. This paper proposes a cascade of paired (left/right, up/down) 1D RNNs as a module in CNNs in order to quickly add global context information to features without the need for stacking many convolutional layers. Experimental results are presented on image classification and semantic segmentation tasks.----------------Pros:--------- The paper is very clear and easy to read.--------- Enough details are given that the paper can likely be reproduced with or without source code.--------- Using 1D RNNs inside CNNs is a topic that deserves more experimental exploration than what exists in the literature.----------------Cons (elaborated on below):--------(1) Contributions relative to, e.g. Bell et al., are minor.--------(2) Disappointed in the actual use of the proposed L-RNN module versus how it's sold in the intro.--------(3) Classification experiments are not convincing.----------------(1,2): The introduction states w.r.t. Bell et al. ""more substantial differences are two fold: first, we treat the L-RNN module as a general block, that can be inserted into any layer of a modern architecture, such as into a residual module. Second, we show (section 4) that the--------L-RNN can be formulated to be inserted into a pre-trained FCN (by initializing with zero recurrence--------matrices), and that the entire network can then be fine-tuned end-to-end.""----------------I felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections. Based on the first contribution (""general block that can be inserted into any layer""), I strongly expected to see the L-RNN block integrated throughout the CNN starting from near the input. However, the architectures for classification and segmentation only place the module towards the very end of the network. While not exactly the same as Bell et al. (there are many technical details that differ), it is close. The paper does not compare to the design from Bell et al. Is there any advantage to the proposed design? Or is it a variation that performs similarly? What happens if L-RNN is integrated earlier in the network, as suggested by the introduction?----------------The second difference is a bit more solid, but still does not rise to a 'substantive difference' in my view. Note that Bell et al. also integrate 1D RNNs into an ImageNet pretrained VGG-16 model. I do, however, think that the method of integration proposed in this paper (zero initialization) may be more elegant and does not require two-stage training by first freezing the lower layers and then later unfreezing them.----------------(3) I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other ask (e.g., detection).----------------One additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.----------------Minor suggestion:--------- Figure 4 is hard to read. The pixelated rounded corners on the yellow boxes are distracting.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_981,https://openreview.net/forum?id=rJJRDvcex,Layer Recurrent Neural Networks,"In this paper, we propose a Layer-RNN (L-RNN) module that is able to learn contextual information adaptively using within-layer recurrence. Our contributions are three-fold:  (i) we propose a hybrid neural network architecture that interleaves traditional convolutional layers with L-RNN module for learning long- range dependencies at multiple levels;  (ii) we show that a L-RNN module can be seamlessly inserted into any convolutional layer of a pre-trained CNN, and the entire network then fine-tuned, leading to a boost in performance;  (iii) we report experiments on the CIFAR-10 classification task, showing that a network with interleaved convolutional layers and L-RNN modules, achieves comparable results (5.39% top1 error) using only 15 layers and fewer parameters to ResNet-164 (5.46%); and on the PASCAL VOC2012 semantic segmentation task, we show that the performance of a pre-trained FCN network can be boosted by 5% (mean IOU) by simply inserting Layer-RNNs.","Layer Recurrent Neural Networks. The authors propose the use of a vertical and horizontal one-dimensional RNN (denoted as L-RNN module) to capture long-range dependencies and summarize convolutional feature maps. L-RNN modules are an alternative to deeper or wider networks, 2D RNNs, dilated (Atrous) convolutional layers, and a simple flatten or global pooling layer when applied to the last convolutional layer for classification. L-RNN modules are faster than 2D RNNs, since rows and columns can be processed in parallel, are easy to implemented, and can be inserted in existing convolutional networks. The authors demonstrate improvements for classification and semantic segmentation.----------------However, further evaluations are required that show for which use cases L-RNNs are superior to alternatives for summarizing convolutional feature maps:----------------1. I suggest to use a fixed CNN with as certain number of layers, and summarize the last feature map by a) a flatten layer, b) global average pooling, c) a 2D RNN, d) and dilated convolutional layers for segmentation. The authors should report both the run-time and number of parameters for these variants in addition to prediction performances. For segmentation, the number of dilated convolutional layers should be chosen such that the number of parameters is similar to a single L-RNN module.----------------2. The authors compare classification performances only on 32x32 CIFAR-10 images. For higher resolution images, the benefit of L-RNN modules to capture long-range dependencies might be more pronounced. I therefore suggest evaluating classification performances on one additional dataset with higher resolution images, e.g. ImageNet or the CUB bird dataset.----------------Additionally, I have the following minor comments:----------------3. The authors use vanilla RNNs. It might be worth investigating LSTMs or GRUs instead.----------------4. For classification, the authors summarize hidden states of the final vertical recurrent layer by global max pooling. Is this different from more common global average pooling or concatenating the final forward and backward recurrent states?----------------5. Table 3 is hard to understand since it mingles datasets (Pascal P and COCO C) and methods (CRF post-processing). I suggest, e.g., using an additional column with CRF ‘yes’ or ‘no’. I further suggest listing the number of parameters and runtime if possible.----------------6. Section 3 does not clearly describe in which order batch-normalization is applied in residual blocks. Figure 2 suggest that the newer BN-ReLU-Conv order described in He et al. (2016) is used. This should be mentioned in the text.----------------Finally, the text needs to be revised to reach publication level quality. Specifically, I have the following comments:----------------7. Equation (1) is the update of a vanilla RNN, which should be stated more clearly. I suggest to first describe (bidirectional) RNNs, to reference GRUs and LSTMs, and then describe how they are applied here to images. Figure 1 should also be referenced in the text.----------------8. In section 2.2, I suggest to describe Bell at al. more clearly. Why are they using eight instead of four RNNs? ----------------9. Section 4 starts with a verbose description about transfer learning, which can be compressed into a single reference or skipped entirely.----------------10. Equation (6) seems to be missing an index i.----------------11.In particular section 5 and 6 contain a lot of clutter and slang, which should be avoided:--------11.1 page 8: ‘As can be seen’, ‘we turn to that case next’--------11.2 page 9: ‘to the very high value’, ‘as noted earlier’,  ‘less context to contribute here’--------11.3 page 10: ‘In fact’, ‘far deeper’, ‘a simple matter of’, ‘there is much left to investigate.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_982,https://openreview.net/forum?id=HJPmdP9le,Efficient Summarization With Read-again And Copy Mechanism,"Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.","Efficient Summarization With Read-again And Copy Mechanism. Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.----------------Contributions:--------The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.----------------Writing:--------The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. ----------------Pros:--------- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.--------- The results are better than the baselines.----------------Cons:--------- The improvements are not that large.--------- Justifications are not strong enough.--------- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.--------- The paper is very application oriented.----------------Question:--------- How does the training speed when compared to the regular LSTM?----------------Some Criticisms:----------------A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn’t consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.--------As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from “read-again” mechanism or the use of “pointing”. --------The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.--------It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are --------The writing of this paper needs more work. In general, it is not very well-written. ----------------Minor comments:----------------Some of the corrections that I would recommend fixing,----------------On page 4: “… better than a single value … ” —> “… scalar gating …”--------On page 4: “… single value lacks the ability to model the variances among these dimensions.” —> “… scalar gating couldn’t capture the ….”--------On page 6: “ … where h_0^2 and h_0^'2 are initial zero vectors … “ —> “… h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence …""----------------There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.----------------Better naming of the models in Table 1 is needed.--------The location of Table 1 is a bit off.----------------[1] Zaremba, Wojciech, and Ilya Sutskever. ""Reinforcement learning neural Turing machines."" arXiv preprint arXiv:1505.00521 362 (2015). --------[2] Gulcehre, Caglar, et al. ""Pointing the Unknown Words."" arXiv preprint arXiv:1603.08148 (2016).",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality.     Issues raised:  - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work.   - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.  - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_983,https://openreview.net/forum?id=HJPmdP9le,Efficient Summarization With Read-again And Copy Mechanism,"Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.","Efficient Summarization With Read-again And Copy Mechanism. This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.----------------1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.-------- --------2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.-------- --------This paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, --------- how much computational costs does the 2nd pass reading add to the end-to-end system? --------- How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?--------- There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality.     Issues raised:  - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work.   - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.  - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_984,https://openreview.net/forum?id=HJPmdP9le,Efficient Summarization With Read-again And Copy Mechanism,"Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.","Efficient Summarization With Read-again And Copy Mechanism. This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. ----------------Detailed comments:-------- ---------Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. -----------------Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?-----------------Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section-----------------Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. -----------------Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.----------------Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality.     Issues raised:  - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work.   - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.  - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_988,https://openreview.net/forum?id=HyGTuv9eg,Incorporating Long-range Consistency In Cnn-based Texture Generation,"Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.","Incorporating Long-range Consistency In Cnn-based Texture Generation. The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. --------The paper claims that this --------a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method--------b) improves performance on texture inpainting tasks compared to the Gatys et al. method--------c) improves results in season transfer when combined with the style transfer method by Gatys et al. --------Furthermore the paper shows that--------d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.----------------I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient.----------------Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms.----------------With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm.----------------Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. ----------------All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance.--------That said, I could imagine this to be a nice contribution to the workshop track though.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns about the computational speed of the approach as well as its advantage over existing methods for some textures, reviewers are excited by the ability of this work to produce structured texture that requires long-range interactions. Overall, the work has contributions that are worth presenting at ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_989,https://openreview.net/forum?id=HyGTuv9eg,Incorporating Long-range Consistency In Cnn-based Texture Generation,"Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.","Incorporating Long-range Consistency In Cnn-based Texture Generation. The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.----------------The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.----------------An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns about the computational speed of the approach as well as its advantage over existing methods for some textures, reviewers are excited by the ability of this work to produce structured texture that requires long-range interactions. Overall, the work has contributions that are worth presenting at ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_990,https://openreview.net/forum?id=HyGTuv9eg,Incorporating Long-range Consistency In Cnn-based Texture Generation,"Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.","Incorporating Long-range Consistency In Cnn-based Texture Generation. This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).----------------The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.----------------My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.----------------All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns about the computational speed of the approach as well as its advantage over existing methods for some textures, reviewers are excited by the ability of this work to produce structured texture that requires long-range interactions. Overall, the work has contributions that are worth presenting at ICLR.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1006,https://openreview.net/forum?id=Hk4_qw5xe,Towards Principled Methods For Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Towards Principled Methods For Training Generative Adversarial Networks. This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. ----------------With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. ----------------I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),1.0
2017_1007,https://openreview.net/forum?id=Hk4_qw5xe,Towards Principled Methods For Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Towards Principled Methods For Training Generative Adversarial Networks. This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.","10: Top 5% of accepted papers, seminal paper",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),0.0
2017_1008,https://openreview.net/forum?id=Hk4_qw5xe,Towards Principled Methods For Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Towards Principled Methods For Training Generative Adversarial Networks. SUMMARY --------This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. ----------------PROS --------Theory, good questions, nice answers. --------Makes an interesting use of concepts form analysis and differential topology. --------Proposes avenues to avoid instability in GANs. ----------------CONS --------A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). ----------------MINOR COMMENTS----------------- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. ----------------- Section 3 provides a nice, intuitive, simple solution. ----------------- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. ----------------- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  ----------------- Lemma 1 would also hold in more generality. ----------------- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). ----------------- In Theorem 2.4, it would be good to remind the reader about p(z). ----------------- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). --------Specify the domain of the random variables. ----------------- relly - > rely ----------------- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)----------------- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. ----------------- Theorem 2.5 ``Therefore'' -> `Then'? ----------------- Theorem 2.6 ``Is a... '' -> `is a' ? ----------------- The number of the theorems is confusing. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),1.0
2017_1009,https://openreview.net/forum?id=Hk4_qw5xe,Towards Principled Methods For Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Towards Principled Methods For Training Generative Adversarial Networks. This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.","10: Top 5% of accepted papers, seminal paper",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Oral),0.0
2017_1010,https://openreview.net/forum?id=SJttqw5ge,Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization,"The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen.  We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.","Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization. Description:----------------This paper presents a reinforcement learning architecture where, based on ""natural-language"" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks.----------------The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an ""analogy-making"" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).----------------The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.----------------Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.----------------The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.--------It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.------------------------Evaluation:----------------The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the ""right"" way to do it.----------------I do not feel the grid world here really represents a ""large-scale task"": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work.----------------Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1011,https://openreview.net/forum?id=SJttqw5ge,Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization,"The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen.  We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.","Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization. This paper presents an architecture and corresponding algorithms for--------learning to act across multiple tasks, described in natural language.--------The proposed system is hierarchical and is closely related to the options--------framework. However, rather than learning a discrete set of options, it learns--------a mapping from natural instructions to an embedding which implicitly (dynamically)--------defines an option. This is a novel and interesting new perspective on options--------which had only slightly been explored in the linear setting (see comments below).--------I find the use of policy distillation particularly relevant for this setting.--------This, on its own, could be a takeaway for many RL readers who might not necessarily--------be interested about NLP applications.----------------In general, the paper does not describe a single, simple, end-to-end,--------recipe for learning with this architecture. It rather relies on many recent--------advances skillfully combined: generalized advantage estimation, analogy-making--------regularizers, L1 regularization, memory addressing, matrix factorization,--------policy distillation. I would have liked to see some analysis but--------understand that it would have certainly been no easy task.--------For example, when you say ""while the parameters of the subtask controller are--------frozen"", this sounds to me like you're having some kind of two-timescale stochastic gradient--------descent. I'm also unsure how you deal with the SMDP structure in your gradient--------updates when you move to the ""temporal abstractions"" setting.----------------I am inclined to believe that this approach has the potential to scale up to--------very large domains, but paper currently does not demonstrate this--------empirically. Like any typical reviewer, I would be tempted to say that--------you should perform larger experiments. However, I'm also glad that you have--------shown that your system also performs well in a ""toy"" domain. The characterization--------in figure 3 is insightful and makes a good point for the analogy regularizer--------and need for hierarchy.----------------Overall, I think that the proposed architecture would inspire other researchers--------and would be worth being presented at ICLR. It also contains novel elements--------(subtask embeddings) which could be useful outside the deep and NLP communities--------into the more ""traditional"" RL communities.----------------# Parameterized Options----------------Sutton et. al (1999) did not explore the concept--------of *parameterized* options originally. It only came later, perhaps first with--------[""Optimal policy switching algorithms for reinforcement--------learning, Comanici & Precup, 2010""] or--------[""Unified Inter and Intra Options Learning Using Policy Gradient Methods"", Levy & Shimkin, 2011].--------Konidaris also has a line of work  on ""parametrized skills"":--------[""Learning Parameterized Skills"". da Silva, Konidaris, Barto, 2012)]--------or [""Reinforcement Learning with Parameterized Actions"". Masson, Ranchod, Konidaris, 2015].----------------Also, I feel that there is a very important distinction to be made with--------the expression ""parametrized options"". In your work, ""parametrized"" comes in--------two flavors. In the spirit of policy gradient methods,--------we can have options whose policies and termination functions are represented--------by function approximators (in the same way that we have function approximation--------for value functions). Those options have parameters and we might call them--------""parameterized"" because of that. This is the setting of Comanicy & Precup (2010),--------Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and--------Mannor (2016) for example.----------------Now, there a second case where options/policies/skills take parameters *as inputs*--------and act accordingly. This is what Konidaris & al. means by ""parameterized"", whose--------meaning differs from the ""function approximation"" case above.--------In your work, the embedding of subtasks arguments is the ""input"" to your options--------and therefore behave as ""parameters"" in the sense of Konidaris.----------------# Related Work----------------I CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's--------work. Branavan's PhD thesis had to do with using control techniques from RL--------in order to interpret natural instructions so as to achieve a goal. For example,--------in ""Reinforcement Learning for Mapping Instructions to Actions"", an RL agent--------learns from ""Windows troubleshooting articles"" to interact with UI elements--------(environment) through a Softmax policy (over linear features) learned by policy--------gradient methods.----------------As you mention under ""Instruction execution"" the focus of your work in--------on generalization, which is not treated explicitely (afaik) in Branavan's work.--------Still, it shares some important algorithmic and architectural similarities which--------should be discussed explicitly or perhaps even compared to in your experiments--------(as a baseline).----------------## Zero-shot and UVFA----------------It might also want to consider--------""Learning Shared Representations for Value Functions in Multi-task--------Reinforcement Learning"", Borsa, Graepel, Shawe-Taylor]--------under the section ""zero-shot tasks generalization"". ------------------------# Minor Issues----------------I first read the abstract without knowing what the paper would be about--------and got confused in the second sentence. You talk about ""longer sequences of--------previously seen instructions"", but I didn't know what clearly--------meant by ""instructions"" until the second to last sentence where you specify--------""instructions described by *natural language*."" You could perhaps--------re-order the sentences to make it clear in the second sentence that you are--------interested in NLP problems.----------------Zero-generalization: I was familiar with the term ""one-shot"" but not ""zero-shot"".--------The way that the second sentence ""[...] to have *similar* zero-shot [...]"" follows--------from the first sentence might as well hold for the ""one-shot"" setting. You--------could perhaps add a citation to ""zero-shot"", or define it more--------explicitly from the beginning and compare it to the one-shot setting. It could--------also be useful if you explain how zero-shot relates to just the notion of--------learning with ""priors"".----------------Under section 3, you say ""cooperate with each other"" which sounds to me very much--------like a multi-agent setting, which your work does not explore in this way.--------You might want to choose a different terminology or explain more precisely if there--------is any connection with the multi-agent setting.----------------The second sentence of section 6 is way to long and difficult to parse. You could--------probably split it in two or three sentences.","7: Good paper, accept","Thank you for the comment and pointing out relevant work.
We’ve posted a common response to the all reviewers as a separate comment above.
We’d appreciate it if you go through the common response as well as this comment.

- Regarding “it relies on many recent advances skillfully combined”
Many recent advances are indeed combined in our paper. We explain why each of technique is needed in the common response. In addition, we revised the paper so that readers can easily differentiate between our own idea and existing recent techniques.

- Regarding dealing with SMDP structure in gradient update in temporal abstraction setting
Just to clarify, the subtask controller is trained first and serves as a parameterized option for the meta controller. So, the two controllers are trained separately. The SMDP structure of the meta controller is implicitly determined by a binary variable c_t in the meta controller itself. The gradient update of the meta controller is also affected by this variable as shown in the Equation (10) in the appendix.

- Regarding parameterized options and other related work
Thank you for pointing our relevant work. We included many of them in the “related work” section.

- Regarding the clarity of the paper (“minor issues”)
Thank you for the detailed comments! We reflected your comments in the revision.
The term “zero-shot learning/generalization” means that the agent generalizes to new tasks “without additional learning process”. This term has been widely used in supervised learning problems where the model should predict previously unseen labels. As you mentioned, zero-shot learning is closely related to “learning with priors”. In order to make zero-shot learning possible, the model (or the agent) should learn prior knowledge about problems (or tasks) and infer the underlying goal given a new problem from the prior. In our work, this prior corresponds to two assumptions. The first assumption is that subtask arguments (e.g., ‘visit / transform / pick up’ v.s. ‘cow / duck / stone …’) are independent of each other, and this is learned through analogy-making. The second assumption is that instructions should be executed sequentially, which is embedded in the structure of the meta controller. By learning or having such priors, the agent can successfully generalize to previously unseen and longer instructions.","The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1012,https://openreview.net/forum?id=SJttqw5ge,Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization,"The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen.  We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.","Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization. This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as ""keep a certain distance from the car in front""). ----------------A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). ----------------Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1013,https://openreview.net/forum?id=SJttqw5ge,Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization,"The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen.  We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.","Communicating Hierarchical Neural Controllers For Learning Zero-shot Task Generalization. The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). ----------------Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. ----------------However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). ----------------The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. ----------------I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. ",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1034,https://openreview.net/forum?id=Sk8csP5ex,The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization,"Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensemble are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network’s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.","The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization. This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.----------------The paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. ----------------It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.----------------This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets.     However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.    I thus cannot recommend acceptance of this paper.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1035,https://openreview.net/forum?id=Sk8csP5ex,The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization,"Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensemble are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network’s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.","The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization. This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).",3: Clear rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets.     However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.    I thus cannot recommend acceptance of this paper.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1036,https://openreview.net/forum?id=Sk8csP5ex,The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization,"Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensemble are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network’s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.","The Loss Surface Of Residual Networks: Ensembles And The Role Of Batch Normalization. Summary:--------In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.----------------Clarity:--------This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.----------------Specific Comments:--------- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)----------------- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)----------------- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets.     However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.    I thus cannot recommend acceptance of this paper.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1037,https://openreview.net/forum?id=r1Usiwcex,Counterpoint By Convolution,"Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce Coconet, a convolutional neural network in the NADE family of generative models. Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation.","Counterpoint By Convolution. The paper tackles the task of music generation. They use an orderless NADE model for the task of ""fill in the notes"". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.----------------This is a well written paper - great job.----------------My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.    This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1038,https://openreview.net/forum?id=r1Usiwcex,Counterpoint By Convolution,"Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce Coconet, a convolutional neural network in the NADE family of generative models. Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation.","Counterpoint By Convolution. The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows----------------The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.--------I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers.----------------Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.----------------Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.    This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1039,https://openreview.net/forum?id=r1Usiwcex,Counterpoint By Convolution,"Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce Coconet, a convolutional neural network in the NADE family of generative models. Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation.","Counterpoint By Convolution. This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.----------------In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.    This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1053,https://openreview.net/forum?id=rkjZ2Pcxe,Adding Gradient Noise Improves Learning For Very Deep Networks,"Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.","Adding Gradient Noise Improves Learning For Very Deep Networks. The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines).----------------On these networks, the question of optimization has so far not been studied as extensively as for more standard networks. A study specific to this class of models is therefore welcome. Results consistently show better optimization properties from adding noise in the training procedure.----------------One issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise. A comparison to results obtained in the literature would be desirable.----------------For example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy). I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance . There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.    We encourage the authors to address these outstanding issues and to resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1054,https://openreview.net/forum?id=rkjZ2Pcxe,Adding Gradient Noise Improves Learning For Very Deep Networks,"Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.","Adding Gradient Noise Improves Learning For Very Deep Networks. This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. ----------------In particular, the authors state ""However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work."" This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.----------------The proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.----------------Unfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017. ",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance . There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.    We encourage the authors to address these outstanding issues and to resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1055,https://openreview.net/forum?id=rkjZ2Pcxe,Adding Gradient Noise Improves Learning For Very Deep Networks,"Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications.  Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we explore the low-overhead and easy-to-implement optimization technique of adding annealed Gaussian noise to the gradient, which we find surprisingly effective when training these very deep architectures.  Unlike classical weight noise, gradient noise injection is complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad. The technique not only helps to avoid overfitting, but also can result in lower training loss. We see consistent improvements in performance across an array of complex models, including state-of-the-art deep networks for question answering and algorithm learning. We observe that this optimization strategy allows a fully-connected 20-layer deep network to escape a bad initialization with standard stochastic gradient descent. We encourage further application of this technique to additional modern neural architectures.","Adding Gradient Noise Improves Learning For Very Deep Networks. The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.----------------The method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn’t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.----------------The paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. ----------------The proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.----------------Despite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. ------------------------Pros:--------* The idea is easy to implement.--------* The method is evaluated on a variety of tasks and for very different models.--------* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.--------* The paper is well-written.------------------------Cons:--------* The idea is not very original.--------* There is no clear theoretical motivation of analysis.--------* Not all the results are convincing.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance . There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.    We encourage the authors to address these outstanding issues and to resubmit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1056,https://openreview.net/forum?id=H178hw9ex,Dynamic Steerable Frame Networks,"Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in Rn. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.","Dynamic Steerable Frame Networks. This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. ----------------This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions:----------------(1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument.----------------(2) The computational cost is not discussed.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented.     The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback.     Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.    - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important).     Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?  Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1057,https://openreview.net/forum?id=H178hw9ex,Dynamic Steerable Frame Networks,"Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in Rn. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.","Dynamic Steerable Frame Networks. I sincerely apologize for the late review!----------------The first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames?----------------The first contribution suggests that ""general frame bases are better suited to represent sensory input data than the commonly used pixel basis."". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the ""Pixel"" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? ----------------I would strongly suggest to improve Fig.3. The Figure uses ""w"" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. ------------------------Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2  I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. ----------------The algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. ",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented.     The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback.     Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.    - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important).     Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?  Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1058,https://openreview.net/forum?id=H178hw9ex,Dynamic Steerable Frame Networks,"Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in Rn. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.","Dynamic Steerable Frame Networks. This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful.----------------Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of ""feature pre-processing"" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement.----------------For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly.----------------In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network.----------------In general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results.----------------There are few minor issues and suggestions related to the paper:--------* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.--------* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?--------* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task.--------* The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary.--------* The hyphenation for ConvNet is incorrect on multiple places (probably `\hyphenation{Conv-Net}` would fix it).","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented.     The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback.     Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.    - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important).     Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?  Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1065,https://openreview.net/forum?id=H13F3Pqll,Inverse Problems In Computer Vision Using Adversarial Imagination Priors,"Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.","Inverse Problems In Computer Vision Using Adversarial Imagination Priors. In this work, the authors propose to use a (perhaps deterministic) retrieval function to replace uniform sampling over the train data in training the discriminator of a GAN.--------Although I like the basic idea, the experiments are very weak.  There are essentially no quantitative results, no real baselines, and only a small amount of not especially convincing qualititative results.   It is honestly hard to review the paper- there isn't any semblance of normal experimental validation.----------------Note:  what is happening with the curves in fig. 6?",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1066,https://openreview.net/forum?id=H13F3Pqll,Inverse Problems In Computer Vision Using Adversarial Imagination Priors,"Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.","Inverse Problems In Computer Vision Using Adversarial Imagination Priors. This paper proposes a model that generates a latent representation of input image(s) and optimizes a reconstruction loss with an adversarial loss (Eq (1)) over nearest neighbors from a bank of images (“memory”).   The framework is adapted to three tasks: (i) image in-painting, (ii) intrinsic image decomposition, (iii) figure-ground layer extraction.  Qualitative results are shown for all three tasks.----------------I think the proposed model has potential merits.  I particularly like the fact that it seems to be reasoning over image composites via matching against a bank of images (somewhat similar to “Segmenting Scenes by Matching Image Composites” work in NIPS 2009).  However, I won’t champion the paper as the overall clarity and evaluation could be improved.----------------More detailed comments:----------------I believe the fatal flaw of the paper is there is no quantitative evaluation of the approach.  At the very least, there should be a comparison against prior work on intrinsic image decomposition (e.g., SIRFS, maybe benchmark on ""intrinsic images in the wild” dataset).----------------I found the writing vague and confusing throughout.  For instance, “memory database” could mean a number of things, and in the end it seems that it’s simply a set of images.  “Imagination” is also vague.  On page 4, R(M,x) has the database and input image as arguments, but Fig 2 doesn’t show the input image as an input to R.  The contributions listed on page 3 should be tightened (e.g., it’s not clear what “Relevant memory retrieval for informative adversarial priors” means).  Fig 3 seems inconsistent with Fig 2 as the module for “memory database” is not present.  The fully-convolutional discriminator could use more details; one possibility is to provide a cost function.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1067,https://openreview.net/forum?id=H13F3Pqll,Inverse Problems In Computer Vision Using Adversarial Imagination Priors,"Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.","Inverse Problems In Computer Vision Using Adversarial Imagination Priors. The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.--------The architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), --------(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, --------and (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.----------------Strong  points.--------- The proposed architecture with memory database is interesting and appears to be novel. ----------------Weak points:--------- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.--------- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. --------- Clarity. The clarity of explanation can be also improved (see below).------------------------Detailed evaluation.----------------Originality:--------- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.----------------- Motivation for the Architecture. The weakest point of the proposed architecture is the ""Memory retrieval engine"" R (section 2.4),--------where images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  --------This should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).------------------------Quality:--------- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  -------- The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  ----------------While this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. ----------------S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.--------ACM Transactions on Graphics, 33(4):159, 2014.----------------Clarity:--------- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the “imagination” and “memory” confusing. From figure 2, it is not clear how the “memories” for the given input image are obtained, which also took me some time to understand.----------------- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the ""feature space”, similar in spirit e.g. to figure 2 in https://arxiv.org/pdf/1612.02136.pdf. Specially, it would be interesting to understand the role of the memory database in this way.------------------------Significance:--------- The paper describes potentially interesting architecture. Given the only proof-of-concept results in toy set-ups, the significance, in the current version, appears only limited. Rather than addressing many different problems, it would be interesting to see benefits of the proposed architecture on realistic challenging data for one of the problems.----------------Overall:--------- The proposed architecture seems novel and potentially interesting, but experiments are only proof-of-concept and clarity can be improved. It is unclear whether the memory matching engine will generalize to other more complicated datasets and problems.  Overall, I am on the edge with this paper, giving the authors the benefit of doubt with a score slightly above the threshold.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.,4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1093,https://openreview.net/forum?id=ByW2Avqgg,Neural Causal Regularization Under The Independence Of Mechanisms Assumption,"Neural networks provide a powerful framework for learning the association between input and response variables and making accurate predictions. However, in many applications such as healthcare, it is important to identify causal relationships between the inputs and the response variables to be able to change the response variables by intervention on the inputs. In pursuit of models whose predictive power comes maximally from causal variables, we propose a novel causal regularizer based on the independence of mechanisms assumption. We utilize the causal regularizer to steer deep neural network architectures towards causally-interpretable solutions. We perform a large-scale analysis of electronic health records. Employing expert's judgment as the causal ground-truth, we show that our causally-regularized algorithm outperforms its L1-regularized equivalence both in predictive performance as well as causal relevance. Finally, we show that the proposed causal regularizer can be used together with representation learning algorithms to yield up to 20% improvement in the causality score of the generated hypotheses.","Neural Causal Regularization Under The Independence Of Mechanisms Assumption. This paper proposes to use a causality score to weight a sparsity regularizer.  In that way, selected variables trade off between being causal and discriminative.  The framework is primarily evaluated on a proprietary health dataset.  While the dataset does give a good motivation to the problem setting, the paper falls a bit short for ICLR due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of Chalupka et al., arXiv Preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.----------------At the core of this paper, the approach is effectively to weight a sparsity regularizer so that ""causal"" variables (as determined by a separate objective) are more likely to be selected.  This is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent.  A theorem on identifiability of causal+discriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers pointed out several issues with the paper, and all recommended rejection. The revision seems to not have been enough to change their minds.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1094,https://openreview.net/forum?id=ByW2Avqgg,Neural Causal Regularization Under The Independence Of Mechanisms Assumption,"Neural networks provide a powerful framework for learning the association between input and response variables and making accurate predictions. However, in many applications such as healthcare, it is important to identify causal relationships between the inputs and the response variables to be able to change the response variables by intervention on the inputs. In pursuit of models whose predictive power comes maximally from causal variables, we propose a novel causal regularizer based on the independence of mechanisms assumption. We utilize the causal regularizer to steer deep neural network architectures towards causally-interpretable solutions. We perform a large-scale analysis of electronic health records. Employing expert's judgment as the causal ground-truth, we show that our causally-regularized algorithm outperforms its L1-regularized equivalence both in predictive performance as well as causal relevance. Finally, we show that the proposed causal regularizer can be used together with representation learning algorithms to yield up to 20% improvement in the causality score of the generated hypotheses.","Neural Causal Regularization Under The Independence Of Mechanisms Assumption. The authors extend their method of causal discovery (Chalupka et al 2016) to include assumptions about sparsity via regularization.  They apply this extension to an interesting private dataset from Sutter Health.  While an interesting direction, I found the presentation somewhat confused, the methodological novelty smaller than the bulk of ICLR works, and the central results (or perhaps data; see below) inadequate to address questions of causality.----------------First, I found the presentation somewhat unclear.  The paper at some points seems to be entirely focused on healthcare data, at other points it uses it as a motivating example, and at other points it is neglected.  Also, algorithm 1 seems unreferenced, and I'm not entirely sure why it is needed.  Figure 2 is not needed for this community.  The key methodological advance in this work appears in section 2.1 (Causal regularizer), but it is introduced amidst toy examples and without clear terminology or standard methodological assumptions/build-up.  In Section 3.1 (bottom of first paragraph), key data and results seem to be relegated to the appendices.  Thus overall the paper read rather haphazardly.  Finally, there seems to be an assumption throughout of fairly intimate familiarity with the Cholupka preprint, which i think should be avoided.  This paper should stand alone.----------------Second, while the technical contributions/novelty are not a focus of the paper's presentation, I am concerned by the lack of methodological advance.  Essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but I can't point to a technical novelty in the paper that the community can not do without.----------------Third, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but I don't think we really have any meaningful quantitative evidence that causality has been learned.  This was briefly discussed (see ""ground truth causality?"" and the response below).  I appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then I think this work is premature, since there is no way to really validate.----------------Overall it's clearly a sincere effort, but I found it wanting in terms of a few critical areas.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers pointed out several issues with the paper, and all recommended rejection. The revision seems to not have been enough to change their minds.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1095,https://openreview.net/forum?id=ByW2Avqgg,Neural Causal Regularization Under The Independence Of Mechanisms Assumption,"Neural networks provide a powerful framework for learning the association between input and response variables and making accurate predictions. However, in many applications such as healthcare, it is important to identify causal relationships between the inputs and the response variables to be able to change the response variables by intervention on the inputs. In pursuit of models whose predictive power comes maximally from causal variables, we propose a novel causal regularizer based on the independence of mechanisms assumption. We utilize the causal regularizer to steer deep neural network architectures towards causally-interpretable solutions. We perform a large-scale analysis of electronic health records. Employing expert's judgment as the causal ground-truth, we show that our causally-regularized algorithm outperforms its L1-regularized equivalence both in predictive performance as well as causal relevance. Finally, we show that the proposed causal regularizer can be used together with representation learning algorithms to yield up to 20% improvement in the causality score of the generated hypotheses.","Neural Causal Regularization Under The Independence Of Mechanisms Assumption. The present submission discusses a ""causal regularizer"", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).----------------+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.----------------- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.----------------- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?----------------- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper ""Towards a Learning Theory of Cause-Effect Inference"" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).----------------On a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers pointed out several issues with the paper, and all recommended rejection. The revision seems to not have been enough to change their minds.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1096,https://openreview.net/forum?id=HkxAAvcxx,Transformation-based Models Of Video Sequences,"In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.  In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.","Transformation-based Models Of Video Sequences. The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.----------------Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.----------------Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying ""if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results"", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1097,https://openreview.net/forum?id=HkxAAvcxx,Transformation-based Models Of Video Sequences,"In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.  In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.","Transformation-based Models Of Video Sequences. Paper Summary--------This paper makes two contributions ---------(1) A model for next step prediction, where the inputs and outputs are in the--------space of affine transforms between adjacent frames.--------(2) An evaluation method in which the quality of the generated data is assessed--------by measuring the reduction in performance of another model (such as a--------classifier) when tested on the generated data.----------------The authors show that according to this metric, the proposed model works better--------than other baseline models (including the recent work of Mathieu et al. which--------uses adversarial training).----------------Strengths--------- This paper attempts to solve a major problem in unsupervised learning--------  with videos, which is evaluating them.--------- The results show that using MSE in transform space does prevent the blurring--------  problem to a large extent (which is one of the main aims of this paper).--------- The results show that the generated data reduces the performance of the C3D--------  model on UCF-101 to a much less extent than other baselines.--------- The paper validates the assumption that videos can be approximated to quite a--------  few time steps by a sequence of affine transforms starting from an initial--------frame.----------------Weaknesses--------- The proposed metric makes sense only if we truly just care about the performance--------  of a particular classifier on a given task. This significantly narrows the--------scope of applicability of this metric because arguably, one the important--------reasons for doing unsupervised learning is to come up a representation that is--------widely applicable across a variety of tasks. The proposed metric would not help--------evaluate generative models designed to achieve this objective.----------------- It is possible that one of the generative models being compared will interact--------  with the idiosyncrasies of the chosen classifier in unintended ways.--------Therefore, it would be hard to draw strong conclusions about the relative--------merits of generative models from the results of such experiments. One way to--------ameliorate this would be to use several different classifiers (C3D,--------dual-stream network, other state-of-the-art methods) and show that the ranking--------of different generative models is consistent across the choice of classifier.--------Adding such experiments would help increase certainty in the conclusions drawn--------in this paper.----------------- Using only 4 or 8 input frames sampled at 25fps seems like very little context--------  if we really expect the model to extrapolate the kind of motion seen in--------UCF-101. The idea of working in the space of affine transforms would be much--------more appealing if the model can be shown to really generated non-trivial motion--------patterns. Currently, the motion patterns seem to be almost linear--------extrapolations.----------------- The model that predicts motion does not have access to content at all. It only--------  gets access to previous motion. It seems that this might be a disadvantage--------because the motion predictor cannot use any cues like object boundaries, or--------decide what to do when two motion fields collide (it is probably easier to argue--------about occlusions in content space).----------------Quality/Clarity--------The paper is clearly written and easy to follow. The assumptions are clearly--------specified and validated. Experimental details seem adequate.----------------Originality--------The idea of generating videos by predicting motion has been used previously.--------Several recent papers also use this idea. However the exact implementation in--------this paper is new. The proposed evaluation protocol is novel.----------------Significance--------The proposed evaluation method is an interesting alternative, especially if it--------is extended to include multiple classifiers representative of different--------state-of-the-art approaches. Given how hard it is to evaluate generative models--------of videos, this paper could help start an effort to standardize on a benchmark--------set.----------------Minor comments and suggestions----------------(1) In the caption for Table 1: ``Each column shows the accuracy on the test set--------when taking a different number of input frames as input"" - ``input"" here refers--------to the input to the classifier (Output of the next step prediction model). However--------in the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8--------with stride 4, and it takes 4 frames at the input"" - here ``input"" refers to--------the input to the next step prediction model. It might be a good idea to rephrase--------these sentences to make the distinction clear.----------------(2) In order to better understand the space of affine transform--------parameters, it might help to include a histogram of these parameters in the--------paper. This can help us see at a glance, what is the typical range of these--------6 parameters, should we expect a lot of outliers, etc.----------------(3) In order to compare transforms A and B, instead of ||A - B||^2, one--------could consider A^{-1}B being close to identity as the metric. Did the authors--------try this ?----------------(4) ""The performance of the classifier on ground truth data is an upper bound on--------the performance of any generative model."" This is not *strictly* true. It is--------possible (though highly unlikely) that a generative model might make the data--------look cleaner, sharper, or highlight some aspect of it which could improve the--------performance of the classifier (even compared to ground truth). This is--------especially true if the the generative model had access to the classifier, it--------could then see what makes the classifier fire and highlight those discriminative--------features in the generated output.----------------Overall--------This paper proposes future prediction in affine transform space. This does--------reduce blurriness and makes the videos look relatively realistic (at least to the--------C3D classifier). However, the paper can be improved by showing that the model can--------predict more non-trivial motion flows and the experiments can be strengthened by--------adding more classifiers besides than C3D.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1098,https://openreview.net/forum?id=HkxAAvcxx,Transformation-based Models Of Video Sequences,"In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.  In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.","Transformation-based Models Of Video Sequences. This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.----------------To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. --------In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.----------------While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:----------------  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.--------  --------  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.--------  --------  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.--------  --------  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. --------  --------  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.----------------  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.----------------  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at http://joo.st/ICLR/ReconstructionsFromGroundTruth are generated from a single groundtruth frame (X_0) which is warped several times for each next frame (X_0 --> Y_1 --> Y_2 --> ...), they said that it was the case. However, it is clear that the reconstruction is based on the previous groundtruth frame (X_t-1 --> Y_t) because it would be impossible to render objects that enter the camera field with motion alone (e.g. the dentist sequence, the top of the toothbrush enter the frame at some point). --------  Perhaps I misunderstood something or there was a misunderstanding between the reviewer and the authors. Here is the original question:--------      (1) In the reconstructions shown on this page - http://joo.st/ICLR/ReconstructionsFromGroundTruth, is the reconstruction at time t (say Y_t) created by applying the estimated affine transform on ground truth at t-1 (X_{t-1}), or by applying it on Y_{t-1} ? In other words, do we start from ground truth X_0 and apply a sequence of transforms, or do we apply the transform to each ground truth frame ?--------  And answer:--------      (1) We start from ground truth X_0 and apply a sequence of transforms. --------  --------  In the end, this qualitative result is not convincing. If the output was just trivally Y_t = X_t-1, then the two videos (X and Y) would render almost the same except for a negligible lag.------------------------To conclude, this paper takes an interesting direction but suffers from important flaws. The most important ones are about the experiments: qualitative experiments are not so convincing (Figure 4 is not as good as in the paper by Srivastava et al) and quantitative results are either missing or questionable. In addition, the network is not really trained end-to-end w.r.t. the final task. Training for the affine loss is hard to intepret and seems to lead to unexpected artifacts. Using a spatial transformer layer would make it possible to train in the image space, e.g. with the robust gradient loss of Mathieu et al. Using per-patch affine spatial transformers is also possible, see the ""Universal Correspondence Network"" of Choy et al. (NIPS'16).",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1117,https://openreview.net/forum?id=ByxpMd9lx,Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks,"Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering.  However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained.  These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.","Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks. The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks.----------------The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared ""layers"" being dependent of the task of interest.----------------The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks.----------------The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale.----------------Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results.----------------Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"One weak and one positive review without much concrete substance. The third review is positive, but the experiments are not that convincing: the gains from transfer are small in table 3 and in table 2 it is unclear how strong the baselines are. Given how competitive ICLR is, the area chair has no alternative than to unfortunately reject this paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1118,https://openreview.net/forum?id=ByxpMd9lx,Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks,"Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering.  However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained.  These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.","Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks. Authors' response well answered my questions. Thanks!--------Evaluation not changed.----------------###----------------This paper proposes a hierarchical framework of transfer learning for sequence tagging, which is expected to help the target task with the source task, by sharing as many levels of representation as possible. It is a general framework for various neural models. The paper has extensive and solid experiments, and the performance is competitive with the state of the art on multiple benchmark datasets. The framework is clear by itself, except that more details about training procedure, i.e. sec-3.3, need to be added. ----------------The experimental results show that for some task pairs {s,t}, this framework can help low-resource target task t, and the improvement increases with more levels of representations can be shared. Firstly, I suggest that the terms *source* and *target* should be more precisely defined in the current framework, because, due to Sec-3.3, the s and t in each pair are sort of interchangeable. That is, either of them can be the *source* or *target* task, especially when p(X=s)=p(X=t)=0.5 is used in the task sampling. The difference is: one is low-resourced and the other is not. Thus it could be thought of as multi-tasking between tasks with imbalanced resource. So one question is: does this framework simultaneously help both tasks in the pair, by learning more generalizable representations for different domains/applications/languages? Or is it mostly likely to only help the low-resourced one? Does it come with sacrifice on the high-resourced side? ----------------Secondly, as the paper shows that the low-resourced tasks are improved for the selected task pairs, it would also be interesting and helpful to know how often this could happen. That is, when the tasks are randomly paired (one chosen from a low-resource pool and the other from a high resource pool), how often could this framework help the low-resourced one?----------------Moreover, the choice of T-A/T-B/T-C lies intuitively in how many levels of representation *could* be shared as possible. This implicitly assumes share more, help more. Although I tend to believe so, it would be interesting to have some empirical comparison. For example, one could perhaps select some cross-domain pair, and see if T-A > T-B > T-C on such pairs, as mentioned in the author’s answer to the pre-review question. ----------------In general, I think this is a solid paper, and more exploration could be done in this direction. So I tend to accept this paper. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"One weak and one positive review without much concrete substance. The third review is positive, but the experiments are not that convincing: the gains from transfer are small in table 3 and in table 2 it is unclear how strong the baselines are. Given how competitive ICLR is, the area chair has no alternative than to unfortunately reject this paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1119,https://openreview.net/forum?id=ByxpMd9lx,Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks,"Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering.  However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained.  These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.","Transfer Learning For Sequence Tagging With Hierarchical Recurrent Networks. This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.----------------It'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"One weak and one positive review without much concrete substance. The third review is positive, but the experiments are not that convincing: the gains from transfer are small in table 3 and in table 2 it is unclear how strong the baselines are. Given how competitive ICLR is, the area chair has no alternative than to unfortunately reject this paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1132,https://openreview.net/forum?id=BkIqod5ll,Convolutional Neural Networks Generalization Utilizing The Data Graph Structure,"Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.","Convolutional Neural Networks Generalization Utilizing The Data Graph Structure. Previous literature uses data-derived adjacency matrix A to obtain neighbors to use as foundation of graph convolution. They propose extending the set of neighbors by additionally including nodes reachable by i<=k steps in this graph. This introduces an extra tunable parameter k, so it needs some justification over the previous k=1 solution. In one experiment provided (Merk), using k=1 worked better. They don't specify which k that used, just that it was big enough for their to be p=5 nodes obtained as neighbors. In the second experiment (MNIST), they used k=1 for their experiments, which is what previous work (Coats & Ng 2011) proposed as well. A compelling experiment would compare to k=1 and show that using k>1 gives improvement strong enough to justify an extra hyper-parameter.",3: Clear rejection,"Please notice that the paper by Coates & Ng (2011) construct locally connected receptive fields in a feed forward neural network style. They connect a given feature with it's 200 most correlated neighbors to create a single output unit. We share the weights and convolve the same weights on all the variables according to the order, generalizing convolutional neural networks. 

The two papers address different neural networks architectures and have different goals - regardless of the hyper-parameter k. 
    
Regarding the hyper-parameter - in 3.3 we explain why lower values of k are preferred over larger values of k. Indeed we have used k=1 when it makes sense. There are situations when this is infeasible, particularly when the graph is sparse. ","This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure.     The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported.     The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version.     The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, https://arxiv.org/pdf/1611.08097v1.pdf' and references therein for more detailed comparisons between the models.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1133,https://openreview.net/forum?id=BkIqod5ll,Convolutional Neural Networks Generalization Utilizing The Data Graph Structure,"Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.","Convolutional Neural Networks Generalization Utilizing The Data Graph Structure. Update: I thank the authors for their comments! After reading them, I decided to increase the rating.----------------This paper proposes a variant of the convolution operation suitable for a broad class of graph structures. For each node in the graph, a set of neighbours is devised by means of random walk (the neighbours are ordered by the expected number of visits). As a result, the graph is transformed into a feature matrix resembling MATLAB’s/Caffe’s im2col output. The convolution itself becomes a matrix multiplication. ----------------Although the proposed convolution variant seems reasonable, I’m not convinced by the empirical evaluation. The MNIST experiment looks especially suspicious. I don’t think that this dataset is appropriate for the demonstration purposes in this case. In order to make their method applicable to the data, the authors remove important structural information (relative locations of pixels) thus artificially increasing the difficulty of the task. At the same time, they are comparing their approach with regular CNNs and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset).----------------I guess, to justify the presence of MNIST (or similar datasets) in the experimental section, the authors should modify their method to incorporate additional graph structure (e.g. relative locations of nodes) in cases when the relation between nodes cannot be fully described by a similarity matrix.----------------I believe, in its current form, the paper is not yet ready for publication but may be later resubmitted to a workshop or another conference after the concern above is addressed.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure.     The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported.     The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version.     The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, https://arxiv.org/pdf/1611.08097v1.pdf' and references therein for more detailed comparisons between the models.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1134,https://openreview.net/forum?id=BkIqod5ll,Convolutional Neural Networks Generalization Utilizing The Data Graph Structure,"Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.","Convolutional Neural Networks Generalization Utilizing The Data Graph Structure. This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.----------------Developing convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. ----------------The two main proposals I see in this paper are:--------1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.--------2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.----------------Perhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. ----------------Specific Comments:--------1) On page 4: ""An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.""  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically). ",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,"This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure.     The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported.     The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version.     The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, https://arxiv.org/pdf/1611.08097v1.pdf' and references therein for more detailed comparisons between the models.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1135,https://openreview.net/forum?id=HyoST_9xl,Dsd: Dense-sparse-dense Training For Deep Neural Networks,"Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50 by 1.1%, respectively. On the WSJ’93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn’t change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.","Dsd: Dense-sparse-dense Training For Deep Neural Networks. Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training.  The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community.----------------The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs. ","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"Important problem, simple (in a positive way) idea, broad experimental evaluation; all reviewers recommend accepting the paper, and the AC agrees. Please incorporate any remaining reviewer feedback.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1136,https://openreview.net/forum?id=HyoST_9xl,Dsd: Dense-sparse-dense Training For Deep Neural Networks,"Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50 by 1.1%, respectively. On the WSJ’93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn’t change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.","Dsd: Dense-sparse-dense Training For Deep Neural Networks. Summary: --------The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations.----------------Pro:--------The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that there’s significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. ----------------Cons & Questions:--------The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs).----------------Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD… approach?","8: Top 50% of accepted papers, clear accept",3: The reviewer is fairly confident that the evaluation is correct,"Important problem, simple (in a positive way) idea, broad experimental evaluation; all reviewers recommend accepting the paper, and the AC agrees. Please incorporate any remaining reviewer feedback.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1137,https://openreview.net/forum?id=HyoST_9xl,Dsd: Dense-sparse-dense Training For Deep Neural Networks,"Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50 by 1.1%, respectively. On the WSJ’93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn’t change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.","Dsd: Dense-sparse-dense Training For Deep Neural Networks. This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results.----------------The proposed idea is novel and interesting.  In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different.----------------The main advantage of the proposed method is its simplicity.  Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase.  Given these, it can be plugged in to training a range of networks, as shown in the experiments.----------------The concern I have is regarding the current empirical evaluation.  As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method.  Standard tricks, such as dropping the learning rate upon ""convergence"" and continuing to learn, can be employed.  The response seems to indicate that these approaches can be effective.  I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper.  An exploration regarding the sparsity hyper-parameter would also be interesting.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Important problem, simple (in a positive way) idea, broad experimental evaluation; all reviewers recommend accepting the paper, and the AC agrees. Please incorporate any remaining reviewer feedback.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1141,https://openreview.net/forum?id=S1_pAu9xl,Trained Ternary Quantization,"Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it’s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16× smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.","Trained Ternary Quantization. The paper shows a different approach to a ternary quantization of weights.--------Strengths:--------1. The paper shows performance improvements over existing solutions--------2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.----------------Weaknesses:--------1. The paper is very incremental.--------2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is ""what is new in the topic"" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience.--------3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation ""it is related to mobile, therefore it is cool"" sufficient.----------------This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.----------------Also - the code was not released is my understanding.",3: Clear rejection,3: The reviewer is fairly confident that the evaluation is correct,The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.,3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1142,https://openreview.net/forum?id=S1_pAu9xl,Trained Ternary Quantization,"Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it’s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16× smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.","Trained Ternary Quantization. This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.--------The group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts. --------I also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.,3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1143,https://openreview.net/forum?id=S1_pAu9xl,Trained Ternary Quantization,"Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it’s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16× smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.","Trained Ternary Quantization. This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values. Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters. This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.----------------Strengths:----------------- Overall well written and algorithm is presented clearly.--------- Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance.--------- I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.----------------Some points:----------------- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Can the authors provide average activation sparsity for each network to help verify this assumption. Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.----------------- In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum). In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set? Does this multiple threshold style network appear in any of the tables or figures? Can it be added?----------------- The authors claim ""ii) Quantized weights play the role of ""learning rate multipliers"" during back propagation."" as a benefit of using trained quantization factors. Why is this a benefit? ----------------- Figure and table captions are not very descriptive.----------------Preliminary Rating:--------I think this is an interesting paper with convincing results but is somewhat lacking in novelty. ----------------Minor notes:--------- Table 3 lists FLOPS rather than Energy for the full precision model. Why?--------- Section 5 'speeding up'--------- 5.1.1 figure reference error last line","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.,3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1144,https://openreview.net/forum?id=S1_pAu9xl,Trained Ternary Quantization,"Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it’s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16× smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.","Trained Ternary Quantization. This paper presents a ternary quantization method for convolutional neural networks. All weights are represented by ternary values multiplied by two scaling coefficients. Both ternary weights and the scaling coefficients are updated using back-propagation. This is useful for CNN model compression. Experiments on AlexNet show that the proposed method is superior Ternary-Weight-Networks (TWN) and DoReFa-Net. This work has the following strengths and weaknesses.----------------Strengths:--------(1). Good results are shown on CIFAR-10 dataset.----------------(2). Massive energy saving of the ternary weights comparing to 32-bit weights.----------------(3). It is well written, and easy to understand.----------------Weaknesses:----------------(1). It seems that this work is an incremental improvement on the existing works. The main difference from Binary-Weight-Networks (BWN) proposed in XNOR-net[1] is using ternary weights instead of binary weights, while ternary weights have been used by many previous works. Both BWN and the proposed method in this paper learn the scaling factors during training. Comparing to Ternary-Weight-Networks (TWN), the main difference is that two independent quantization factors are used for positive and negative weights, while TWN utilizes the same scaling factor for all weights. ----------------(2). In the experiment, the authors did not process the first conv layer and the last fully-connected layer. The results of processing all layers should be given for fair comparison. ----------------(3). In the experiment, comparison with BWN is not reported. In [1], the top-1 and top-5 error of AlexNet on ImageNet of BWN is 43.2% and 20.6%, which is comparable with the method proposed in this paper (42.5% and 20.3%). However, the BWN only uses binary weights and all layers are binarized including the first conv layer and the last fully-connected layer. ----------------(4). For the baseline method of full precision alexnet (with BN), the reported accuracy seems to be too low (44.1% top-1 error). Commonly, using batch normalization can boost the accuracy, while the reported accuracy are much lower than alexnet without batch normalization. On the other hand, the error rates of pre-trained model of alexnet (with BN) reported by the official MatConvNet [2] are 41.8% and 19.2%. ----------------(5). The proposed method should be evaluated on the original AlexNet, whose accuracy is publicly available for almost all deep learning frameworks like caffe. Moreover, more experiments should be added on ImageNet, like VGG-S, VGG-16, GoogleNet or ResNet.----------------(6). In previous methods such as XNOR-net and TWN, most of the 32-bit multiply operation can be replaced by addition by using binary or ternary weights. However, the proposed method in this paper utilizes independent scaling factors for positive and negative weights. Thus it seems that the multiply operation can not be replaced by addition. ------------------------References:--------[1] Mohammad R, Vicente O, Joseph R, Ali F. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016--------[2] http://www.vlfeat.org/matconvnet/pretrained/#imagenet-ilsvrc-classification",3: Clear rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.,3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1145,https://openreview.net/forum?id=S1_pAu9xl,Trained Ternary Quantization,"Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it’s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16× smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.","Trained Ternary Quantization. This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).----------------The relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.----------------Furthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).----------------I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.--------I would have also liked to see discussion of the wall time to result using this training procedure.","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.,3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1170,https://openreview.net/forum?id=BybtVK9lg,Autoencoding Variational Inference For Topic Models,"Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.","Autoencoding Variational Inference For Topic Models. The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)----------------In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. ----------------Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?----------------Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.----------------Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?----------------None of the numbers include error bars. Are the results statistically significant?------------------------Minor comments:----------------Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?----------------The idea of using an inference network is much older, cf. Helmholtz machine. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1171,https://openreview.net/forum?id=BybtVK9lg,Autoencoding Variational Inference For Topic Models,"Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.","Autoencoding Variational Inference For Topic Models. This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.----------------Minor comments:--------Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. --------A typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”----------------In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?----------------In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?----------------How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?----------------It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).----------------Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. ----------------[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14--------[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1172,https://openreview.net/forum?id=BybtVK9lg,Autoencoding Variational Inference For Topic Models,"Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.","Autoencoding Variational Inference For Topic Models. This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:--------Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?--------The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?--------The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf--------Section 4.1: error in the equation. The last term should be --------Prod_i exp(delta*_r_i) * exp((1-delta)*s_i).--------Last paragraph 4.1. The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training. While these aspects may be important in practice they are somewhat incremental.--------I couldn’t find the size of the vocabularies of the datasets in the paper. Does this method work well for very high dimensional sparse document representations?--------The comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me. --------In the end, it’s a useful paper to read, but it’s not going to be the highlight of the conference. The relative increment is somewhat small and seems to heavily rely optimization tricks.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1173,https://openreview.net/forum?id=BybtVK9lg,Autoencoding Variational Inference For Topic Models,"Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.","Autoencoding Variational Inference For Topic Models. The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1209,https://openreview.net/forum?id=BJFG8Yqxl,Group Sparse Cnns For Question Sentence Classification With Answer Sets,"Classifying question sentences into their corresponding categories is an important task with wide applications, for example in many websites' FAQ sections.  However, traditional question classification techniques do not fully utilize the well-prepared answer data which has great potential for improving question sentence representations which could lead to better classification performance. In order to encode answer information into question representation, we first introduce novel group sparse autoencoders which could utilize the group information in the answer set to refine question representation. We then propose a new group sparse convolutional neural network which could naturally learn the question representation with respect to their corresponding answers by implanting the group sparse autoencoders into the traditional convolutional neural network. The proposed model show significant improvements over strong baselines on four datasets.  ","Group Sparse Cnns For Question Sentence Classification With Answer Sets. The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). ----------------Clarity of the paper is a bit low. --------- Do you use only p-th group's activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? --------- In Equation (7), RHS misses the summation over p, and wondering it is a simple typo.--------- Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs).----------------Other comments are as follows:--------- Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn't. I think the fair comparison should be to autoencoders that combines classification for their objective function.--------- Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn't look like 1 (e.g., very last column looks like 3).--------- Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments?--------- The comparison to the baseline model is missing. I believe the baseline model shouldn't be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of \alpha and \beta.----------------Missing reference:--------Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:  - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.  - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.  In its current form, the paper cannot be recommended for acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1210,https://openreview.net/forum?id=BJFG8Yqxl,Group Sparse Cnns For Question Sentence Classification With Answer Sets,"Classifying question sentences into their corresponding categories is an important task with wide applications, for example in many websites' FAQ sections.  However, traditional question classification techniques do not fully utilize the well-prepared answer data which has great potential for improving question sentence representations which could lead to better classification performance. In order to encode answer information into question representation, we first introduce novel group sparse autoencoders which could utilize the group information in the answer set to refine question representation. We then propose a new group sparse convolutional neural network which could naturally learn the question representation with respect to their corresponding answers by implanting the group sparse autoencoders into the traditional convolutional neural network. The proposed model show significant improvements over strong baselines on four datasets.  ","Group Sparse Cnns For Question Sentence Classification With Answer Sets. This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. ----------------Pros: --------- group-sparse auto-encoder seems new to me.--------- extensive experiments on QA tasks. ----------------Cons:--------- The idea is somewhat incremental.--------- Writing need to be improved. --------- Lack of ablation studies to show the effectiveness of the proposed approach. ----------------Moreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:  - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.  - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.  In its current form, the paper cannot be recommended for acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1211,https://openreview.net/forum?id=BJFG8Yqxl,Group Sparse Cnns For Question Sentence Classification With Answer Sets,"Classifying question sentences into their corresponding categories is an important task with wide applications, for example in many websites' FAQ sections.  However, traditional question classification techniques do not fully utilize the well-prepared answer data which has great potential for improving question sentence representations which could lead to better classification performance. In order to encode answer information into question representation, we first introduce novel group sparse autoencoders which could utilize the group information in the answer set to refine question representation. We then propose a new group sparse convolutional neural network which could naturally learn the question representation with respect to their corresponding answers by implanting the group sparse autoencoders into the traditional convolutional neural network. The proposed model show significant improvements over strong baselines on four datasets.  ","Group Sparse Cnns For Question Sentence Classification With Answer Sets. This paper propose to classify questions by leveraging corresponding answers. The proposed method uses group sparse autoencoders to model question groups.----------------The proposed method offers improved accuracy over baselines. But the baseline used is a little stale. Would be interesting to see how it compares to more recent CNN and RNN based methods. It would also be interesting to see the contribution of each components. For example, how much GSA contributed to the improvement.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:  - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.  - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.  In its current form, the paper cannot be recommended for acceptance.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1218,https://openreview.net/forum?id=BJC8LF9ex,Recurrent Neural Networks For Multivariate Time Series With Missing Values,"Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.","Recurrent Neural Networks For Multivariate Time Series With Missing Values. This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes.----------------Positives----------------------------- Clear definition of the task (handling missing values for classification of time series)--------- Many interesting baselines to test the new model against.--------- The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters).--------- The extensive tests done on the datasets is probably the greatest asset of this paper.----------------Negatives------------------------------ The paper could use some double checking for typos.--------- The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.--------- No mention of any methods from the statistics litterature.----------------Here are the two main points of this review that informs my decision:----------------1. The results, while promising, are below expectations. The paper hasn’t been able to convince me that GRU-simple (without intervals) isn’t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it’s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn’t significantly outperforms it.----------------2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient’s condition. That would mean that an expert system based on this data would always seem to be one step behind. --------The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that “simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values” is unsubstantiated and actually disproven later in the paper. --------Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. --------Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. ----------------Given that, while I find that the work that has been put into the paper is above average, I wouldn’t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1219,https://openreview.net/forum?id=BJC8LF9ex,Recurrent Neural Networks For Multivariate Time Series With Missing Values,"Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.","Recurrent Neural Networks For Multivariate Time Series With Missing Values. This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. ----------------pros: --------1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;--------2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. ----------------cons:--------1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. --------2) the datasets used in this paper are small. --------3) the decaying effect might not be able to generalize to other domains. ",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1220,https://openreview.net/forum?id=BJC8LF9ex,Recurrent Neural Networks For Multivariate Time Series With Missing Values,"Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.","Recurrent Neural Networks For Multivariate Time Series With Missing Values. The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.--------IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I’m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1224,https://openreview.net/forum?id=rJ0JwFcex,Neuro-symbolic Program Synthesis,"Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.","Neuro-symbolic Program Synthesis. The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.----------------The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.----------------Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.----------------More comments:----------------I am unclear about the model at several places:--------- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?--------- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?--------- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?----------------Regarding the experiments,--------- Could you present some baseline results on FlashFill benchmark based on previous work?--------- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)--------- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?--------- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?----------------Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1225,https://openreview.net/forum?id=rJ0JwFcex,Neuro-symbolic Program Synthesis,"Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.","Neuro-symbolic Program Synthesis. This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.----------------Questions/Comments:----------------- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?----------------- Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function? ----------------- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1226,https://openreview.net/forum?id=rJ0JwFcex,Neuro-symbolic Program Synthesis,"Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.","Neuro-symbolic Program Synthesis. This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).----------------There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.----------------Other miscellaneous comments:--------* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.--------* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).--------* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.--------* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.--------* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)--------* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1227,https://openreview.net/forum?id=Hk3mPK5gg,Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning,"In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35\% higher score than the second place.","Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning. The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.----------------The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.----------------If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.----------------I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.------------------- Added after rebuttal:----------------I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.  Experts agree that the authors do a good job at justifying the majority of the design decisions.    pros:  - insights into the SOTA Doom player    cons:  - lack of pure technical novelty: the various elements have existed previously    This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.  With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook  as to how features can be combined for SOTA performance on FPS-style scenarios.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1228,https://openreview.net/forum?id=Hk3mPK5gg,Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning,"In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35\% higher score than the second place.","Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning. This paper basically applies A3C to 3D spatial navigation tasks. ----------------- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper-----------------  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust----------------- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. ",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.  Experts agree that the authors do a good job at justifying the majority of the design decisions.    pros:  - insights into the SOTA Doom player    cons:  - lack of pure technical novelty: the various elements have existed previously    This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.  With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook  as to how features can be combined for SOTA performance on FPS-style scenarios.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1229,https://openreview.net/forum?id=Hk3mPK5gg,Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning,"In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35\% higher score than the second place.","Training Agent For First-person Shooter Game With Actor-critic Curriculum Learning. This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.----------------Two of my concerns have remained unanswered (see AnonReviewer2, below). ----------------In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.  Experts agree that the authors do a good job at justifying the majority of the design decisions.    pros:  - insights into the SOTA Doom player    cons:  - lack of pure technical novelty: the various elements have existed previously    This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.  With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook  as to how features can be combined for SOTA performance on FPS-style scenarios.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1238,https://openreview.net/forum?id=Hk95PK9le,Deep Biaffine Attention For Neural Dependency Parsing,"This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark—outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%—and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.","Deep Biaffine Attention For Neural Dependency Parsing. The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it's a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB.----------------The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model ""deeper"", whereas adding linear bias terms isn't confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). ----------------I think that this paper is more proper for NLP conferences. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"When replicating the results of the CoNLL 2017 shared task from the models here https://nlp.stanford.edu/data/CoNLL17_Parser_Models/ , I ran into an issue as I could not get the same performance as is reported in the paper. My steps in reproducing, I showcase English LinES treebank:      download ud-treebanks-conll2017 that was distributed among participants of CoNLL 2017 shared task     download respective model from https://nlp.stanford.edu/data/CoNLL17_Parser_Models/     download test sets ud-test-v2.0-conll2017     tag test set file ""en_lines.conllu"" in ud-test-v2.0-conll2017 by the downloaded model ""English-LinES-Tagger""     parse tagged in step 4 test set file ""en_lines.conllu"" located in ""English-LinES-Tagger"" by downloaded model ""English-LinES-Parser""     evaluate by the conll17_ud_eval.py script with the gold data and parsed result.  The result is different than the one reported in the paper. So what could have gone wrong?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1239,https://openreview.net/forum?id=Hk95PK9le,Deep Biaffine Attention For Neural Dependency Parsing,"This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark—outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%—and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.","Deep Biaffine Attention For Neural Dependency Parsing. The paper brings the new STOA in PTB dependency parsing. The numbers are very impressive.----------------Built upon the framework of K&G parser, this improvement is achieved by mainly two things -- (1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer.----------------Although I think the bilinear modification make some sense intuitively, I don't think this contribution alone is strong enough for a conference publication. The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"When replicating the results of the CoNLL 2017 shared task from the models here https://nlp.stanford.edu/data/CoNLL17_Parser_Models/ , I ran into an issue as I could not get the same performance as is reported in the paper. My steps in reproducing, I showcase English LinES treebank:      download ud-treebanks-conll2017 that was distributed among participants of CoNLL 2017 shared task     download respective model from https://nlp.stanford.edu/data/CoNLL17_Parser_Models/     download test sets ud-test-v2.0-conll2017     tag test set file ""en_lines.conllu"" in ud-test-v2.0-conll2017 by the downloaded model ""English-LinES-Tagger""     parse tagged in step 4 test set file ""en_lines.conllu"" located in ""English-LinES-Tagger"" by downloaded model ""English-LinES-Parser""     evaluate by the conll17_ud_eval.py script with the gold data and parsed result.  The result is different than the one reported in the paper. So what could have gone wrong?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1240,https://openreview.net/forum?id=Hk95PK9le,Deep Biaffine Attention For Neural Dependency Parsing,"This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark—outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%—and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.","Deep Biaffine Attention For Neural Dependency Parsing. This is primarily an engineering paper. The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages.----------------The architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015). The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters.----------------The performance of the model is impressive, but how the performance is achieved is not very impressive. I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"When replicating the results of the CoNLL 2017 shared task from the models here https://nlp.stanford.edu/data/CoNLL17_Parser_Models/ , I ran into an issue as I could not get the same performance as is reported in the paper. My steps in reproducing, I showcase English LinES treebank:      download ud-treebanks-conll2017 that was distributed among participants of CoNLL 2017 shared task     download respective model from https://nlp.stanford.edu/data/CoNLL17_Parser_Models/     download test sets ud-test-v2.0-conll2017     tag test set file ""en_lines.conllu"" in ud-test-v2.0-conll2017 by the downloaded model ""English-LinES-Tagger""     parse tagged in step 4 test set file ""en_lines.conllu"" located in ""English-LinES-Tagger"" by downloaded model ""English-LinES-Parser""     evaluate by the conll17_ud_eval.py script with the gold data and parsed result.  The result is different than the one reported in the paper. So what could have gone wrong?",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1253,https://openreview.net/forum?id=r1Chut9xl,Inference And Introspection In Deep Generative Models Of Sparse Data,"Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ","Inference And Introspection In Deep Generative Models Of Sparse Data. The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model's generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models.----------------To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters \theta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters \phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation).----------------The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text.----------------The second contribution is more interesting. In optimising generative parameters (\theta) and variational parameters (\phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters \phi as *global* variational parameters, and the predicted mean \mu(x) and covariance \Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). --------Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to.----------------Some things aren't entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify).--------The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_\theta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data.----------------The final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document.----------------About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question.----------------The analysis of singular values seems like an interesting way to investigate how the model is using its capacity.  However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them.----------------As for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads ""we create a document comprising a subset of words in the the context’s Wikipedia page."" which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b.----------------In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper introduces a number of ideas / heuristics for learning and interpreting deep generative models of text (tf-idf weighting, a combination of using an inference networks with direct optimization of the variational parameters, a method for inducing context-sensitive word embeddings). Generally, the last bit is the most novel, interesting and promising one, however, I agree with the reviewers that empirical evaluation of this technique does not seem sufficient.     Positive:  -- the ideas are sensible   -- the paper is reasonably well written and clear    Negative  -- most ideas are not so novel  -- the word embedding method requires extra analysis / evaluation, comparison to other methods for producing context-sensitive embeddings, etc",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1254,https://openreview.net/forum?id=r1Chut9xl,Inference And Introspection In Deep Generative Models Of Sparse Data,"Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ","Inference And Introspection In Deep Generative Models Of Sparse Data. First I would like to apologize for the delay in reviewing.----------------Summary : In this paper a variational inference is adapted to deep generative models, showing improvement for non-negative sparse dataset. The authors offer as well a method to interpret the data through the model parameters.----------------The writing is generally clear. The methods seem correct. The introspection approach appears to be original. I found very interesting the experiment on the polysemic word embedding. I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task.----------------Minor :--------Eq. 2: too many closing parentheses","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper introduces a number of ideas / heuristics for learning and interpreting deep generative models of text (tf-idf weighting, a combination of using an inference networks with direct optimization of the variational parameters, a method for inducing context-sensitive word embeddings). Generally, the last bit is the most novel, interesting and promising one, however, I agree with the reviewers that empirical evaluation of this technique does not seem sufficient.     Positive:  -- the ideas are sensible   -- the paper is reasonably well written and clear    Negative  -- most ideas are not so novel  -- the word embedding method requires extra analysis / evaluation, comparison to other methods for producing context-sensitive embeddings, etc",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1255,https://openreview.net/forum?id=r1Chut9xl,Inference And Introspection In Deep Generative Models Of Sparse Data,"Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ","Inference And Introspection In Deep Generative Models Of Sparse Data. This paper introduces three tricks for training deep latent variable models on sparse discrete data:--------1) tf-idf weighting--------2) Iteratively optimizing variational parameters after initializing them with an inference network--------3) A technique for improving the interpretability of the deep model----------------The first idea is sensible but rather trivial as a contribution. The second idea is also sensible, but is conceptually not novel. What is new is the finding that it works well for the dataset used in this paper.----------------The third idea is interesting, and seems to give qualitatively reasonable results. The quantitative semantic similarity results don’t seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper introduces a number of ideas / heuristics for learning and interpreting deep generative models of text (tf-idf weighting, a combination of using an inference networks with direct optimization of the variational parameters, a method for inducing context-sensitive word embeddings). Generally, the last bit is the most novel, interesting and promising one, however, I agree with the reviewers that empirical evaluation of this technique does not seem sufficient.     Positive:  -- the ideas are sensible   -- the paper is reasonably well written and clear    Negative  -- most ideas are not so novel  -- the word embedding method requires extra analysis / evaluation, comparison to other methods for producing context-sensitive embeddings, etc",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1256,https://openreview.net/forum?id=r1Chut9xl,Inference And Introspection In Deep Generative Models Of Sparse Data,"Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ","Inference And Introspection In Deep Generative Models Of Sparse Data. This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.----------------The idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work.----------------Similarly, the idea of further optimizing the ELBO is interesting but not fully explored. It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy.----------------Overall the ideas in this paper are good but I'd like to see them a little more fleshed out.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper introduces a number of ideas / heuristics for learning and interpreting deep generative models of text (tf-idf weighting, a combination of using an inference networks with direct optimization of the variational parameters, a method for inducing context-sensitive word embeddings). Generally, the last bit is the most novel, interesting and promising one, however, I agree with the reviewers that empirical evaluation of this technique does not seem sufficient.     Positive:  -- the ideas are sensible   -- the paper is reasonably well written and clear    Negative  -- most ideas are not so novel  -- the word embedding method requires extra analysis / evaluation, comparison to other methods for producing context-sensitive embeddings, etc",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1266,https://openreview.net/forum?id=HJjiFK5gx,Neural Program Lattices,"We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.","Neural Program Lattices. First I would like to apologize for the late review.----------------This paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al.. This allows them to train their model with less supervision than Reed & de Freitas. ----------------Overall the model is a nice extension of NPI. While it requires less supervision than NPI, it still requires ""sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy"". This may limit the scope of this work.----------------The paper claims that their ""method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic).  Such sequences are relatively easy to gather in many natural settings"". It would be great if the authors clarify what they mean by ""relatively easy to gather in many natural settings"". They also claim that ""the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems"". However, this paper only addresses two toy problems which are neither ""natural settings"" nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition). ----------------In the introduction, the author states that ""Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy."" What are the ""existing techniques"", they are referring to? This work only addresses the problem of long addition and puzzle solving in a block world. Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and Sukhbaatar et al. (""Mazebase: A sandbox for learning from games"") shows that a memory network can solve puzzles in a blockworld with little supervision.----------------In the conclusion,  the author states that ""remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get."" However for all the experiments, they ""include a small number of FULL samples"" (FULL == ""samples with full program traces""). Unfortunately even if this means that they need less FULL examples, they still need ""full program traces"", contradicting their final claim. Moreover, as shown figure 7, their model does not use a ""small number of FULL samples"" but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128. ----------------""All experiments were run with 10 different random seeds"": does the environment change as well between the runs, i.e. are the FULL examples different between the runs? If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft. ----------------Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples.----------------Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( ""123+45"" would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.?----------------Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version.----------------Overall, it is a nice extension of Reed & de Freitas, but I'm a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work). This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before. ----------------Missing references:--------""Learning simple arithmetic procedures"", Cottrell et al.--------""Neural gpus learn algorithms"", Kaiser & Sutskever--------""Mazebase: A sandbox for learning from games"", Sukhbaatar et al.--------""Learning simple algorithms from examples"", Zaremba et al.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper demonstrates a novel (although somewhat obvious) extension to NPI, namely moving away from training exclusively on full traces (in order to model the nested calling of subprograms) to training, in part on low-level program traces. By exploiting a continuous stack in the vein of Das et al. 1992, Joulin and Mikolov 2015, or Grefenstette et al. 2015 (any of which could plausibly have been used here, by my reading, contrary to claims made by the authors in discussion), they can model nested calls with a stack-like structure which needs only partial supervision of a few full traces. The claim is that the resulting model is more sample efficient than the NPI of Reed and de Freitas.    The reviews are mixed. R1 is grumpy about citations, which I see as grounds for rejection, but raises the point that the claims to data efficiency may be overblown since it is only shown that few full traces are needed to train the model, not that it can infer structure without full traces, which would be more impressive. Confusingly, they refer to continuous stacks as probabilistic, which is misleading as while there is (in all variants mentioned above) a possible probabilistic interpretation of non-{0, 1} push/pop operations, the updates to the stack state are deterministic, as are all other aspects of the network except test-time sampling of actions from the multivariate distribution induced by the softmax on output.    R2 has not understood the paper or background material if they mistakenly believe that the stack-like structure and resulting model are probabilistic, as there is no sampling of actions on the stack, but only deterministic state updates. The superficiality of the review combined with the fairly crucial misunderstanding sadly means I cannot rely on it to make a recommendation.    R3 is broadly sympathetic to the paper, while recognising that it still requires information about the program structure through partial FULL supervision in order to learn to manipulate the continuous stack (and thus nested function calls). It is disappointing that the authors did not reply to this point in this review. I acknowledge that they address this claim in part in a response to R1, but the reply is mostly that they will rewrite the formulation of their claims.    Overall, the novelty of this paper lies in the integration of an existing flavour of differentiable data structure, variants of which have recently been presented at NIPS in 2015, into NPI in order to learn program structure. This would have made for an excellent paper if this augmentation had shown that, without partial supervision on the stack operations (thus training solely from low level traces) the model had been able to infer program structures. The need to provide some full traces is disappointing in this respect, although the claim about providing a more data-efficient training regime thanks to the data structure is plausible.   Stil, overall, the PCs encourage the authors to address the remainig issues in the camera reasy version of their paper and to present this as a poster at the main conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1267,https://openreview.net/forum?id=HJjiFK5gx,Neural Program Lattices,"We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.","Neural Program Lattices. The paper presents the Neural Program Lattice (NPL), extending the previous Neural Programmer-Interpreters (NPI). The main idea is to generalize stack manipulation of NPI by making it probabilistic. This allows the content of the stack to be stochastic than deterministic, and the paper describes the feed-forward steps of NPL's program inference similar to the NPI formulation. A new objective function is provided to train the model that maximizes the probability of NPL model correctly predicting operation sequences, from execution traces. We believe this is an important extension. The experimental results illustrate that the NPL is able to learn task executions in a clean setting with perfect observations.----------------The paper is clearly presented and its background literature (i.e., NPI) is well covered. We also believe the paper is presenting a conceptually/technically meaningful extension of NPI, which will be of interest to a broad audience. We are still a bit concerned whether the NPL would be directly applicable for noisy observations (e.g., human skeletons) in a continuous space with less explicit structure, so more discussions will be interesting.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper demonstrates a novel (although somewhat obvious) extension to NPI, namely moving away from training exclusively on full traces (in order to model the nested calling of subprograms) to training, in part on low-level program traces. By exploiting a continuous stack in the vein of Das et al. 1992, Joulin and Mikolov 2015, or Grefenstette et al. 2015 (any of which could plausibly have been used here, by my reading, contrary to claims made by the authors in discussion), they can model nested calls with a stack-like structure which needs only partial supervision of a few full traces. The claim is that the resulting model is more sample efficient than the NPI of Reed and de Freitas.    The reviews are mixed. R1 is grumpy about citations, which I see as grounds for rejection, but raises the point that the claims to data efficiency may be overblown since it is only shown that few full traces are needed to train the model, not that it can infer structure without full traces, which would be more impressive. Confusingly, they refer to continuous stacks as probabilistic, which is misleading as while there is (in all variants mentioned above) a possible probabilistic interpretation of non-{0, 1} push/pop operations, the updates to the stack state are deterministic, as are all other aspects of the network except test-time sampling of actions from the multivariate distribution induced by the softmax on output.    R2 has not understood the paper or background material if they mistakenly believe that the stack-like structure and resulting model are probabilistic, as there is no sampling of actions on the stack, but only deterministic state updates. The superficiality of the review combined with the fairly crucial misunderstanding sadly means I cannot rely on it to make a recommendation.    R3 is broadly sympathetic to the paper, while recognising that it still requires information about the program structure through partial FULL supervision in order to learn to manipulate the continuous stack (and thus nested function calls). It is disappointing that the authors did not reply to this point in this review. I acknowledge that they address this claim in part in a response to R1, but the reply is mostly that they will rewrite the formulation of their claims.    Overall, the novelty of this paper lies in the integration of an existing flavour of differentiable data structure, variants of which have recently been presented at NIPS in 2015, into NPI in order to learn program structure. This would have made for an excellent paper if this augmentation had shown that, without partial supervision on the stack operations (thus training solely from low level traces) the model had been able to infer program structures. The need to provide some full traces is disappointing in this respect, although the claim about providing a more data-efficient training regime thanks to the data structure is plausible.   Stil, overall, the PCs encourage the authors to address the remainig issues in the camera reasy version of their paper and to present this as a poster at the main conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1268,https://openreview.net/forum?id=HJjiFK5gx,Neural Program Lattices,"We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.","Neural Program Lattices. Neural Programmer-Interpreters (NPI) achieves greatly reduced sample complexity and better generalization than flat seq2seq models for program induction, but requires program traces at multiple levels of abstraction for training, which is a very strong form of supervision. One obvious way to improve this situation, addressed in this work, is to only train on the lowest-level traces, with a latent compositional program structure. This makes sense because the ""raw"" low-level traces can be cheaply gathered in many cases just by watching expert demonstrations, without being explicitly told the more temporally abstract structures.----------------This paper shows that a variant of NPI, named NPL, can achieve even better generalization performance with weaker supervision (mostly flat traces), and also extends the model to a new grid world task. Unfortunately, it still requires being told the overall program structure by being given a few *full* execution traces. Still, I see this as important progress. It extends NPI in a quite nontrivial way by introducing a stack mechanism modeling the latent program call structure, which makes the training process much more closely match what the model does at test time. The results tell us that flat execution traces can take us almost all the way toward learning compositional programs from demonstrations - the hard part is of course learning to actually discover the subprogram structure.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper demonstrates a novel (although somewhat obvious) extension to NPI, namely moving away from training exclusively on full traces (in order to model the nested calling of subprograms) to training, in part on low-level program traces. By exploiting a continuous stack in the vein of Das et al. 1992, Joulin and Mikolov 2015, or Grefenstette et al. 2015 (any of which could plausibly have been used here, by my reading, contrary to claims made by the authors in discussion), they can model nested calls with a stack-like structure which needs only partial supervision of a few full traces. The claim is that the resulting model is more sample efficient than the NPI of Reed and de Freitas.    The reviews are mixed. R1 is grumpy about citations, which I see as grounds for rejection, but raises the point that the claims to data efficiency may be overblown since it is only shown that few full traces are needed to train the model, not that it can infer structure without full traces, which would be more impressive. Confusingly, they refer to continuous stacks as probabilistic, which is misleading as while there is (in all variants mentioned above) a possible probabilistic interpretation of non-{0, 1} push/pop operations, the updates to the stack state are deterministic, as are all other aspects of the network except test-time sampling of actions from the multivariate distribution induced by the softmax on output.    R2 has not understood the paper or background material if they mistakenly believe that the stack-like structure and resulting model are probabilistic, as there is no sampling of actions on the stack, but only deterministic state updates. The superficiality of the review combined with the fairly crucial misunderstanding sadly means I cannot rely on it to make a recommendation.    R3 is broadly sympathetic to the paper, while recognising that it still requires information about the program structure through partial FULL supervision in order to learn to manipulate the continuous stack (and thus nested function calls). It is disappointing that the authors did not reply to this point in this review. I acknowledge that they address this claim in part in a response to R1, but the reply is mostly that they will rewrite the formulation of their claims.    Overall, the novelty of this paper lies in the integration of an existing flavour of differentiable data structure, variants of which have recently been presented at NIPS in 2015, into NPI in order to learn program structure. This would have made for an excellent paper if this augmentation had shown that, without partial supervision on the stack operations (thus training solely from low level traces) the model had been able to infer program structures. The need to provide some full traces is disappointing in this respect, although the claim about providing a more data-efficient training regime thanks to the data structure is plausible.   Stil, overall, the PCs encourage the authors to address the remainig issues in the camera reasy version of their paper and to present this as a poster at the main conference.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1269,https://openreview.net/forum?id=rkaRFYcgl,Low-rank Passthrough Neural Networks,"Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these ""Passthrough Networks"" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.","Low-rank Passthrough Neural Networks. The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.----------------Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.----------------An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). ----------------However, there are few problems with the evaluation:----------------- In the highway network experiment, the author does not compare with a baseline.--------We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size , second layer of size , third layer of size ) and not in the gate functions. Also, how did you select the hyperparameter values?.----------------- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.--------Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.-----------------Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.----------------- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.----------------- it is not clear when to use low-rank or low-rank + diagonal from the experiments.----------------Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers seem to agree that the framework presented is not very novel, something I agree with.  The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1270,https://openreview.net/forum?id=rkaRFYcgl,Low-rank Passthrough Neural Networks,"Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these ""Passthrough Networks"" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.","Low-rank Passthrough Neural Networks. The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.--------That said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.",5: Marginally below acceptance threshold,Results are SOTA on the memory tasks as far as I know.,"The reviewers seem to agree that the framework presented is not very novel, something I agree with.  The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1271,https://openreview.net/forum?id=rkaRFYcgl,Low-rank Passthrough Neural Networks,"Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these ""Passthrough Networks"" architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.","Low-rank Passthrough Neural Networks. The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.----------------The paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- ""We presented a framework that unifies the description various types of recurrent and feed-forward--------neural networks as passthrough neural networks."" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)----------------Aside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers seem to agree that the framework presented is not very novel, something I agree with.  The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1272,https://openreview.net/forum?id=HJtN5K9gx,Learning Disentangled Representations In Deep Generative Models,"Deep generative models provide a powerful and flexible means to learn complex distributions over data by incorporating neural networks into latent-variable models. Variational approaches to training such models introduce a probabilistic encoder that casts data, typically unsupervised, into an entangled and unstructured representation space. While unsupervised learning is often desirable, sometimes even necessary, when we lack prior knowledge about what to represent, being able to incorporate domain knowledge in characterising certain aspects of variation in the data can often help learn better disentangled representations. Here, we introduce a new formulation of semi-supervised learning in variational autoencoders that allows precisely this. It permits flexible specification of probabilistic encoders as directed graphical models via a stochastic computation graph, containing both continuous and discrete latent variables, with conditional distributions parametrised by neural networks. We demonstrate how the provision of structure, along with a few labelled examples indicating plausible values for some components of the latent space, can help quickly learn disentangled representations. We then evaluate its ability to do so, both qualitatively by exploring its generative capacity, and quantitatively by using the disentangled representation to perform classification, on a variety of models and datasets.","Learning Disentangled Representations In Deep Generative Models. This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances.----------------Minor:--------- I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_1273,https://openreview.net/forum?id=HJtN5K9gx,Learning Disentangled Representations In Deep Generative Models,"Deep generative models provide a powerful and flexible means to learn complex distributions over data by incorporating neural networks into latent-variable models. Variational approaches to training such models introduce a probabilistic encoder that casts data, typically unsupervised, into an entangled and unstructured representation space. While unsupervised learning is often desirable, sometimes even necessary, when we lack prior knowledge about what to represent, being able to incorporate domain knowledge in characterising certain aspects of variation in the data can often help learn better disentangled representations. Here, we introduce a new formulation of semi-supervised learning in variational autoencoders that allows precisely this. It permits flexible specification of probabilistic encoders as directed graphical models via a stochastic computation graph, containing both continuous and discrete latent variables, with conditional distributions parametrised by neural networks. We demonstrate how the provision of structure, along with a few labelled examples indicating plausible values for some components of the latent space, can help quickly learn disentangled representations. We then evaluate its ability to do so, both qualitatively by exploring its generative capacity, and quantitatively by using the disentangled representation to perform classification, on a variety of models and datasets.","Learning Disentangled Representations In Deep Generative Models. This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.  This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.----------------This development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.  From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).  Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.----------------This flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.----------------The paper's title and the way it is written make me expect a lot more than what is currently in the paper.  I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really ""structured interpretation"" of the generative model (title), but I didn't see any of these.  The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.----------------Aside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.  If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.  Therefore the ""plug-in"" estimation has its limitations.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_1274,https://openreview.net/forum?id=HJtN5K9gx,Learning Disentangled Representations In Deep Generative Models,"Deep generative models provide a powerful and flexible means to learn complex distributions over data by incorporating neural networks into latent-variable models. Variational approaches to training such models introduce a probabilistic encoder that casts data, typically unsupervised, into an entangled and unstructured representation space. While unsupervised learning is often desirable, sometimes even necessary, when we lack prior knowledge about what to represent, being able to incorporate domain knowledge in characterising certain aspects of variation in the data can often help learn better disentangled representations. Here, we introduce a new formulation of semi-supervised learning in variational autoencoders that allows precisely this. It permits flexible specification of probabilistic encoders as directed graphical models via a stochastic computation graph, containing both continuous and discrete latent variables, with conditional distributions parametrised by neural networks. We demonstrate how the provision of structure, along with a few labelled examples indicating plausible values for some components of the latent space, can help quickly learn disentangled representations. We then evaluate its ability to do so, both qualitatively by exploring its generative capacity, and quantitatively by using the disentangled representation to perform classification, on a variety of models and datasets.","Learning Disentangled Representations In Deep Generative Models. This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.----------------I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. ----------------On a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?----------------The experiment in section 4.3 is interesting and demonstrates a useful property of the approach.----------------The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.----------------Overall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.----------------Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. ----------------Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1278,https://openreview.net/forum?id=Bk3F5Y9lx,Epitomic Variational Autoencoders,"In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called `epitome' such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.","Epitomic Variational Autoencoders. This paper is refreshing and elegant in its handling of ""over-sampling"" in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the ""creative"" regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between!----------------On a secondary note, regarding terminology: Pls avoid using ""the KL term"" as in section 2.1, there are so many ""KL terms"" related to VAE-s, it ultimately gets out of control. ""Generative error"" is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a ""KL term"" (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2].----------------[1] https://arxiv.org/pdf/1508.06585v5.pdf--------[2] https://arxiv.org/pdf/1509.00519.pdf",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_1279,https://openreview.net/forum?id=Bk3F5Y9lx,Epitomic Variational Autoencoders,"In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called `epitome' such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.","Epitomic Variational Autoencoders. This paper replaces the Gaussian prior often used in a VAE with a group sparse prior. They modify the approximate posterior function so that it also generates group sparse samples. The development of novel forms for the generative model and inference process in VAEs is an active and important area of research. I don't believe the specific choice of prior proposed in this paper is very well motivated however. I believe several of the conceptual claims are incorrect. The experimental results are unconvincing, and I suspect compare log likelihoods in bits against competing algorithms in nats.----------------Some more detailed comments:----------------In Table 1, the log likelihoods reported for competing techniques are all in nats. The reported log likelihood of cVAE using 10K samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10K k-means mixture model to the data (eg as done in ""A note on the evaluation of generative models""). It should nearly impossible to outperform a 10K k-means mixture on Parzen estimation, which makes me extremely skeptical of these eVAE results. However, if you assume that the eVAE log likelihood is actually in bits, and multiply it by log 2 to convert to nats, then it corresponds to a totally believable log likelihood. Note that some Parzen window implementations report log likelihood in bits. Is this experiment comparing log likelihood in bits to competing log likelihoods in nats? (also, label units -- eg bits or nats -- in table)----------------It would be really, really, good to report and compare the variational lower bound on the log likelihood!! Alternatively, if you are concerned your bound is loose, you can use AIS to get a more exact measure of the log likelihood. Even if the Parzen window results are correct, Parzen estimates of log likelihood are extremely poor. They possess any drawback of log likelihood evaluation (which they approximate), and then have many additional drawbacks as well.----------------The MNIST sample quality does not appear to be visually competitive. Also -- it appears that the images are of the probability of activation for each pixel, rather than actual samples from the model. Samples would be more accurate, but either way make sure to describe what is shown in the figure.----------------There are no experiments on non-toy datasets.----------------I am still concerned about most of the issues I raised in my questions below. Briefly, some comments on the authors' response:----------------1. ""minibatches are constructed to not only have a random subset of training examples but also be balanced w.r.t. to epitome assignment (Alg. 1, ln. 4).""--------Nice! This makes me feel better about why all the epitomes will be used.----------------2. I don't think your response addresses why C_vae would trade off between data reconstruction and being factorial. The approximate posterior is factorial by construction -- there's nothing in C_vae that can make it more or less factorial.----------------3. ""For C_vae to have zero contribution from the KL term of a particular z_d (in other words, that unit is deactivated), it has to have all the examples in the training set be deactivated (KL term of zero) for that unit""--------This isn't true. A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples in the training set, and have non-zero KL for other training examples.----------------4. The VAE loss is trained on a lower bound on the log likelihood, though it does have a term that looks like reconstruction error. Naively, I would imagine that if it overfits, this would correspond to data samples becoming more likely under the generative model.----------------5/6. See Parzen concerns above. It's strange to train a binary model, and then treat it's probability of activation as a sample in a continuous space.----------------6. ""we can only evaluate the model from its samples""--------I don't believe this is true. You are training on a lower bound on the log likelihood, which immediately provides another method of quantitative evaluation. Additionally, you could use techniques such as AIS to compute the exact log likelihood.----------------7. I don't believe Parzen window evaluation is a better measure of model quality, even in terms of sample generation, than log likelihood.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1280,https://openreview.net/forum?id=Bk3F5Y9lx,Epitomic Variational Autoencoders,"In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called `epitome' such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.","Epitomic Variational Autoencoders. The paper presents a version of a variational autoencoder that uses a discrete latent variable that masks the activation of the latent code, making only a subset (an ""epitome"") of the latent variables active for a given sample. The justification for this choice is that by letting different latent variables be active for different samples, the model is forced to use more of the latent code than a usual VAE.--------While the problem of latent variable over pruning is important and has been highlighted in the literature before in the context of variational inference, the proposed solution doesn't seem to solve it beyond, for instance, a mixture of VAEs. Indeed, a mixture of VAEs would have been a great baseline for the experiments in the paper, as it uses a categorical variable (the mixture component) along with multiple VAEs. The main difference between a mixture and an epitomic VAE is the sharing of parameters between the different ""mixture components"" in the epitomic VAE case.--------The experimental section presents misleading results.--------1. The log-likelihood of the proposed models is evaluated with Parzen window estimator. A significantly more accurate lower bound on likelihood that is available for the VAEs is not reported. In reviewer's experience continuous MNIST likelihood of upwards of 900 nats is easy to obtain with a modestly sized VAE.--------2. The exposition changes between dealing with binary MNIST and continuous MNIST experiments. This is confusing, because these versions of the dataset present different challenges for modeling with likelihood-based models. Continuous MNIST is harder to model with high-capacity likelihood optimizing models, because the dataset lies in a proper subspace of the 784-dimensional space (some pixels are always or almost always equal to 0), and hence probability density can be arbitrarily large on this subspace. Models that try to maximize the likelihood often exploit this option of maximizing the likelihood by concentrating the probability around the subspace at the expense of actually modeling the data. The samples of a well-tuned VAE trained on binary MNIST (or a VAE trained on continuous MNIST to which noise has been appropriately added) tend to look much better than the ones presented in experimental results.--------3. The claim that the VAE uses its capacity to ""overfit"" to the training data is not justified. No evidence is presented that the reconstruction likelihood on the training data is significantly higher than the reconstruction likelihood on the test data. It's misleading to use a technical term like ""overfitting"" to mean something else.--------4. The use of dropout in dropout VAE is not specified: is dropout applied to the latent variables, or to the hidden layers of the encoder/decoder? The two options will exhibit very different behaviors.--------5. MNIST eVAE samples and reconstructions look more like a more diverse version of 2d VAE samples/reconstructions - they are blurry, the model doesn't encode precise position of strokes. This is consistent with an interpretation of eVAE as a kind of mixture of smaller VAEs, rather than a higher-dimensional VAE. It is misleading to claim that it outperforms a high-dimensional VAE based on this evidence.----------------In reviewer's opinion the paper is not yet ready for publication. A stronger baseline VAE evaluated with evidence lower bound (or another reliable method) is essential for comparing the proposed eVAE to VAEs.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1281,https://openreview.net/forum?id=Bk3F5Y9lx,Epitomic Variational Autoencoders,"In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called `epitome' such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.","Epitomic Variational Autoencoders. This paper proposes an elegant solution to a very important problem in VAEs, namely that the model over-regularizes itself by killing off latent dimensions. People have used annealing of the KL term and “free bits” to hack around this issue but a better solution is needed.--------The offered solution is to introduce sparsity for the latent representation: for every input only a few latent distributions will be activated but across the dataset many latents can still be learned. --------What I didn’t understand is why the authors need the topology in this latent representation. Why not place a prior over arbitrary subsets of latents? That seems to increase the representational power a lot without compromising the solution to the problem you are trying to solve. Now the number of ways the latents can combine is no longer exponentially large, which seems a pity. --------The first paragraph on p.7 is a mystery to me: “An effect of this …samples”. How can under-utilization of model capacity lead to overfitting?--------The experiments are modest but sufficient. --------This paper has an interesting idea that may resolve a fundamental issue of VAEs and thus deserves a place in this conference.","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_1282,https://openreview.net/forum?id=r1IRctqxg,Sample Importance In Training Deep Neural Networks,"The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that ""easy"" samples -- samples that are correctly and confidently classified at the end of the training -- shape parameters closer to the output, while the ""hard"" samples impact parameters closer to the input to the network. Further, ""easy"" samples are relevant in the early training stages, and ""hard"" in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.","Sample Importance In Training Deep Neural Networks. The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \phi^t_{i,j} is never defined, only \phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).----------------The paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)----------------The study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions-------- - “the overall sample importance is different under different epochs” => yes the norm of the gradient is expected to vary-------- - “Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops” => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables----------------The question of Figure 4 is absurd “Is Sample Importance the same as Negative log-likelihood of a sample?”. Of course not.----------------The results are very bad on CIFAR which discredits the applicability of those results.----------------On Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%----------------Despite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.",2: Strong rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers provided detailed, confident reviews and there was significant discussion between the parties.     Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.    I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:    - Dependence of the criteria on the learning rate (this does not make sense to me); and  - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)    I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1283,https://openreview.net/forum?id=r1IRctqxg,Sample Importance In Training Deep Neural Networks,"The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that ""easy"" samples -- samples that are correctly and confidently classified at the end of the training -- shape parameters closer to the output, while the ""hard"" samples impact parameters closer to the input to the network. Further, ""easy"" samples are relevant in the early training stages, and ""hard"" in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.","Sample Importance In Training Deep Neural Networks. This paper examines the so called ""Sample Importance"" of each sample of a training data set, and its effect to the overall learning process.----------------The paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.--------The paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first. However, it is unclear how one should define ""Easy"" training cases.----------------In addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.----------------Possible Improvements:--------It would be nice to factor out the magnitudes of the gradients to the contribution of ""sample importance"". Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.----------------It would also be interesting if improvements based on ""Sample importance"" could be made to the batch selection algorithm to beat the baseline of random batch selection.------------------------Overall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers provided detailed, confident reviews and there was significant discussion between the parties.     Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.    I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:    - Dependence of the criteria on the learning rate (this does not make sense to me); and  - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)    I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1284,https://openreview.net/forum?id=r1IRctqxg,Sample Importance In Training Deep Neural Networks,"The contribution of each sample during model training varies across training iterations and the model's parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that ""easy"" samples -- samples that are correctly and confidently classified at the end of the training -- shape parameters closer to the output, while the ""hard"" samples impact parameters closer to the input to the network. Further, ""easy"" samples are relevant in the early training stages, and ""hard"" in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.","Sample Importance In Training Deep Neural Networks. (paper summary) The authors introduce the notion of “sample importance”, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the “overall importance”, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.----------------(detailed review)--------I have several objections to this paper. First and foremost, I am not convinced of the “sample importance” as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \sum_t g_i^t vs \sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: “if” the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The “input Fisher” norm, \mathbb{E} \frac{\partial \log p} {\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.----------------The experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.----------------PROS:--------+ extensive experiments----------------CONS:--------- sample importance is a heuristic, not entirely well justified--------- SI yields limited insight into training of neural nets--------- SI does not inform curriculum learning",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers provided detailed, confident reviews and there was significant discussion between the parties.     Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.    I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:    - Dependence of the criteria on the learning rate (this does not make sense to me); and  - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)    I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1288,https://openreview.net/forum?id=ry_sjFqgx,Program Synthesis For Character Level Language Modeling,"We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.  Our experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.","Program Synthesis For Character Level Language Modeling. This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.----------------Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities.----------------Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.----------------Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods.     Pro:  - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.  - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods.       Cons:  - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)    Overall, the PCs have determined that this work deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1289,https://openreview.net/forum?id=ry_sjFqgx,Program Synthesis For Character Level Language Modeling,"We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.  Our experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.","Program Synthesis For Character Level Language Modeling. The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided.----------------Cons/suggestions:--------- The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.--------- The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.--------- More compact/convincing examples of human interpretability would be helpful.----------------Other comments--------- Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.","8: Top 50% of accepted papers, clear accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods.     Pro:  - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.  - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods.       Cons:  - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)    Overall, the PCs have determined that this work deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1290,https://openreview.net/forum?id=ry_sjFqgx,Program Synthesis For Character Level Language Modeling,"We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram.  Our experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.","Program Synthesis For Character Level Language Modeling. This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.----------------Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.----------------It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.----------------*Pros*--------1. Novel approach.--------2. Good results.----------------*Cons*--------1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.----------------*Comments*--------1. Please include n-gram results in the table for Wikipedia results.","8: Top 50% of accepted papers, clear accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods.     Pro:  - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.  - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods.       Cons:  - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)    Overall, the PCs have determined that this work deserves to appear at the conference.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1294,https://openreview.net/forum?id=r1te3Fqel,End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension,"This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.","End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension. SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.----------------THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.----------------The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.----------------Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results.----------------Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don't feel the paper is ready in its current form to be accepted to the conference.----------------Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1295,https://openreview.net/forum?id=r1te3Fqel,End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension,"This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.","End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension. The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.----------------There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:----------------1. The use of convolution model, and--------2. Dynamic chunking----------------Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.----------------The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.----------------The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.----------------In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1296,https://openreview.net/forum?id=r1te3Fqel,End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension,"This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.","End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension. SUMMARY.--------The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.--------The model first encodes the passage and the query using a recurrent neural network.--------With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.--------The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.--------Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.--------Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.--------Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.--------The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.----------------The method is tested on the SQUAD dataset and outperforms the proposed baselines.------------------------------------------OVERALL JUDGMENT--------The method presented in this paper is interesting but not very motivated in some points.--------For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.--------The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.--------In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.------------------------------------------DETAILED COMMENTS----------------Equation (13) i should be s, not s^l.----------------I still do not understand the sentence "" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN"". The RNN is over what all the words in the chunk? in the passage? --------The answer the authors gave in the response does not clarify this point.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1300,https://openreview.net/forum?id=rk9eAFcxg,Variational Recurrent Adversarial Deep Domain Adaptation,"We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.","Variational Recurrent Adversarial Deep Domain Adaptation. The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings.----------------The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4.  ----------------Detailed comments:--------1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA.  What do you think are the two dominant latent factors encoded in figure 1 (c)? ----------------2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this?----------------3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left. ",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper offers a contribution to domain adaptation. The novelty with respect to methodology is modest, utilizing an existing variational RNN formulation and adversarial training method in this setting. But the application is important and results are strong. Improving the analysis of the results, and studying variants of the approach to understand the contribution of each component, will make this paper considerably stronnger. We encourage the authors to revise accordingly.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1301,https://openreview.net/forum?id=rk9eAFcxg,Variational Recurrent Adversarial Deep Domain Adaptation,"We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.","Variational Recurrent Adversarial Deep Domain Adaptation. Update: I thank the authors for their comments! After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.----------------This paper proposes a domain adaptation technique for time series. The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).----------------Pros:----------------1. The authors consider a very important application of domain adaptation.----------------2. The paper is well-written and relatively easy to read.----------------3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.----------------Cons:----------------1. The novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.----------------2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)----------------Additional comments:----------------1. I’m not convinced by the discussion presented in Section 4.4. I don’t think the visualization of firing patterns can be used to support the efficiency of the proposed method.----------------2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.----------------Overall, it’s a solid paper but I’m not sure if it is up to the ICLR standard.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper offers a contribution to domain adaptation. The novelty with respect to methodology is modest, utilizing an existing variational RNN formulation and adversarial training method in this setting. But the application is important and results are strong. Improving the analysis of the results, and studying variants of the approach to understand the contribution of each component, will make this paper considerably stronnger. We encourage the authors to revise accordingly.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1302,https://openreview.net/forum?id=rk9eAFcxg,Variational Recurrent Adversarial Deep Domain Adaptation,"We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.","Variational Recurrent Adversarial Deep Domain Adaptation. This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.----------------Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.----------------I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:----------------- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.----------------- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper offers a contribution to domain adaptation. The novelty with respect to methodology is modest, utilizing an existing variational RNN formulation and adversarial training method in this setting. But the application is important and results are strong. Improving the analysis of the results, and studying variants of the approach to understand the contribution of each component, will make this paper considerably stronnger. We encourage the authors to revise accordingly.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1303,https://openreview.net/forum?id=SJ_QCYqle,Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets,"The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns of interest (including hurricanes, extra-tropical cyclones, weather fronts, blocking events, etc.) found in simulation data for which labeled data is not available at large scale for all simulations of interest. We present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. This architecture is designed to fully model multi-channel simulation data, temporal dynamics and unlabelled data within a reconstruction and prediction framework so as to improve the detection of a wide range of extreme weather events.  Our architecture can be viewed as a 3D convolutional autoencoder with an additional modified one-pass bounding box regression loss.  We demonstrate that our approach is able to leverage temporal information and unlabelled data to improve localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data, and facilitate further work in understanding and mitigating the effects of climate change.","Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets. This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term.------------------------Pros:----------------The application of object detection techniques to extreme weather event detection problem is unique, to my knowledge.----------------The paper is well-written and describes the method well, including a survey of the related work.----------------The best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature.  Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach.------------------------Cons:----------------The benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result.----------------It’s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result;  I’d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases.  The paper does acknowledge this and provide potential explanations in Sec. 4.3, however.----------------As other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion.  On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events.  Still, it would be useful to also report results at higher overlap thresholds.----------------Minor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself.----------------Minor: table 4 -- shouldn’t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers?------------------------Overall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain.   The results aren't striking, but the model is ablated appropriately and shown to be beneficial.  For a final version, it would be nice to see results at higher overlap thresholds.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper addresses the little explored domain (at least from computer vision perspective) of analyzing weather data. All reviewers found the application interesting, yet felt that the technical side was somewhat standard. On the positive note, the authors try to use semi-supervised techniques due to limited labeled data, which is less explored in the object detection domain. The also released code, which is a plus. Overall, the application is nice, however, none of the reviewers was swayed by the paper. I suggest that the authors re-submit their work to a vision conference, where applications like this may be more enthusiastically received.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1304,https://openreview.net/forum?id=SJ_QCYqle,Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets,"The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns of interest (including hurricanes, extra-tropical cyclones, weather fronts, blocking events, etc.) found in simulation data for which labeled data is not available at large scale for all simulations of interest. We present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. This architecture is designed to fully model multi-channel simulation data, temporal dynamics and unlabelled data within a reconstruction and prediction framework so as to improve the detection of a wide range of extreme weather events.  Our architecture can be viewed as a 3D convolutional autoencoder with an additional modified one-pass bounding box regression loss.  We demonstrate that our approach is able to leverage temporal information and unlabelled data to improve localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data, and facilitate further work in understanding and mitigating the effects of climate change.","Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets. [EDIT: The thoughtful author responses addressed my major concerns. The github links for data and code will be really helpful for reproducing results (I haven't looked carefully, but this is great). The revision addressed many issues, including the additional results. As such I am upgrading my rating from a 5 to a 6 and recommend acceptance of the paper.]----------------The paper proposes to apply deep nets to perform detection and localization of extreme weather events in simulated weather data. The problem is related to object detection in computer vision in that the input is a 2D “image” (multichannel spatial weather data) or 3D “video” (temporal version of the data) and the output is a bounding box (spatial-temporal localization of a weather event) and class label (weather event type). It differs from standard object detection in that the input has multiple heterogenous channels and labeled data is scarce.----------------A simple but quite reasonable deep net is proposed for the task based on similar approaches in computer vision. While proposal based systems are most popular in vision currently (in particular Faster-RCNN) the proposed approach is simple and a fine starting point. There is little innovation on the part of the detection system, but as noted, it is a valid application of ideas from computer vision to the task at hand. ----------------The authors propose both a supervised approach (only ground truth bounding box location/label is used) and a semi-supervised approach that additionally incorporates the reconstruction loss as a regularization. In all cases the losses are fairly standard and again, reasonable. The only confusing bit is that the “semi-supervised” loss actually has all the labels used for the “supervised” loss and additionally incorporates the reconstruction loss. Hence, the “semi-supervised” loss is actually stronger, which makes the terminology a bit confusing.----------------The paper is easy to follow, but notation is sloppy. For example, above equation 5 it states that “the loss is a weighted combination of reconstruction error and bounding box regression loss”; actually it’s a combination of the supervised and unsupervised loss (Lsup and Lunsup), and Lrec is not defined (although I assume Lrec=Lunsup). The paper is fairly non-technical, but nevertheless these minor issues should be fixed. (E.g., see also reference to “figure 4 and 4”). ----------------The biggest concern w the paper though is experimental results. Only a single figure and table of results are shown (figure 4 and table 4). The metrics are not defined (what is mean average recall?). Only 2D versus 3D version of the model are shown, and supervised and semi-supervised. Moreover, numbers seem a bit all over the place, without consistent patterns (e.g., why is 2D supervised better than the seemingly much strong 3D semi-supervised?). One of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms? Regardless, the experiments are fairly sparse and ablation studies and more discussion lacking. It is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors).----------------Minor nit: the authors use both a classification loss and an “objectness” loss. I’ve never seen both used together like this (normally objectness is used in two-stage object proposal systems where in the first stage class-agnostic proposals are given and in the second stage these are cropped and a class-specific classifier is applied). I strongly suspect removing the objectness loss would not impact results since the classification loss should provide strictly stronger supervisory signal. Regardless, this is a fairly non-standard choice and should be justified (experimentally).----------------Overall this is a borderline paper. I do believe that it is valuable to apply computer vision techniques to a domain that I’ve see little work on in our community. That being said, I have no expertise on this type of data -- it’s possible this deep learning techniques are now routinely used in the climate science literature (I suspect not, though). Overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature). The use of reconstruction loss to improve results in the data-sparse setting is interesting, but the experimental results are inconclusive. The experimental validation is generally insufficient. Reproducibility for future research is difficult unless the data is open-sourced. Overall, I think this paper is a good start, and with improved experiments and more careful writing I think could eventually make for a decent paper.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper addresses the little explored domain (at least from computer vision perspective) of analyzing weather data. All reviewers found the application interesting, yet felt that the technical side was somewhat standard. On the positive note, the authors try to use semi-supervised techniques due to limited labeled data, which is less explored in the object detection domain. The also released code, which is a plus. Overall, the application is nice, however, none of the reviewers was swayed by the paper. I suggest that the authors re-submit their work to a vision conference, where applications like this may be more enthusiastically received.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1305,https://openreview.net/forum?id=SJ_QCYqle,Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets,"The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns of interest (including hurricanes, extra-tropical cyclones, weather fronts, blocking events, etc.) found in simulation data for which labeled data is not available at large scale for all simulations of interest. We present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. This architecture is designed to fully model multi-channel simulation data, temporal dynamics and unlabelled data within a reconstruction and prediction framework so as to improve the detection of a wide range of extreme weather events.  Our architecture can be viewed as a 3D convolutional autoencoder with an additional modified one-pass bounding box regression loss.  We demonstrate that our approach is able to leverage temporal information and unlabelled data to improve localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data, and facilitate further work in understanding and mitigating the effects of climate change.","Semi-supervised Detection Of Extreme Weather Events In Large Climate Datasets. This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. ----------------For the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.----------------I am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). --------The experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). ----------------I also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? ----------------Finally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.----------------Preliminary Rating:--------I think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). ----------------Clarification:--------In the paper you say ""While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study."" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?----------------Minor notes:-------- Please provide years for Prabhat et al. references rather than a and b.-------- Footnote in 4.2 could be inline text with similar space.-------- 4.3 second paragraph the word table is not capitalized like elsewhere.-------- 4.3 4th paragraph the word section is not capitalized like elsewhere.----------------Edit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper addresses the little explored domain (at least from computer vision perspective) of analyzing weather data. All reviewers found the application interesting, yet felt that the technical side was somewhat standard. On the positive note, the authors try to use semi-supervised techniques due to limited labeled data, which is less explored in the object detection domain. The also released code, which is a plus. Overall, the application is nice, however, none of the reviewers was swayed by the paper. I suggest that the authors re-submit their work to a vision conference, where applications like this may be more enthusiastically received.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1312,https://openreview.net/forum?id=HyenWc5gx,Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning,"Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks.  We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.","Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning. This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A. The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A.--------Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning.------------------------General comments/questions:--------- As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task. All experiments show that you can improve on the new task only. --------- The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading. The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data.--------- One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together. That is, you train the model from scratch on both tasks simultaneously (which sharing parameters). It is not clear this is the same as what is referred to in Sec. 8 as ""joint training"". Can you please explain more clearly what you refer to as joint training?--------- Why can't we find the same baselines in both Table 2 and Table 3? For example Table 2 is missing ""joint training"", and Table 3 is missing GRU trained on the target task.--------- While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task). I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task.------------------------Writing comments--------- The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely.--------- Ensemble methodology paragraph does not fit the flow of the paper. I would rather explain it in the experiments section, rather than including it as part of your approach.--------- Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The proposed approach is not consistently applied for the different experiments; this significantly harms the overall value of the research. The results are also quite domain-specific, and it is not clear if the findings would hold more generally. The paper is not clearly organised or written and does not give a specific enough introduction to the field of transfer learning.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1313,https://openreview.net/forum?id=HyenWc5gx,Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning,"Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks.  We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.","Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning. This paper proposes a regularization technique for neural network training that relies on having multiple related tasks or datasets in a transfer learning setting. The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules. The paper is clearly written and the experiments seem relatively thorough. ----------------Overall this is a nice paper but does not fully address how robust the proposed technique is. For each experiment there seems to be a slightly different application of the proposed technique, or a lot of ensembling and cross validation. I can’t figure out if this is because the proposed technique does not work well in general and thus required a lot of fiddling to get right in experiments, or if this is simply an artifact of ad-hoc experiments to try and get the best performance overall. If more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper. ----------------Overall the proposed method seems nice and possibly useful for other problems. However in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains. The authors could help by discussing this explicitly for all experiments in one place rather than listing the various choices / approaches in each experiment. As an example, these sorts of phrases make me very unsure how reliable the method is in practice versus how much the authors had to engineer this regularizer to perform well:--------“We noticed that equation 8 is actually prone to overfitting away from a good solution on the test set although it often finds a pretty good one early in training. “----------------The introduction section should first review the definitions of transfer learning vs multi-task learning to make the discussion more clear. It also deems justification why “catastrophic forgetting” is actually a problem. If the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting. This paper explores sequential transfer so it’s not obvious why forgetting the source task matters.----------------Section 7 introduces the logical rules engine in a fairly specific context. Rather it would be good state more generally what this system entails to help people figure out how this method would apply to other problems.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The proposed approach is not consistently applied for the different experiments; this significantly harms the overall value of the research. The results are also quite domain-specific, and it is not clear if the findings would hold more generally. The paper is not clearly organised or written and does not give a specific enough introduction to the field of transfer learning.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1314,https://openreview.net/forum?id=HyenWc5gx,Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning,"Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks.  We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.","Representation Stability As A Regularizer For Improved Text Analytics Transfer Learning. This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem. --------It also describes an ensembling strategy for combining models that were learned using transfer learning from different sources.--------It puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition.--------As the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks.----------------These are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus. From the title and abstract/conclusion I would've expected a focus on the transfer learning problem. However, the description of the authors' approach is merely a page, and its evaluation is only another page. In order to show that this idea is a new methodological advance, --------it would've been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem). Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem. That is OK, but I would've rather expected that from a paper called something like ""Improved knowledge transfer and distillation for text analytics"". If accepted, I encourage the authors to change the title to something along those lines.----------------The many pieces also made it hard for me to follow the authors' train of thought. I'm sure the authors had a good reason for their section ordering, but I didn't see the red thread in it. How about re-organizing the sections as follows to discuss one contribution at a time?--------1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10. That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10). One clue that the current structure is suboptimal is that there are 11 sections...----------------I like the authors' idea for transfer learning without catastropic forgetting, and I must admit I would've rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper. I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.  ",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The proposed approach is not consistently applied for the different experiments; this significantly harms the overall value of the research. The results are also quite domain-specific, and it is not clear if the findings would hold more generally. The paper is not clearly organised or written and does not give a specific enough introduction to the field of transfer learning.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1315,https://openreview.net/forum?id=SJZAb5cel,A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.","A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks. The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).----------------Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP ""complexity"") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call ""successive regularization"".----------------Experiments:----------------- The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;----------------- The information ""transferred"" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.----------------- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1316,https://openreview.net/forum?id=SJZAb5cel,A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.","A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks. this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.----------------although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:----------------first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.----------------second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. ----------------third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. ----------------minor issue:--------- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].----------------[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf--------[2] http://www.cs.cmu.edu/~nasmith/LSP/",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1317,https://openreview.net/forum?id=SJZAb5cel,A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.","A Joint Many-task Model: Growing A Neural Network For Multiple Nlp Tasks. The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as “pipeline” — the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don’t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.----------------On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there — sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don’t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn’t strictly speaking fair.----------------I admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it’s training set size (and loss on this task etc).",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1318,https://openreview.net/forum?id=B16dGcqlx,Third Person Imitation Learning,"Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.","Third Person Imitation Learning. This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.----------------While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.----------------Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"pros:  - new problem  - huge number of experimental evaluations, based in part on open-review comments    cons:  - the main critiques related to not enough experiments being run; this has been addressed in the revised version    The current reviewer scores do not yet reflect the many updates provided by the authors.  I therefore currently learn in favour of seeing this paper accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1319,https://openreview.net/forum?id=B16dGcqlx,Third Person Imitation Learning,"Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.","Third Person Imitation Learning. The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.----------------I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.--------There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order)--------- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: ""Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills""; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.--------  There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: ""we find that this simple approach has been able to solve the problems""--------- The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?--------- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?--------- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.--------- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?--------- The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?----------------UPDATE:--------I updated the score. Please see my response to the rebuttal below.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"pros:  - new problem  - huge number of experimental evaluations, based in part on open-review comments    cons:  - the main critiques related to not enough experiments being run; this has been addressed in the revised version    The current reviewer scores do not yet reflect the many updates provided by the authors.  I therefore currently learn in favour of seeing this paper accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1320,https://openreview.net/forum?id=B16dGcqlx,Third Person Imitation Learning,"Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.","Third Person Imitation Learning. The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).----------------The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.----------------I would have expected to see comparison to the following methods added to Figure 3:--------1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.--------2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.--------3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).----------------Including these results would in my view significantly enhance the impact of the paper.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"pros:  - new problem  - huge number of experimental evaluations, based in part on open-review comments    cons:  - the main critiques related to not enough experiments being run; this has been addressed in the revised version    The current reviewer scores do not yet reflect the many updates provided by the authors.  I therefore currently learn in favour of seeing this paper accepted.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1324,https://openreview.net/forum?id=B184E5qee,Improving Neural Language Models With A Continuous Cache,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.","Improving Neural Language Models With A Continuous Cache. The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.----------------The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.----------------The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.----------------My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.","7: Good paper, accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Reviewers agree that this paper is based on a ""trick"" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary.     - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods.     - The clarity of the writing and good use of references was appreciated     - This paper is a nice complement/rebuttal to ""Frustratingly Short Attention Spans in Neural Language Modeling"".    Including the discussion about this paper as it might be helpful as it was controversial:     """"""  The technical contribution may appear ""limited"" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing ""data"" with ""timesteps""). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.    (Timesteps)    Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. ""Frustratingly Short Attention Spans in Neural Language Modeling"" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.    This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.    (Data)    Speaking to AnonReviewer1's comment, ""A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)""    Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.    The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.  """"""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1325,https://openreview.net/forum?id=B184E5qee,Improving Neural Language Models With A Continuous Cache,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.","Improving Neural Language Models With A Continuous Cache. This paper proposes a simple extension to a neural network language model by adding a cache component. --------The model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup. --------The final probability of a word is a linear interpolation between a standard language model and the cache language model. --------Additionally, an alternative that uses global normalization instead of linear interpolation is also presented. --------Experiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.----------------There is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.--------However, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive. --------While it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks). --------A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)----------------Some questions:--------- In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both? --------- Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset. --------- It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse?",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Reviewers agree that this paper is based on a ""trick"" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary.     - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods.     - The clarity of the writing and good use of references was appreciated     - This paper is a nice complement/rebuttal to ""Frustratingly Short Attention Spans in Neural Language Modeling"".    Including the discussion about this paper as it might be helpful as it was controversial:     """"""  The technical contribution may appear ""limited"" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing ""data"" with ""timesteps""). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.    (Timesteps)    Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. ""Frustratingly Short Attention Spans in Neural Language Modeling"" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.    This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.    (Data)    Speaking to AnonReviewer1's comment, ""A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)""    Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.    The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.  """"""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_1326,https://openreview.net/forum?id=B184E5qee,Improving Neural Language Models With A Continuous Cache,"We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.","Improving Neural Language Models With A Continuous Cache. This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.----------------They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.--------I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.----------------I recommend this interesting and well analyzed paper be accepted.","9: Top 15% of accepted papers, strong accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Reviewers agree that this paper is based on a ""trick"" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary.     - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods.     - The clarity of the writing and good use of references was appreciated     - This paper is a nice complement/rebuttal to ""Frustratingly Short Attention Spans in Neural Language Modeling"".    Including the discussion about this paper as it might be helpful as it was controversial:     """"""  The technical contribution may appear ""limited"" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing ""data"" with ""timesteps""). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.    (Timesteps)    Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. ""Frustratingly Short Attention Spans in Neural Language Modeling"" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.    This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.    (Data)    Speaking to AnonReviewer1's comment, ""A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)""    Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.    The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.  """"""",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1327,https://openreview.net/forum?id=ByEPMj5el,Out-of-class Novelty Generation: An Experimental Foundation,"Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.","Out-of-class Novelty Generation: An Experimental Foundation. This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model's ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model's ability to generate new and interesting object types.----------------I am not very familiar with the literature on computational creativity research, so I can't judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking.----------------As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation.----------------Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper aims to present an experimental framework for selecting machine learning models that can generate novel objects. As the work is devoted to a relatively subjective area of study, it is not surprising that opinions of the work are mixed.    A large section of the paper is devoted to review, and more detail could be given to the experimental framework. It is not clear whether the framework can actually be useful outside the synthetic setup described. Moreover, I worry it encourages unhealthy directions for the field. Over 1000 models were trained and evaluated. There is no form of separate held-out comparison: the framework encourages people to keep trying random stuff until the chosen measure reports success.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1328,https://openreview.net/forum?id=ByEPMj5el,Out-of-class Novelty Generation: An Experimental Foundation,"Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.","Out-of-class Novelty Generation: An Experimental Foundation. First, the bad:----------------This paper is frustratingly written. The grammar is fine, but:-------- - The first four pages are completely theoretical and difficult to follow without any concrete examples. These sections would benefit greatly from a common example woven through the different aspects of the theoretical discussion.-------- - The ordering of the exposition is also frustrating. I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order. Perhaps a reordering of some details could fix this. Recommendation: give the most naturally ordered oral presentation of the work and then order the paper similarly.----------------Finally, the description of the experiments is cursory, and I found myself wondering whether the details omitted were important or not. Including experimental details in a supplementary section could help assuage these fears.----------------The good:----------------What the paper does well is to gather together past work on novelty generation and propose a unified framework in which to evaluate past and future models. This is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty. The experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer’s knowledge).----------------Overall I recommend the paper be accepted, but I strongly recommend rewriting some components to make it more digestible. As with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible.",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper aims to present an experimental framework for selecting machine learning models that can generate novel objects. As the work is devoted to a relatively subjective area of study, it is not surprising that opinions of the work are mixed.    A large section of the paper is devoted to review, and more detail could be given to the experimental framework. It is not clear whether the framework can actually be useful outside the synthetic setup described. Moreover, I worry it encourages unhealthy directions for the field. Over 1000 models were trained and evaluated. There is no form of separate held-out comparison: the framework encourages people to keep trying random stuff until the chosen measure reports success.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1329,https://openreview.net/forum?id=ByEPMj5el,Out-of-class Novelty Generation: An Experimental Foundation,"Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.","Out-of-class Novelty Generation: An Experimental Foundation. The authors proposed an way to measure the generation of out-of-distribution novelty. Their methods implied, if a model trained on MNIST digits could generate some samples are more like letters judged by anther model trained both on MNIST and letters,  the model trained on MNIST could be seen as having  the ability to generate novel samples. Some empirical experiments were reported. --------The novelty is hard to define. The proposed metric is also problematic. A naive combination of MNIST and letters dataset do not represent the natural distribution of handwritten digits and letters. IT means that the model trained on the combination could not properly distinguished digits and letters. The proposed out-of-class count and out-of-class max are thus pointless. For the ""novel"" samples in Fig. 3,  they are clearly digits. I guess they quantize the samples to binary. If they would quantize the samples to 8 bit, the resulting images would look even more like digits. ",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper aims to present an experimental framework for selecting machine learning models that can generate novel objects. As the work is devoted to a relatively subjective area of study, it is not surprising that opinions of the work are mixed.    A large section of the paper is devoted to review, and more detail could be given to the experimental framework. It is not clear whether the framework can actually be useful outside the synthetic setup described. Moreover, I worry it encourages unhealthy directions for the field. Over 1000 models were trained and evaluated. There is no form of separate held-out comparison: the framework encourages people to keep trying random stuff until the chosen measure reports success.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1330,https://openreview.net/forum?id=ByEPMj5el,Out-of-class Novelty Generation: An Experimental Foundation,"Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.","Out-of-class Novelty Generation: An Experimental Foundation. This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them.----------------The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An ""in-house"" annotation tool was used but it's unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field.----------------This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric.----------------Moreover, defining English letters as ""novel"" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more ""novel"" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"This paper aims to present an experimental framework for selecting machine learning models that can generate novel objects. As the work is devoted to a relatively subjective area of study, it is not surprising that opinions of the work are mixed.    A large section of the paper is devoted to review, and more detail could be given to the experimental framework. It is not clear whether the framework can actually be useful outside the synthetic setup described. Moreover, I worry it encourages unhealthy directions for the field. Over 1000 models were trained and evaluated. There is no form of separate held-out comparison: the framework encourages people to keep trying random stuff until the chosen measure reports success.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1340,https://openreview.net/forum?id=HJKkY35le,Mode Regularized Generative Adversarial Networks,"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training, thus providing a unified solution to the missing modes problem.","Mode Regularized Generative Adversarial Networks. The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution.  The presented approaches are entirely based on some intuitive arguments. As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point.  The paper will benefit from more rigorous theoretical justification of the presented approaches. ",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper presents some intuitions about why GAN training is difficult, and proposes a somewhat remedy that seems to help.     Pros   - a detailed explanation of the motivation   - side experiments to demonstrate the properties of the new method    Cons   - There are already several closely-related approaches, namely VAEGAN, with similar motivations and procedures. This paper argues that their method is conceptually distinct from the VAEGAN, but I and other reviewers found the arguments underdeveloped unconvincing.   - Most of the evaluations are qualitative.    The search space of reasonable modifications to the GAN objective is so large that either a more principled or systematic (as in, do more comparison experiments) exploration of the space is necessary.   Given the importance of the topic, the PCs still believe this paper should be accepted, but we encourage the authors to further revise their paper to address the remaining outstanding issues.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1341,https://openreview.net/forum?id=HJKkY35le,Mode Regularized Generative Adversarial Networks,"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training, thus providing a unified solution to the missing modes problem.","Mode Regularized Generative Adversarial Networks. Summary:----------------This paper proposes several regularization objective such as ""geometric regularizer"" and ""mode regularizer"" to stabilize the training of GAN models. Specifically, these regularizes are proposed to alleviate the mode-missing behaviors of GANs.----------------Review:----------------I think this is an interesting paper that discusses the mode-missing behavior of GANs and proposes new evaluation metric to evaluate this behavior. However, the core ideas of this paper are not very innovative to me. Specifically, there has been a lot of papers that combine GAN with an autoencoder and the settings of this paper is very similar to the other papers such as Larsen et al. As I pointed out in my pre-review comments, in the Larsen et al. both the geometric regularizer and model regularizer has been proposed in the context of VAEs and the way they are used is essentially the same as this paper. I understand the argument of the authors that the VAEGAN is a VAE that is regularized by GAN and in this paper the main generative model is a GAN that is regularized by an autoencoder, but at the end of the day, both the models are combining the autoencoder and GAN in a pretty much same way, and to me the resulting model is not very different. I also understand the other argument of the authors that Larsen et al is using VAE while this paper is using an autoencoder, but I am still not convinced how this paper outperforms the VAEGAN by just removing the KL term of the VAE. I do like that this paper looks at the autoencoder objective as a way to alleviate the missing mode problem of GANs, but I think that alone does not have enough originality to carry the paper.----------------As pointed out in the public comments by other people, I also suggest that the authors do an extensive comparison of this work and Larsen et al. in terms of missing mode, sample quality and quantitative performances such as inception score.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents some intuitions about why GAN training is difficult, and proposes a somewhat remedy that seems to help.     Pros   - a detailed explanation of the motivation   - side experiments to demonstrate the properties of the new method    Cons   - There are already several closely-related approaches, namely VAEGAN, with similar motivations and procedures. This paper argues that their method is conceptually distinct from the VAEGAN, but I and other reviewers found the arguments underdeveloped unconvincing.   - Most of the evaluations are qualitative.    The search space of reasonable modifications to the GAN objective is so large that either a more principled or systematic (as in, do more comparison experiments) exploration of the space is necessary.   Given the importance of the topic, the PCs still believe this paper should be accepted, but we encourage the authors to further revise their paper to address the remaining outstanding issues.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_1342,https://openreview.net/forum?id=HJKkY35le,Mode Regularized Generative Adversarial Networks,"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training, thus providing a unified solution to the missing modes problem.","Mode Regularized Generative Adversarial Networks. The authors identify two very valid problem of mode-missing in Generative Adversarial Networks, explain their intuitions as to why these problems occur and propose ways to remedy it. The first problem is about the discriminator becoming too good (close to 0 on fake, and 1 on real data) and providing 0 gradients to the generator. The second problem is that GANs are prone to missing modes of the data generating distribution entirely. The authors propose two regularization techniques to address these problems: Geometric Metrics Regularizer and Mode Regularizer----------------Overall, I felt that this is a good paper, providing a good analysis of the problems and proposing sensible solutions - if lacking solid from-first-principles motivation for the particular choices made. My other critique is the focus on manifolds, almost completely disregarding the probability density on the manifold - see my detailed comment below.----------------Detailed comments on the Geometric Metrics Regularizer: The motivation for this is to provide a way to measure and penalize distance between two degenerate probability distributions concentrated on non-overlapping manifolds, those of the generator and of the real data. There are different ways one could go about measuring difference between two manifolds or probability distributions concentrated on manifolds, for example:----------------- projection heuristic: measure the average distance between each point x on manifold A and the corresponding nearest point on manifold B (let’s call it the projection of x onto B).--------- earth mover’s distance: establish a smooth mapping between the two manifolds that maps denser areas on manifold A to nearby denser areas of manifold B, and measure the average distance between corresponding pairs.----------------The two heuristics are similar but while the earth mover distance is a divergence measure for distributions, the projection heuristic only measures the divergence of the manifolds, disregarding the distributions in question.--------The authors propose measuring the average distance between a point on the real data manifold and a point it gets mapped to by the composition of the encoder and the generator. While E○G will map to the generative manifold, it is unclear to me if they would map to a high-probability region on that manifold, so this probably doesn’t implement anything like Earth Mover’s Distance. On this note, I have just remembered seeing this before: https://github.com/danielvarga/earth-moving-generative-net As the encoder is trained so that E○G(x) is close to x on average, it feels like a variant of the projection heuristic above. Would the authors agree with this assessment?","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents some intuitions about why GAN training is difficult, and proposes a somewhat remedy that seems to help.     Pros   - a detailed explanation of the motivation   - side experiments to demonstrate the properties of the new method    Cons   - There are already several closely-related approaches, namely VAEGAN, with similar motivations and procedures. This paper argues that their method is conceptually distinct from the VAEGAN, but I and other reviewers found the arguments underdeveloped unconvincing.   - Most of the evaluations are qualitative.    The search space of reasonable modifications to the GAN objective is so large that either a more principled or systematic (as in, do more comparison experiments) exploration of the space is necessary.   Given the importance of the topic, the PCs still believe this paper should be accepted, but we encourage the authors to further revise their paper to address the remaining outstanding issues.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1343,https://openreview.net/forum?id=HJKkY35le,Mode Regularized Generative Adversarial Networks,"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training, thus providing a unified solution to the missing modes problem.","Mode Regularized Generative Adversarial Networks. This paper does a good job of clearly articulating a problem in contemporary training of GANs, coming up with an intuitive solution via regularizers in addition to optimizing only the discriminator score, and conducting clever experiments to show that the regularizers have the intended effect. ----------------There are recent related and improved GAN variants (ALI, VAEGAN, potentially others), which are included in qualitative comparisons, but not quantitative. It would be interesting to see whether these other types of modified GANs already make some progress in addressing the missing modes problem. If code is available for those methods, the paper could be strengthened a lot by running the mode-missing benchmarks on them (even if it turns out that a ""competing"" method can get a better result in some cases).----------------The experiments on digits and faces are good for validating the proposed regularizers. However, if the authors can show better results on CIFAR-10, ImageNet, MS-COCO or some other more diverse and challenging dataset, I would be more convinced of the value of the proposed method. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper presents some intuitions about why GAN training is difficult, and proposes a somewhat remedy that seems to help.     Pros   - a detailed explanation of the motivation   - side experiments to demonstrate the properties of the new method    Cons   - There are already several closely-related approaches, namely VAEGAN, with similar motivations and procedures. This paper argues that their method is conceptually distinct from the VAEGAN, but I and other reviewers found the arguments underdeveloped unconvincing.   - Most of the evaluations are qualitative.    The search space of reasonable modifications to the GAN objective is so large that either a more principled or systematic (as in, do more comparison experiments) exploration of the space is necessary.   Given the importance of the topic, the PCs still believe this paper should be accepted, but we encourage the authors to further revise their paper to address the remaining outstanding issues.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1347,https://openreview.net/forum?id=ryCcJaqgl,Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series,"Local trends of time series characterize the intermediate upward and downward patterns of time series. Learning and forecasting the local trend in time series data play an important role in many real applications, ranging from investing in the stock market, resource allocation in data centers and load schedule in smart grid. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-to-end hybrid neural network that predicts the local trend of time series based on local and global contextual features. TreNet leverages convolutional neural networks (CNNs) to extract salient features from local raw data of time series. Meanwhile, considering long-range dependencies existing in the sequence of historical local trends, TreNet uses a long-short term memory recurrent neural network (LSTM) to capture such dependency. Furthermore, for predicting the local trend, a feature fusion layer is designed in TreNet to learn joint representation from the features captured by CNN and LSTM. Our proposed TreNet demonstrates its effectiveness by outperforming conventional CNN, LSTM, HMM method and various kernel based baselines on real datasets.","Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series. Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model's architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I'm going to bump up my score from a clear rejection to a borderline.-------------------------------------This paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher ""feature fusion"" (i.e., dense) layer to combine the LSTM's and ConvNet's outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN).----------------Strengths:--------- A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning.--------- Accepting the author's assumptions as true for the moment, the proposed architecture seems intuitive and well-designed.----------------Weaknesses:--------- Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from ""on high"" (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends.--------- The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary.--------- Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments.--------- The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive.--------- Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak.----------------One thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently.----------------Regarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above ""separate trends from noise"" problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013.----------------I appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML).",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.    As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1348,https://openreview.net/forum?id=ryCcJaqgl,Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series,"Local trends of time series characterize the intermediate upward and downward patterns of time series. Learning and forecasting the local trend in time series data play an important role in many real applications, ranging from investing in the stock market, resource allocation in data centers and load schedule in smart grid. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-to-end hybrid neural network that predicts the local trend of time series based on local and global contextual features. TreNet leverages convolutional neural networks (CNNs) to extract salient features from local raw data of time series. Meanwhile, considering long-range dependencies existing in the sequence of historical local trends, TreNet uses a long-short term memory recurrent neural network (LSTM) to capture such dependency. Furthermore, for predicting the local trend, a feature fusion layer is designed in TreNet to learn joint representation from the features captured by CNN and LSTM. Our proposed TreNet demonstrates its effectiveness by outperforming conventional CNN, LSTM, HMM method and various kernel based baselines on real datasets.","Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series. 1) Summary----------------This paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.-------- --------2) Contributions----------------+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.--------+ Comparison to deep and shallow baselines.----------------3) Suggestions for improvement----------------Add a LRCN baseline and discussion:--------The benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular ""long-term recurrent convolutional network"" (LRCN) of Donahue et al (https://arxiv.org/abs/1411.4389). This approach stacks a LSTM on top of CNN features and is typically used on time series of video frames for tasks that are more general than local linear trend prediction. Furthermore, LRCN does not require the hand-crafted preprocessing of time series to extract piecewise linear approximations needed by the LSTM of the TreNet architecture proposed here. Finally, LRCN might be more parameter efficient, as it does not have the fully connected fusion layers of TreNet (eq. 8). ----------------Add more complex multivariate datasets:--------The currently used 3 datasets are limited, especially compared to modern research in representation learning for time series forecasting. For instance, and of particular interest to ICLR, I would suggest investigating future frame prediction on natural video datasets like UCF101 where CNN+LSTM are typically used albeit with a more complex loss (cf. for instance the popular adversarial one of Mathieu et al). Although different from the task of local linear trend prediction, it would be interesting to see how TreNet could be applied to the encoder stage of existing encoder-decoder architectures for frame prediction. It seems that decoupling short term and long term motion representation learning (for instance) could be beneficial in natural videos, as they often contain fast object motions together with slower camera ones.----------------Clarification about the target variables:--------The authors need to clarify whether they handle separately or jointly the duration and slope. The text is ambiguous and seems to suggest training two separate models, one for slope, one for duration, which is particularly puzzling considering that predicting them jointly is in fact much easier (just two output variables instead of one), makes more sense, and is entirely feasible with the current method.----------------Other parts of the text can be improved too. For instance, the authors can vastly compress the generic description of standard convnet and LSTM equations in section 4, while the preprocessing of the time series needs to appear much earlier.----------------4) Conclusion----------------Although the architecture seems promising, the current experiments are too preliminary to validate its usefulness, in particular to existing alternatives like LRCN, which are not compared to.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.    As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1349,https://openreview.net/forum?id=ryCcJaqgl,Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series,"Local trends of time series characterize the intermediate upward and downward patterns of time series. Learning and forecasting the local trend in time series data play an important role in many real applications, ranging from investing in the stock market, resource allocation in data centers and load schedule in smart grid. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-to-end hybrid neural network that predicts the local trend of time series based on local and global contextual features. TreNet leverages convolutional neural networks (CNNs) to extract salient features from local raw data of time series. Meanwhile, considering long-range dependencies existing in the sequence of historical local trends, TreNet uses a long-short term memory recurrent neural network (LSTM) to capture such dependency. Furthermore, for predicting the local trend, a feature fusion layer is designed in TreNet to learn joint representation from the features captured by CNN and LSTM. Our proposed TreNet demonstrates its effectiveness by outperforming conventional CNN, LSTM, HMM method and various kernel based baselines on real datasets.","Trenet: Hybrid Neural Networks For Learning The Local Trend In Time Series. Revision of the review:--------The authors did a commendable job of including additional references and baseline experiments.-----------------------------------This paper presents a hybrid architecture for time series prediction, focusing on the slope and duration of linear trends. The architecture consists of combining a 1D convnet for local time series and an LSTM for time series of trend descriptors. The convnet and LSTM features are combined into an MLP for predicting either the slope or the duration of the next trend in a 1D time series. The method is evaluated on 3 small datasets.----------------Summary:--------This paper, while relative well written and presenting an interesting approach, has several methodology flaws, that should be handled by new experiments.----------------Pros:--------The idea of extracting upward or downward trends from time series - although these should, ideally be learned, not rely on an ad-hoc technique, given that this is a submission for ICLR.----------------Methodology:--------* In section 3, what do you mean by predicting “either [the duration] -------- or [slope] --------” of the trend? Predictions are valid only if those two predictions are done jointly. The two losses should be combined during training.--------* In the entire paper, the trend slope and duration need to be predicted jointly. Predicting a time series without specifying the horizon of the prediction is meaningless. If the duration of the trends is short, the time series could go up or down alternatively; if the duration of the trend is long, the slope might be close to zero. Predictions at specific horizons are needed.--------* In general, time series prediction for such applications as trading and load forecasting is pointless if no decision is made. A trading strategy would be radically different for short-term and noisy oscillations or from long-term, stable upward or downward trend. An actual evaluation in terms of trading profit/loss should be added for each of the baselines, including the naïve baselines.--------* As mentioned earlier in the pre-review questions, an important baseline is missing: feeding the local time series to the convnet and connecting the convnet directly to the LSTM, without ad-hoc trend extraction.--------* The convnet -> LSTM architecture would need an analysis of the convnet filters and trend prediction representation.--------* Trend prediction/segmentation by the convnet could be an extra supervised loss.--------* The detailed analysis of the trend extraction technique is missing.--------* In section 5, the SVM baselines have local trend and local time series vectors concatenated. Why isn’t the same approach used for LSTM baselines (as a multivariate input) and why the convnet operates only on local --------* An important, “naïve” baseline is missing: next local trend slope and duration = previous local trend slope and duration.----------------Missing references:--------The related work section is very partial and omits important work in hybrid convnet + LSTM architectures, in particular:--------Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2014.--------Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014.--------Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video classification with convolutional neural networks. In CVPR, 2014.----------------The organization of the paper needs improvement:--------* Section 3 does not explain the selection of the maximal tolerable variance in each trend segment. The appendix should be moved to the core part of the paper.--------* Section 4 is unnecessarily long and gives well known details and equations about convnets and LSTMs. The only variation from standard algorithm descriptions is that   are concatenated. The feature fusion layer can be expressed by a simple MLP on the concatenation of R(T(l)) and C(L(t)). Details could be moved to the appendix.----------------Additional questions:--------*In section 5, how many datapoints are there in each dataset? Listing only the number of local trends is uninformative.----------------Typos:--------* p. 5, top “duration and slop”",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.    As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1356,https://openreview.net/forum?id=HJDdiT9gl,Generating Long And Diverse Responses With Neural Conversation Models,"Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.","Generating Long And Diverse Responses With Neural Conversation Models. The paper proposes modification to seq2seq model to help it handle the problems when long responses are needed. --------Though the technical contributions may be of value, the work in my personal opinion is not in the right direction towards helping dialog systems. Essentially we try to generate long responses that sound ``nice"" yet are not grounded to any reality, they just need to be related to the question and not suffers from obvious mistakes. Yet, the architectural innovations proposed may be of merit.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The reviewers agree that the work presents interesting, but incremental, results. They are also unconvinced that this is the right direction for research on dialogue systems to go in or that the methods presented will appeal to a broader audience. It seems the authors have some work to do on the larger argument, before this work can be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1357,https://openreview.net/forum?id=HJDdiT9gl,Generating Long And Diverse Responses With Neural Conversation Models,"Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.","Generating Long And Diverse Responses With Neural Conversation Models. The paper is clearly interesting in that it does address important problems (length and diversity of responses) in sequence-to-sequence models. The two ideas put forward (glimpse model and segment-based stochastic decoding) both seem ideas in the right direction. I was however not so sold on the argument that these are particularly suitable for conversations.----------------The results indicate that the ideas do indeed generate longer and also somewhat more sensible target sequences and as such the paper makes progress w.r.t these important problems. So overall I would suggest accepting the paper even though the flavor of the proposed ideas are somewhat ""small steps"". ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The reviewers agree that the work presents interesting, but incremental, results. They are also unconvinced that this is the right direction for research on dialogue systems to go in or that the methods presented will appeal to a broader audience. It seems the authors have some work to do on the larger argument, before this work can be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1358,https://openreview.net/forum?id=HJDdiT9gl,Generating Long And Diverse Responses With Neural Conversation Models,"Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.","Generating Long And Diverse Responses With Neural Conversation Models. This paper considers the problem of generating long and diverse responses in dialog systems. Two techniques are proposed to the seq-to-seq framework: (1) glimpse model that trains on fixed-length segments of the target side at a time, and (2) a segment-based stochastic decoding technique which injects diversity earlier in the generated responses. The large scale experiments on 2.3B conversation messages are quite impressive.  Experiments on human evaluation should also be encouraged. ----------------With all these said, I am still not 100% convinced that machine generated long sequence is the right direction for dialog systems. As shown in Figure 3 (a), human evaluation shows that the proposed system is not significantly better than the baselines. I think more analysis and user preference mining should be done in the future to help us understand the nature of this problem. ","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The reviewers agree that the work presents interesting, but incremental, results. They are also unconvinced that this is the right direction for research on dialogue systems to go in or that the methods presented will appeal to a broader audience. It seems the authors have some work to do on the larger argument, before this work can be accepted.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1363,https://openreview.net/forum?id=SkYbF1slg,An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax,"A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.","An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax. This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.----------------While the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.----------------A few questions that the authors may want to clarify:--------1. Page 4, last paragraph: ""from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)"". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?--------2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?--------3. At the end of page 9: ""we will discuss how to get optimal solution of C for two specific cases"". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.","8: Top 50% of accepted papers, clear accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Accept (Poster),1.0
2017_1364,https://openreview.net/forum?id=SkYbF1slg,An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax,"A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.","An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax. This paper proposes a hierarchical infomax method. My comments are as follows: ----------------(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message. ----------------(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\breve{Y}) is an “upper” bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable “lower” bound of it).----------------Minor comments:--------(1) If (2.11) is approximation of (2.8), “\approx” should be used. ----------------(2) Why K_1 instead of N in Eq.(2.11)?----------------(3) In Eq.(2.12), H(X) should disappear?----------------(4) Can you divide Section 3 into subsections?",5: Marginally below acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Accept (Poster),0.0
2017_1365,https://openreview.net/forum?id=SkYbF1slg,An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax,"A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.","An Information-theoretic Framework For Fast And Robust Unsupervised Learning Via Neural Population Infomax. This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.----------------This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.----------------Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper",Accept (Poster),1.0
2017_1372,https://openreview.net/forum?id=SJJKxrsgl,Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes,"We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.","Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes. This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina.----------------The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.----------------The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy.----------------Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective.----------------Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim.----------------Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This was a borderline case. All reviewers and the AC appeared to find the paper interesting, while having some reservations. Given the originality of the work, the PCs decided to lean toward acceptance. We do encourage however the authors to revise their paper based on reviewer feedback as much as possible, to increase its potential for impact.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1373,https://openreview.net/forum?id=SJJKxrsgl,Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes,"We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.","Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes. The paper presented an extension to the current visual attention model that learns a deformable sampling lattice.  Comparing to the fixed sampling lattice from previous works, the proposed method shows different sampling strategy can emerge depending on the visual classification tasks. The authors empirically demonstrated the learnt sampling lattice outperforms the fixed strategies. More interestingly, when the attention mechanism is constrained  to be translation only, the proposed model learns a sampling lattice resembles the retina found in the primate retina.  ------------------------Pros:--------+ The paper is generally well organized and written --------+ The qualitative analysis in the experimental section is very comprehensive.----------------Cons:---------  The paper could benefit substantially from additional experiments on different datasets.---------  It is not clear from the tables the proposed learnt sampling  lattice offer any computation benefit when comparing to  a fixed sampling strategy with zooming capability, e.g. the one used in DRAW model.----------------Overall, I really like the paper. I think the experimental section can be improved by additional experiments and more quantitative analysis with other baselines. Because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g.  linear relationship--------between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This was a borderline case. All reviewers and the AC appeared to find the paper interesting, while having some reservations. Given the originality of the work, the PCs decided to lean toward acceptance. We do encourage however the authors to revise their paper based on reviewer feedback as much as possible, to increase its potential for impact.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1374,https://openreview.net/forum?id=SJJKxrsgl,Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes,"We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.","Emergence Of Foveal Image Sampling From Learning To Attend In Visual Scenes. This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. ----------------The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.----------------Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice.",6: Marginally above acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This was a borderline case. All reviewers and the AC appeared to find the paper interesting, while having some reservations. Given the originality of the work, the PCs decided to lean toward acceptance. We do encourage however the authors to revise their paper based on reviewer feedback as much as possible, to increase its potential for impact.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1375,https://openreview.net/forum?id=SyW2QSige,Towards Information-seeking Agents,"We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.","Towards Information-seeking Agents. Pros:----------------* The general idea behind the paper seems pretty novel and potentially quite cool.--------* The specific technical implementation seems pretty reasonable and well-thought through.--------* The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities. --------* The writing is pretty clear.  I basically felt like I could replicate much of what they did from their paper descriptions. ------------------------Cons:----------------* The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. ----------------* The specific tasks that they try are quite simple.   I really don't know whether their approach is better than a bunch of simpler things on these tasks.   ----------------Taking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper.  (I know how it feels to be under a deadline without a complete set of results.)   If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine.  Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great.  But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it's really hard to tell how much of an advance is made here.     But it does seem like a potentially fruitful research direction.   ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes information gain as an intermediate reward signal to train deep networks to answer questions. The motivation and model are interesting, however the experiments fail to deliver. There is a lack of comparative simple baselines, the performance of the model is not sufficiently analyzed, and the actual tasks proposed are too simple to promise that the results would easily generalize to more useful tasks. This paper has a lot of good directions but definitely requires more work. I encourage the authors to follow the advice of reviewers and explore the various directions proposed so that this work can live up to its potential.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1376,https://openreview.net/forum?id=SyW2QSige,Towards Information-seeking Agents,"We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.","Towards Information-seeking Agents. This paper proposes a setting to learn models that will seek information (e.g., by asking question) in order to solve a given task. They introduce a set of tasks that were designed for that goal. They show that it is possible to train models to solve these tasks with reinforcement learning.----------------One key motivation for the tasks proposed in this work are the existence of games like 20Q or battleships where an agent needs to ask questions to solve a given task. It is quite surprising that the authors do not actually consider these games as potential tasks to explore (beside the Hangman). It is also not completely clear how the tasks have been selected. A significant amount of work has been dedicated in the past to understand the property of games like 20Q (e.g., Navarro et al., 2010) and how humans solve them.  It would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature, and how humans would perform on them.  In particular, Cohen & Lake, 2016m have recently studied the 20 questions games in their paper “Searching large hypothesis spaces by asking questions” where they both evaluate the performance of humans and computer. I believe that this paper would really benefits from a similar study.----------------Developing the ability of models to actively seek for information to solve a task is a very interesting but challenging problem. In this paper, all of the  tasks require the agent to select a questions from a finite set of clean and informative possibilities. This allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings.----------------This paper also show that by using a relatively standard mix of deep learning models and reinforcement learning, they are able to train agents that can solve these tasks in the way it was intended to. This validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toy-ish settings with perfect information and a fixed number of questions may be too simple. ----------------While it is interesting to see that their agent are able to perform well on all of their tasks, the absence of baselines limit the conclusions we can draw from these experiments. For example in the Hangman experiment, it seems that the frequency based model obtains promising performance. It would interesting to see how good are baselines that may use the co-occurrence of letters or the frequency of character n-grams.------------------------Overall, this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question. However, the current analysis of the tasks is a bit limited, and it is hard to draw any conclusion from them. It would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes information gain as an intermediate reward signal to train deep networks to answer questions. The motivation and model are interesting, however the experiments fail to deliver. There is a lack of comparative simple baselines, the performance of the model is not sufficiently analyzed, and the actual tasks proposed are too simple to promise that the results would easily generalize to more useful tasks. This paper has a lot of good directions but definitely requires more work. I encourage the authors to follow the advice of reviewers and explore the various directions proposed so that this work can live up to its potential.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1377,https://openreview.net/forum?id=SyW2QSige,Towards Information-seeking Agents,"We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.","Towards Information-seeking Agents. This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.----------------Both GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.----------------The experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes information gain as an intermediate reward signal to train deep networks to answer questions. The motivation and model are interesting, however the experiments fail to deliver. There is a lack of comparative simple baselines, the performance of the model is not sufficiently analyzed, and the actual tasks proposed are too simple to promise that the results would easily generalize to more useful tasks. This paper has a lot of good directions but definitely requires more work. I encourage the authors to follow the advice of reviewers and explore the various directions proposed so that this work can live up to its potential.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1381,https://openreview.net/forum?id=SJk01vogl,Adversarial Examples For Generative Models,"We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.","Adversarial Examples For Generative Models. Comments: ----------------""This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,--------which often have telltale noise""----------------Is this really true?  If it were the case, wouldn't it imply that training ""against"" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  ----------------Pros: --------  -The question of whether adversarial examples exist in generative models, and indeed how the definition of ""adversarial example"" carries over is an interesting one.  --------  -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  --------  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  ----------------Issues: --------  -Paper is significantly over length at 13 pages.  --------  -The beginning of the paper should more clearly motivate its purpose.  --------  -Paper has ""generative models"" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called ""adversarial examples for generative models"".  --------   -I think that the introduction contains too much background information - it could be tightened.  ",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1382,https://openreview.net/forum?id=SJk01vogl,Adversarial Examples For Generative Models,"We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.","Adversarial Examples For Generative Models. This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the ""latent attack"" which finds adversarial perturbation in the input so as to match the latent representation of a target input.----------------I think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of ""Adversarial Images for Variational Autoencoders"" that essentially proposes the same ""latent attack"" idea of this paper with both L2 distance and KL divergence.----------------Novelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*. However I still find it interesting to see how these standard methods compare in this new problem domain.----------------The clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the ""classification-based adversaries"" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of ""latent attack"" is proposed which works much better than the ""classification-based adversaries"". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. ""in our attempts to be thorough, we have had a hard time keeping the length down"" is not a valid excuse.----------------In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1383,https://openreview.net/forum?id=SJk01vogl,Adversarial Examples For Generative Models,"We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.","Adversarial Examples For Generative Models. After the rebuttal:----------------The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. ----------------For me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing ""generative models"" by ""encoder-decoder networks"" in the title. Then I would tend towards recommending acceptance.------------------------------Initial review:----------------The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.----------------Detailed comments:----------------1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. ""in our attempts to be thorough, we have had a hard time keeping the length down"" is a bad excuse - it may be hard, but has to be done.----------------2) I intentionally avoided term ""generative model"" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.----------------3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. ----------------4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. ----------------Smaller remarks:--------1) Usage of ""Oracle"" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.--------2) Beginning of section 4: ""All three methods work for any generative architecture that relies on a learned latent representation z"" - ""are technically applicable to"" would be more correct than ""work for"". --------3) 4.1: ""confidentally""",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1384,https://openreview.net/forum?id=HyY4Owjll,Boosted Generative Models,"We propose a new approach for using boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our algorithm can leverage  many existing base learners, including recent latent variable models. Further, our approach allows the ensemble to leverage discriminative models trained to distinguish real data from model generated data. We show theoretical conditions under which incorporating a new model to the ensemble will improve the fit and empirically demonstrate the effectiveness of boosting on density estimation and sample generation on real and synthetic datasets.","Boosted Generative Models. This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.----------------The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC.----------------The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.----------------With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.----------------Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.----------------[R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf----------------PROS:--------Novel and intriguing idea--------Strong theoretical guarantees----------------CONS:--------Resulting boosted model is un-normalized--------Discriminator based boosting is expensive, due to sampling via MCMC--------Weak experimental section",6: Marginally above acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1385,https://openreview.net/forum?id=HyY4Owjll,Boosted Generative Models,"We propose a new approach for using boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our algorithm can leverage  many existing base learners, including recent latent variable models. Further, our approach allows the ensemble to leverage discriminative models trained to distinguish real data from model generated data. We show theoretical conditions under which incorporating a new model to the ensemble will improve the fit and empirically demonstrate the effectiveness of boosting on density estimation and sample generation on real and synthetic datasets.","Boosted Generative Models. The authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting. The approach is simple and elegant and basically creates an unnormalized product of experts model, where the individual experts are trained greedily to optimize the overall joint model. Unfortunately, this approach results in a joint model that has some undesirable properties: a unknown normalisation constant for the joint model and therefore an intractable log-likelihood on the test set; and it makes drawing exact samples from the joint model intractable. These problems can unfortunately not be fixed by using different base learners, but are a direct result of the product of experts formulation of boosting.--------  --------The experiments on 2 dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e.g. bagging. But the experiments on MNIST are less convincing: Without an undisputable measure like e.g. log-likelihood it is hard to draw conclusions from the samples in Figure 2; and visually they look weak compared to even simple models like e.g. NADE.----------------I think the paper could be improved significantly by adding a quantitative analysis: investigating the effect of combining undirected (e.g. RBM), undirected (e.g. VAE) and autoregressive (e.g. NADE) models and by measuring the improvement over the number of base learners. But this would require a method to estimate the partition function Z or estimating some proxy.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1386,https://openreview.net/forum?id=HyY4Owjll,Boosted Generative Models,"We propose a new approach for using boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our algorithm can leverage  many existing base learners, including recent latent variable models. Further, our approach allows the ensemble to leverage discriminative models trained to distinguish real data from model generated data. We show theoretical conditions under which incorporating a new model to the ensemble will improve the fit and empirically demonstrate the effectiveness of boosting on density estimation and sample generation on real and synthetic datasets.","Boosted Generative Models. The paper proposes two approaches to boosting generative models, both based on likelihood ratio estimates. The approaches are evaluated on synthetic data, as well as on MNIST dataset for the tasks of generating samples and semi-supervised learning.--------While the idea of boosting generative models and the proposed methods are interesting, the reviewer finds the experiments unconvincing for the following reasons.--------1. The bagging baseline in section 3.1 seems to be just refitting a model to the same dataset, raising the probability to power alpha, and renormalizing. This makes it more peaked, but it's not clear why this is a good baseline. Please let me know if I misunderstood the procedure.--------2. The sample generation experiment in section 3.2 uses a very slowly converging Markov chain, as can be seen in the similarity of plots c and f, d and g, e and h. It seems unlikely therefore that the resulting samples are from the stationary distribution. A qualitative evaluation using AIS seems to be necessary here.--------3. In the same section the choices for alphas seem quite arbitrary - what happens when a more obvious choice of alpha_i=1 for all i is made?--------4. It seems hard to infer anything from the semisupervised classification results reported: the baseline RBM seems to perform as well as the boosted models.----------------The work is mostly clearly written and (as far as the reviewer knows) original.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1402,https://openreview.net/forum?id=B1gtu5ilg,Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects,"We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.","Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects. This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the ""Tenenbaum objects” is shown.----------------Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.----------------Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.----------------More details:----------------The “image purification” paper is very related to this work:----------------[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.----------------There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/).----------------Somewhat related to the proposed method is recent work on multi-view 3D object retrieval:----------------[B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016.----------------There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object).  Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view.----------------Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape):----------------[C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016.----------------[D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016.----------------For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean).  It has been shown that different layers and feature comparisons perform differently for a given task, e.g.,----------------[E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016.----------------[F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_1403,https://openreview.net/forum?id=B1gtu5ilg,Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects,"We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.","Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects. I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. ----------------I think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- ""we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]"". I am skeptical of any claimed connections bigger than that.----------------I think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think ""Remarkably"" is justified in the statement ""Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet""----------------I'm not an expert on human vision, but from browsing online and from what I've learned before it seems that ""object persistence"" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1404,https://openreview.net/forum?id=B1gtu5ilg,Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects,"We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.","Transfer Of View-manifold Learning To Similarity Perception Of Novel Objects. On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl .----------------In summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1405,https://openreview.net/forum?id=ry7O1ssex,Generative Adversarial Networks As Variational Training Of Energy Based Models,"In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density  is approximated by a variational distribution  that is easy to sample from. The training of VGAN takes a two step procedure: given ,  is updated to maximize the lower bound;  is then updated one step with samples drawn from  to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where  corresponds to the discriminator and  corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.","Generative Adversarial Networks As Variational Training Of Energy Based Models. Our understanding of GAN to date is still vague. Although there have been some efforts relating GAN to energy models, I personally consider that the perspective of this paper, namelying understanding GAN (a variant of GAN, to be more precise) as variational training of an energy model is the most natural and elegant.  The derivation up to equation (5) and the reduction to (7) are very nice.  I think this is the most important contribution of the paper. ----------------The techniques introduced in sections 5 and 6 are somewhat ad hoc, and lack clarity. Referring to the version I looked at (not sure though if it is the latest), section 6 contain some errors/typos (stuff around p_z(x|\tilde{x}). The presentation of section 6 needs to improve in clarity.  But I think this does not shadow the main contribution of the paper, namely, that perspectives given in sections 2-4.  ----------------Overall I very much enjoy the presented insight of this paper into GAN.----------------I do have some comment/question regarding equation (7). This equation formulates a variant of GAN, or a model resembling GAN. I am happy to see that the entropy term pops up there, which should save GAN from degenerating its generative distribution or from missing modes.  The swapping of the min-max order in this formulation however makes me wonder if this variant of GAN indeed reflects the ""principle"" of GAN, or it is in fact a different principle, which happens to gives rise to a model  that ""resembles"" GAN.  Of course, my question may be merely philosophical rather than mathematical, and I won't expect a precise anwer. Nonetheless, if the author can provide additional insignts on this, it would be appreciated.","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,0.0
2017_1406,https://openreview.net/forum?id=ry7O1ssex,Generative Adversarial Networks As Variational Training Of Energy Based Models,"In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density  is approximated by a variational distribution  that is easy to sample from. The training of VGAN takes a two step procedure: given ,  is updated to maximize the lower bound;  is then updated one step with samples drawn from  to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where  corresponds to the discriminator and  corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.","Generative Adversarial Networks As Variational Training Of Energy Based Models. The paper drives a variant of GAN's min max game optimizations from EBM's NLL minimization and propose a new generative model from this derivation.----------------In introduction, could you elaborate why accurate distance metric is needed for unsupervised learning.----------------It's a bit hard to read the paper as it jumps from points to points without clear connection and laying down the background.  The introduction, refers to many different concepts without any clear connection. And other sections as well is very incomprehensible even if one is familiar with the concepts.----------------There has been lots of interests in similar area recently, Authors do cite some of them in introduction and related work but the relationship and comparison are missing.----------------In the results, if it's comparison of the quality of samples it has to include other recent works and compare those as well. And if it's semi-supervised learning, again, the numbers should be compared to other works.----------------In summary, unfortunately the paper is very clear and cumbersome to read which makes it hard to judge it fairly. I strongly suggest re-write of the paper in more coherent matter to make it easier to read. And also extend the experiments with more comparisons with other works.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1407,https://openreview.net/forum?id=ry7O1ssex,Generative Adversarial Networks As Variational Training Of Energy Based Models,"In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density  is approximated by a variational distribution  that is easy to sample from. The training of VGAN takes a two step procedure: given ,  is updated to maximize the lower bound;  is then updated one step with samples drawn from  to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where  corresponds to the discriminator and  corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.","Generative Adversarial Networks As Variational Training Of Energy Based Models. This paper presents an adversarial training formulation for energy models. Although the relationship between energy-based models and GANs is abundantly clear in the literature, the contributions of this paper seems to be a multimodal energy estimate, and training a transition operator instead of a sampler. The core technical contribution of the paper is not clear. The only two quantitative results in the paper demonstrate that the proposed method learns better features than a traditional GAN, and that the proposed method does a better job in SSL than the authors' chosen baseline. In either case, no comparison to existing literature is made, even though GANs have been used for SSL previously (https://arxiv.org/abs/1606.03498). The qualitative results (visual comparison of quality of samples) is inconclusive.----------------The lack of comparison to existing literature makes this paper a clear reject. To improve this paper, the authors need to make the core contribution of the paper much better, and situate the paper very clearly wrt existing literature. The motivation for the development of the model should be clearer. Further, experiments need to compare with existing literature.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1408,https://openreview.net/forum?id=ry7O1ssex,Generative Adversarial Networks As Variational Training Of Energy Based Models,"In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density  is approximated by a variational distribution  that is easy to sample from. The training of VGAN takes a two step procedure: given ,  is updated to maximize the lower bound;  is then updated one step with samples drawn from  to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where  corresponds to the discriminator and  corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions.","Generative Adversarial Networks As Variational Training Of Energy Based Models. This paper presents a bridging of energy-based models and GANs, where -- starting from the energy-based formalism -- they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work). The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization. It is also discussed in another ICLR submission (Dai et al. ""Calibrating Energy-based Generative Adversarial Networks""). There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model). The specific motivation for this second contribution is not particularly clear. ----------------The two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term. However, unfortunately the authors present no results directly supporting either contribution. For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model. Especially in the light of the impressive samples from the EBGAN model that has no corresponding term. The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions. But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate. ----------------The results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets. They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN. However these--------quantitative results are not particularly compelling as they show limited improvement over baselines. Also, there is no reference to the many--------existing semi-supervised results on these datasets. ----------------Summary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either--------contribution.",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Reject,1.0
2017_1409,https://openreview.net/forum?id=ByZvfijeg,Higher Order Recurrent Neural Networks,"In this paper, we study novel neural network structures to better model long term dependency in sequential data.  We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. ","Higher Order Recurrent Neural Networks. The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks.----------------Some points.----------------1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight.----------------2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.----------------3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.----------------4)  It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this.------------------------I like the basic idea of the paper, but the points above make me think it is not ready for publication.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.  However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1410,https://openreview.net/forum?id=ByZvfijeg,Higher Order Recurrent Neural Networks,"In this paper, we study novel neural network structures to better model long term dependency in sequential data.  We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. ","Higher Order Recurrent Neural Networks. I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. --------I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.  However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1411,https://openreview.net/forum?id=ByZvfijeg,Higher Order Recurrent Neural Networks,"In this paper, we study novel neural network structures to better model long term dependency in sequential data.  We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. ","Higher Order Recurrent Neural Networks. This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.------------------------The reviewer can see few issues with this paper.----------------Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.------------------------Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.------------------------Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.------------------------[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16",3: Clear rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow.  However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1412,https://openreview.net/forum?id=S1JG13oee,B-gan: Unified Framework Of Generative Adversarial Networks,"Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ","B-gan: Unified Framework Of Generative Adversarial Networks. In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching. The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective. In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x).----------------My main problem with this paper is that it is unclear why any of this is useful. The connection to density estimation is interesting, but any derived conclusions between the two seem questionable. For example, in previous density estimation literature, the Pearson divergence is more stable. The authors claim that the same holds for GANS and try to show this in their experiments. Unfortunately, the experiments section is very confusing with unilluminating figures. Looking at the graph of density ratios is not particularly illuminating. They claim that for the Pearson divergence and modified KL-divergence, ""the learning did not stop"" by looking at the graph of density ratios. This is completely hand-wavey and no further evidence is given to back this claim. Also, why was the normal GAN objective not tried in light of this analysis? Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work.----------------I think this paper would be much improved if it was rewritten in a clear fashion. As it stands, it is difficult to understand the motivation or intuition behind this work.",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The paper may have an interesting contribution, but at present its motivation, and the presentation in general, are not clear enough. After a re-write, the paper might become quite interesting and should be submitted to some other forum.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1413,https://openreview.net/forum?id=S1JG13oee,B-gan: Unified Framework Of Generative Adversarial Networks,"Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ","B-gan: Unified Framework Of Generative Adversarial Networks. This paper proposes b-GAN, which trains a discriminator by estimating density ratio that minimizes Bregman divergence. The authors also discuss how b-GANs relate to f-GAN and the original GAN work, providing a unifying view through the lens of density ratio estimation. ----------------Note that the unifying view applies only to GAN variants which optimize density ratios. In general, GANs which use MMD in the discriminator step do not fit in the b-GAN framework except for special choices of the kernel. ----------------I was a bit confused about the dual relationship between f-GAN and b-GAN. Are the conditions on the function f the same in both cases? If so, what's the difference between f-GAN and b-GAN (other than the fact that the former has been derived using f-divergence and the latter has been derived using Bregman divergence)?----------------One of the original claims was that b-GANs optimize f-divergence directly as opposed to f-GAN and GAN. However, in practice, the authors optimize an approximation to the f-divergence; the quality of the approximation is not quantified anywhere, so b-GAN doesn't seem more principled than f-GAN and GAN.----------------The experiments left me a bit confused and were not very illuminating on the choice of f. ----------------Overall, I liked the connections to the density ratio estimation literature. The appendix seems like a scattered collection right now. Some re-writing of the text would significantly improve this paper. ",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper may have an interesting contribution, but at present its motivation, and the presentation in general, are not clear enough. After a re-write, the paper might become quite interesting and should be submitted to some other forum.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1414,https://openreview.net/forum?id=S1JG13oee,B-gan: Unified Framework Of Generative Adversarial Networks,"Generative adversarial networks (GANs) are successful deep generative models. They are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats density ratio estimation and f-divergence minimization. Our algorithm offers a new unified perspective toward understanding GANs and is able to make use of multiple viewpoints obtained from the density ratio estimation research, e.g. what divergence is stable and relative density ratio is useful. ","B-gan: Unified Framework Of Generative Adversarial Networks. This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences. Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients. I have three concerns with this submission.----------------First, the exposition of the paper must be significantly improved. The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.----------------Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN. This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN. Further work is necessary to rule out (in a principled manner!) many of the proposed variants of the algorithm.----------------Third, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4. The authors claim that these figures show that ""learning does not stop"", but such behavior can also be attributed to the typical chaotic dynamics of GANs. Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).----------------Overall, I believe this submission calls for significant improvements before being considered for publication.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The paper may have an interesting contribution, but at present its motivation, and the presentation in general, are not clear enough. After a re-write, the paper might become quite interesting and should be submitted to some other forum.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1432,https://openreview.net/forum?id=HkuVu3ige,On Orthogonality And Learning Recurrent Networks With Long Term Dependencies,"It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.","On Orthogonality And Learning Recurrent Networks With Long Term Dependencies. This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use.----------------The experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear.----------------The experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability—but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1.----------------Fig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task?----------------“Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.” This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn’t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance—i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text.----------------Overall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work explores a very interesting way to deal with the problem of propagating information through RNNs. I think the approach looks promising but the reviewers point out that the experimental evaluation is a bit lacking. In particular, it focuses on simple problems and does not demonstrate a clear advantage over LSTMs. I would encourage the authors to continue pursuing this direction, but without a theoretical advance I believe that additional empirical evidence would still be needed here.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1433,https://openreview.net/forum?id=HkuVu3ige,On Orthogonality And Learning Recurrent Networks With Long Term Dependencies,"It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.","On Orthogonality And Learning Recurrent Networks With Long Term Dependencies. The paper is well-motivated, and is part of a line of recent work investigating the use of orthogonal weight matrices within recurrent neural networks. While using orthogonal weights addresses the issue of vanishing/exploding gradients, it is unclear whether anything is lost, either in representational power or in trainability, by enforcing orthogonality. As such, an empirical investigation that examines how these properties are affected by deviation from orthogonality is a useful contribution.----------------The paper is clearly written, and the primary formulation for investigating soft orthogonality constraints (representing the weight matrices in their SVD factorized form, which gives explicit control over the singular values) is clean and natural, albeit not necessarily ideal from a practical computational standpoint (as it requires maintaining multiple orthogonal weight matrices each requiring an expensive update step). I am unaware of this approach being investigated previously.----------------The experimental side, however, is somewhat lacking. The paper evaluates two tasks: a copy task, using an RNN architecture without transition non-linearities, and sequential/permuted sequential MNIST. These are reasonable choices for an initial evaluation, but are both toy problems and don't shed much light on the practical aspects of the proposed approaches. An evaluation in a more realistic setting would be valuable (e.g., a language modeling task).----------------Furthermore, while investigating pure RNN's makes sense for evaluating effects of orthogonality, it feels somewhat academic: LSTMs also provide a mechanism to capture longer-term dependencies, and in the tasks where the proposed approach was compared directly to an LSTM, it was significantly outperformed. It would be very interesting to see the effects of the proposed soft orthogonality constraint in additional architectures (e.g., deep feed-forward architectures, or whether there's any benefit when embedded within an LSTM, although this seems doubtful).----------------Overall, the paper addresses a clear-cut question with a well-motivated approach, and has interesting findings on some toy datasets. As such I think it could provide a valuable contribution. However, the significance of the work is restricted by the limited experimental settings (both datasets and network architectures).",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The work explores a very interesting way to deal with the problem of propagating information through RNNs. I think the approach looks promising but the reviewers point out that the experimental evaluation is a bit lacking. In particular, it focuses on simple problems and does not demonstrate a clear advantage over LSTMs. I would encourage the authors to continue pursuing this direction, but without a theoretical advance I believe that additional empirical evidence would still be needed here.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1434,https://openreview.net/forum?id=HkuVu3ige,On Orthogonality And Learning Recurrent Networks With Long Term Dependencies,"It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.","On Orthogonality And Learning Recurrent Networks With Long Term Dependencies. Vanishing and exploding gradients makes the optimization of RNNs very challenging. The issue becomes worse on tasks with long term dependencies that requires longer RNNs. One of the suggested approaches to improve the optimization is to optimize in a way that the transfer matrix is almost orthogonal. This paper investigate the role of orthogonality on the optimization and learning which is very important. The writing is sound and clear and arguments are easy to follow. The suggested optimization method is very interesting. The main shortcoming of this paper is the experiments which I find very important and I hope authors can update the experiment section significantly. Below I mention some comments on the experiment section:----------------1- I think the experiments are not enough. At the very least, report the result on the adding problem and language modeling task on Penn Treebank.----------------2- I understand that the copying task becomes difficult with non-lineary. However, removing non-linearity makes the optimization very different and therefore, it is very hard to conclude anything from the results on the copying task.----------------3- I was not able to find the number of hidden units used for RNNs in different tasks.----------------4- Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used.----------------5- The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin. Am I right?----------------6- What do we learn from Figure 2? It is left without any discussion.",5: Marginally below acceptance threshold,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The work explores a very interesting way to deal with the problem of propagating information through RNNs. I think the approach looks promising but the reviewers point out that the experimental evaluation is a bit lacking. In particular, it focuses on simple problems and does not demonstrate a clear advantage over LSTMs. I would encourage the authors to continue pursuing this direction, but without a theoretical advance I believe that additional empirical evidence would still be needed here.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1438,https://openreview.net/forum?id=B1-Hhnslg,Prototypical Networks For Few-shot Learning,"A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.","Prototypical Networks For Few-shot Learning. The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages. ",5: Marginally below acceptance threshold,3: The reviewer is fairly confident that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the relationships of this work to existing work in literature (both in terms of a discussion to clarify the novelty, and in terms of more complete empirical comparisons). Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1439,https://openreview.net/forum?id=B1-Hhnslg,Prototypical Networks For Few-shot Learning,"A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.","Prototypical Networks For Few-shot Learning. This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.----------------Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this.",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the relationships of this work to existing work in literature (both in terms of a discussion to clarify the novelty, and in terms of more complete empirical comparisons). Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1440,https://openreview.net/forum?id=B1-Hhnslg,Prototypical Networks For Few-shot Learning,"A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.","Prototypical Networks For Few-shot Learning. *** Paper Summary ***----------------This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.----------------*** Review ***----------------The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. ----------------The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example ""the query"" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class ----------------I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?----------------Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.----------------*** References ***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09",4: Ok but not good enough - rejection,5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the relationships of this work to existing work in literature (both in terms of a discussion to clarify the novelty, and in terms of more complete empirical comparisons). Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1441,https://openreview.net/forum?id=HkEI22jeg,Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses,"Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary to model complex computations of neurons that cannot be captured by previous methods. Specifically, multilayer recurrent neural networks that share features across neurons outperform generalized linear models (GLMs) in predicting the spiking responses of parasol ganglion cells in the primate retina to natural images. The networks achieve good predictive performance given surprisingly small amounts of experimental training data. Additionally, we present a novel GLM-RNN hybrid model with separate spatial and temporal processing components which provides insights into the aspects of retinal processing better captured by the recurrent neural networks.","Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses. This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.----------------This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.----------------I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   ----------------I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.","8: Top 50% of accepted papers, clear accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.    I am confident enough to defend acceptance of this paper for a poster.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1442,https://openreview.net/forum?id=HkEI22jeg,Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses,"Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary to model complex computations of neurons that cannot be captured by previous methods. Specifically, multilayer recurrent neural networks that share features across neurons outperform generalized linear models (GLMs) in predicting the spiking responses of parasol ganglion cells in the primate retina to natural images. The networks achieve good predictive performance given surprisingly small amounts of experimental training data. Additionally, we present a novel GLM-RNN hybrid model with separate spatial and temporal processing components which provides insights into the aspects of retinal processing better captured by the recurrent neural networks.","Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses. This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.----------------On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.----------------On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.----------------I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.----------------I suspect followup work building on this proof of concept will be increasingly exciting.----------------Minor comments:--------Sec 3.2:--------I didn't understand the role of the 0.833 ms bins.--------Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".----------------Fig. 4 would be better with the x-axis on a log scale.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.    I am confident enough to defend acceptance of this paper for a poster.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),1.0
2017_1443,https://openreview.net/forum?id=HkEI22jeg,Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses,"Developing accurate predictive models of sensory neurons is vital to understanding sensory processing and brain computations. The current standard approach to modeling neurons is to start with simple models and to incrementally add interpretable features. An alternative approach is to start with a more complex model that captures responses accurately, and then probe the fitted model structure to understand the neural computations. Here, we show that a multitask recurrent neural network (RNN) framework provides the flexibility necessary to model complex computations of neurons that cannot be captured by previous methods. Specifically, multilayer recurrent neural networks that share features across neurons outperform generalized linear models (GLMs) in predicting the spiking responses of parasol ganglion cells in the primate retina to natural images. The networks achieve good predictive performance given surprisingly small amounts of experimental training data. Additionally, we present a novel GLM-RNN hybrid model with separate spatial and temporal processing components which provides insights into the aspects of retinal processing better captured by the recurrent neural networks.","Multilayer Recurrent Network Models Of Primate Retinal Ganglion Cell Responses. This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.----------------Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. ----------------In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. ----------------It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. --------I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.    I am confident enough to defend acceptance of this paper for a poster.",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,Accept (Poster),0.0
2017_1444,https://openreview.net/forum?id=rk5upnsxe,Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes,"Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work  well in the recurrent setting. In this paper we propose a unified view of  normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework  in the context of convolutional neural nets and recurrent neural networks, showing  improvements over baselines in image classification, language modeling as well as super-resolution.  ","Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes. The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines. ----------------Pros:----------------* It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution.--------* The paper is clearly written.   --------* From an architectural standpoint, the actual comparisons seem well motivated.   (For instance, I'm glad they tried DN* and BN* -- if they hadn't tried those, I would have wanted them too.) ----------------Cons:----------------* I'm not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization.  (Oh, actually -- looking at the other reviews, everyone else seems to have noticed this too.   I'll not beat a dead horse about this any further.)----------------* I'm nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models.  I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks.   In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn't contribute that much to performance but was somewhat expensive computationally.  Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached. ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Caffe cifar10 network can obtain around 82 % accuracy without any normalization, just by setting the learning rate schedules to {60K, 65K and 70K} starting from 0.001, 0.0001, 0.00001. Therefore, the advantages of L1 regularization are not very convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1445,https://openreview.net/forum?id=rk5upnsxe,Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes,"Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work  well in the recurrent setting. In this paper we propose a unified view of  normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework  in the context of convolutional neural nets and recurrent neural networks, showing  improvements over baselines in image classification, language modeling as well as super-resolution.  ","Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes. This paper empirically studies multiple combinations of various tricks to improve the performance of deep neural networks on various tasks. Authors investigate various combinations of normalization techniques together with additional regularizations. ----------------The paper makes few interesting empirical observations, such that the L1 regularizer on top of the activations is relatively useful for most of the tasks. ----------------In general, it seems that this work can be significantly improved by providing more precise study of existing normalization techniques. Also, studying more closely the overall volumes of the summation and suppression fields (e.g. how many samples one needs to collect for a robust enough normalization) would be useful.----------------In more detail, the work seems to have the following issues:--------* Divisive normalization, is used extensively in Krizhevsky12 (LRN). It is almost exactly the same definition as in equation 1, however with slightly different constants. Therefore claiming that it is less explored is questionable.--------* It is not clear whether the Divisive normalization does subtract the mean from the activation as there is a contradiction in its definition in equation 1 and 3. This questions whether the ""General Formulation of Normalization"" is correct.--------* In seems that Divisive normalization is used also in Jarrett09, called Contrast Normalization, with a definition more similar to equation 3 (subtracting the mean).--------* In case of the RNN experiments, it would be more clear to provide the absolute size of the summation and suppression field as BN may be inferior to DN due to a small batch size.--------* It is unclear what and how are measured the results shown in Table 10. Also it is unclear what are the sizes of the suppression/summation fields for the CIFAR and Super Resolution experiments.----------------Minor, relatively irrelevant issues:--------* It is usually better to pick a stronger baseline for the tasks. The selected CIFAR model from Caffe seems to be quite far from the state of the art on the CIFAR dataset. A stronger baseline (e.g. the widely available ResNet) would allow to see whether the proposed techniques are useful for the more recent models as well.--------* Double caption for Table 7/8.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"Caffe cifar10 network can obtain around 82 % accuracy without any normalization, just by setting the learning rate schedules to {60K, 65K and 70K} starting from 0.001, 0.0001, 0.00001. Therefore, the advantages of L1 regularization are not very convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),0.0
2017_1446,https://openreview.net/forum?id=rk5upnsxe,Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes,"Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work  well in the recurrent setting. In this paper we propose a unified view of  normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework  in the context of convolutional neural nets and recurrent neural networks, showing  improvements over baselines in image classification, language modeling as well as super-resolution.  ","Normalizing The Normalizers: Comparing And Extending Network Normalization Schemes. *** Paper Summary ***----------------This paper proposes a unified view on normalization. The framework encompases layer normalization, batch normalization and local contrast normalization. It also suggests decorrelating the inputs through L1 regularization of the activations. Results are reported on three tasks: CIFAR classification, PTB Language models and super resolution on Berkeley dataset.----------------*** Review Summary ***----------------Overall, I feel it is good to refresh the community about local normalization schemes and other mechanism to favor unit competitions. The paper reads well and reports results on various setups, with sufficient discussion. ----------------*** Detailed Review ***----------------The paper is clear and reads well. It lacks a few reference to prior research. Also I am surprised that ""Local Contrast Normalization"" is not said anywhere, as it is a common terminology in the neural network and vision literature. ----------------It is unclear to me why you chose to pair L1 regularization of the activation and normalization. They seem complementary. Would it make sense to apply L1 regularization to the baseline to highlight it is helpful on its own. Overall, it seems the only thing that brings a consistent improvement across all setups.----------------On related work, maybe it would be worthwhile to insist that Local Contrast Normalization (LCN) used to be very popular [Pinto et al, 2008, Jarret et al 2009, Sermanet et al 2012; Quoc Le 2013] and effective. It is great to connect this litterature to current work on layer normalization and batch normalization. Similarly, sparsity or group sparsity of the activation has shown effective in the past [Rozell et al 08, Kavukcuoglu et al 09] and need more exposure today.----------------Finally, since dropout is so popular but interact poorly with normalizer estimates, I feel it would be worthwhile to report results with dropout beyond the baseline and discuss how the different normalization scheme interact with it.----------------Overall, I feel it is good to refresh the community about local normalization schemes and other mechanism to favor unit competitions. The paper reads well and reports results on various setups, with sufficent discussion. ----------------*** References ***----------------Jarrett, Kevin, Koray Kavukcuoglu, and Yann Lecun. ""What is the best multi-stage architecture for object recognition?."" 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009.----------------Pinto, N., Cox, D., DiCarlo, J.: Why is real-world visual object recognition hard?--------PLoS Comput Biol 4 (2008)----------------Le, Quoc V. ""Building high-level features using large scale unsupervised learning."" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013.----------------P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house--------numbers digit classification. In ICPR, 2012.----------------C. Rozell, D. Johnson, and B. Olshausen. Sparse coding via thresholding and local competition in neural circuits.Neural Computation, 2008.----------------K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun. Learning invariant features through topographic filter maps. In CVPR, 2009.","9: Top 15% of accepted papers, strong accept",5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature,"Caffe cifar10 network can obtain around 82 % accuracy without any normalization, just by setting the learning rate schedules to {60K, 65K and 70K} starting from 0.001, 0.0001, 0.00001. Therefore, the advantages of L1 regularization are not very convincing.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Accept (Poster),1.0
2017_1450,https://openreview.net/forum?id=SyOvg6jxx,#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning,"Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation.   In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results.  Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.","#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning. This paper introduces a new way of extending the count based exploration approach to domains where counts are not readily available. The way in which the authors do it is through hash functions. Experiments are conducted on several domains including control and Atari. ----------------It is nice that the authors confirmed the results of Bellemare in that given the right ""density"" estimator, count based exploration can be effective. It is also great the observe that given the right features, we can crack games like Montezuma's revenge to some extend.----------------I, however, have several complaints:----------------First, by using hashing, the authors did not seem to be able to achieve significant improvements over past approaches. Without ""feature engineering"", the authors achieved only a fraction of the performance achieved in Bellemare et al. on Montezuma's Revenge. The proposed approaches In the control domains, the authors also does not outperform VIME. So experimentally, it is very hard to justify the approach. ----------------Second, hashing, although could be effective in the domains that the authors tested on, it may not be the best way of estimating densities going forward. As the environments get more complicated, some learning methods, are required for the understanding of the environments instead of blind hashing. The authors claim that the advantage of the proposed method over Bellemare et al. is that one does not have to design density estimators. But I would argue that density estimators have become readily available (PixelCNN, VAEs, Real NVP, GANs) that they can be as easily applied as can hashing. Training the density estimators is not difficult problem as more.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The paper proposes a simple approach to exploration that uses a hash of the current state within a exploration bonus approach (there are some modifications to learned hash codes, but this is the basic approach). The method achieves reasonable performance on Atari game tasks (sometimes outperformed by other approaches, but overall performing well), and it's simplicity is its main appeal (although the autoencoder-based learned hash seems substantially less simple, so actually loses some advantage there).     The paper is likely borderline, as the results are not fantastic: the approach is typically outperformed or similarly-performed by one of the comparison approaches (though it should be noted that no comparison approach performs well over all tasks, so this is not necessarily that bad). But overall, especially because so many of these methods have tunable hyperparameters it was difficult to get a clear understanding of just how these experimental results fit.    Pros:  + Simple method for exploration that seems to work reasonably well in practice  + Would have the potential to be widely used because of its simplicity    Cons:  - Improvements over previous approaches are not always there, and it's not clear whether the algorithm has any ""killer app"" domain where it is just clearly the best approach     Overall, this work in its current form is too borderline. The PCs encourage the authors to strengthen the empirical validation and resubmit.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1451,https://openreview.net/forum?id=SyOvg6jxx,#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning,"Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation.   In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results.  Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.","#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning. The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman.----------------Several points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts). Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games. In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent. The results indicate that the approach clearly improves over the baseline. The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity. Third, the paper presents results on the sensitivity to the granularity of the abstraction.----------------I have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions. I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices. For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS? The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent.----------------The authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts. There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any. Why?----------------It seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange. The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses.----------------The case study on Montezuma's revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper.----------------So, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper.------------------- After response:----------------Thank you for the thorough response, and again my apologies for the late reply.----------------I appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting.----------------The paper addresses an important problem (exploration), suggesting a ""simple"" (compared to density estimation) counting method via hashing. It is a nice alternative approach to the one offered by Bellemare et al. If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper. Specifically, I am not as concerned about beating the state of the art in Montezuma's as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO. This paper shows that we should not give up on simple hashing. There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible. Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention.----------------Not important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man. I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN. Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments). The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count! This is hardly indicative of DQN's sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes. Another source of stability, DQN uses a target network that changes infrequently. Perhaps the authors made a mistake in the reference graph in the figure? (I see no Figure 9 in https://arxiv.org/pdf/1602.01783v2.pdf , I assume the authors meant Figure S9)",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a simple approach to exploration that uses a hash of the current state within a exploration bonus approach (there are some modifications to learned hash codes, but this is the basic approach). The method achieves reasonable performance on Atari game tasks (sometimes outperformed by other approaches, but overall performing well), and it's simplicity is its main appeal (although the autoencoder-based learned hash seems substantially less simple, so actually loses some advantage there).     The paper is likely borderline, as the results are not fantastic: the approach is typically outperformed or similarly-performed by one of the comparison approaches (though it should be noted that no comparison approach performs well over all tasks, so this is not necessarily that bad). But overall, especially because so many of these methods have tunable hyperparameters it was difficult to get a clear understanding of just how these experimental results fit.    Pros:  + Simple method for exploration that seems to work reasonably well in practice  + Would have the potential to be widely used because of its simplicity    Cons:  - Improvements over previous approaches are not always there, and it's not clear whether the algorithm has any ""killer app"" domain where it is just clearly the best approach     Overall, this work in its current form is too borderline. The PCs encourage the authors to strengthen the empirical validation and resubmit.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1452,https://openreview.net/forum?id=SyOvg6jxx,#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning,"Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation.   In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results.  Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.","#exploration: A Study Of Count-based Exploration For Deep Reinforcement Learning. This paper proposed to use a simple count-based exploration technique in high-dimensional RL application (e.g., Atari Games). The counting is based on state hash, which implicitly groups (quantizes) similar state together. The hash is computed either via hand-designed features or learned features (unsupervisedly with auto-encoder). The new state to be explored receives a bonus similar to UCB (to encourage further exploration).----------------Overall the paper is solid with quite extensive experiments. I wonder how it generalizes to more Atari games. Montezuma’s Revenge may be particularly suitable for approaches that implicitly/explicitly cluster states together (like the proposed one), as it has multiple distinct scenarios, each with small variations in terms of visual appearance, showing clustering structures. On the other hand, such approaches might not work as well if the state space is fully continuous (e.g. in RLLab experiments). ----------------The authors did not answer my question about why the hash code needs to be updated during training. I think it is mainly because the code still needs to be adaptive for a particular game (to achieve lower reconstruction error) in the first few iterations . After that stabilization is the most important. Sec. 2.3 (Learned embedding) is quite confusing (but very important). I hope that the authors could make it more clear (e.g., by writing an algorithm block) in the next version.","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes a simple approach to exploration that uses a hash of the current state within a exploration bonus approach (there are some modifications to learned hash codes, but this is the basic approach). The method achieves reasonable performance on Atari game tasks (sometimes outperformed by other approaches, but overall performing well), and it's simplicity is its main appeal (although the autoencoder-based learned hash seems substantially less simple, so actually loses some advantage there).     The paper is likely borderline, as the results are not fantastic: the approach is typically outperformed or similarly-performed by one of the comparison approaches (though it should be noted that no comparison approach performs well over all tasks, so this is not necessarily that bad). But overall, especially because so many of these methods have tunable hyperparameters it was difficult to get a clear understanding of just how these experimental results fit.    Pros:  + Simple method for exploration that seems to work reasonably well in practice  + Would have the potential to be widely used because of its simplicity    Cons:  - Improvements over previous approaches are not always there, and it's not clear whether the algorithm has any ""killer app"" domain where it is just clearly the best approach     Overall, this work in its current form is too borderline. The PCs encourage the authors to strengthen the empirical validation and resubmit.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1468,https://openreview.net/forum?id=BJK3Xasel,Nonparametric Neural Networks,"Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an  penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an  penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\it AdaRad}, and obtain promising results.","Nonparametric Neural Networks. This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below:----------------What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments.----------------It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?","7: Good paper, accept",3: The reviewer is fairly confident that the evaluation is correct,"The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1469,https://openreview.net/forum?id=BJK3Xasel,Nonparametric Neural Networks,"Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an  penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an  penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\it AdaRad}, and obtain promising results.","Nonparametric Neural Networks. I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge.----------------The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data.--------That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would  have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions.----------------I have a few other comments to make:----------------1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory?----------------I understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection.----------------In some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure. ----------------What i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight?----------------2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight? ----------------As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda  be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true? ----------------How much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they?----------------What happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result?----------------I have a few additional questions:----------------1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values?----------------2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think.----------------I changed my rating to 7, while hoping that the authors will address my comments above.------------------------ ","7: Good paper, accept",4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),1.0
2017_1470,https://openreview.net/forum?id=BJK3Xasel,Nonparametric Neural Networks,"Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an  penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an  penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\it AdaRad}, and obtain promising results.","Nonparametric Neural Networks. This paper addresses the problem of allowing networks to change the number of units that are used during training.  This is done in a simple but elegant and well-motivated way.  Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero.  The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units.  In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case.----------------One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters.   One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem.  The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero.  It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches.  In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case.----------------The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically.  This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular.  However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks.----------------Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time.  Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy.  This means that the cost of grid search is not always paid, but the slowness of the authors’ approach may be endemic.  The authors do not discuss how this issue will scale as much larger networks are trained.  It is a concern that this approach may not be practical for large-scale networks, because training will be very slow.----------------In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.  I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful.  ----------------Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point.  The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision.  But the current results remain pretty speculative.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.",3: The reviewer is fairly confident that the evaluation is correct,Accept (Poster),0.0
2017_1471,https://openreview.net/forum?id=rkuDV6iex,An Empirical Analysis Of Deep Network Loss Surfaces,"The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.","An Empirical Analysis Of Deep Network Loss Surfaces. First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.----------------Major points:------------------ ""As we saw in the previous section, the minima of deep network loss functions are for the most part decent.""----------------All you said in the previous section was that theory shows that there are no bad minima under ""strong assumptions"". There is no practical proof that minima do not vary in quality.------------------ ""This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a ""decent minima quickly"" is reduced to the task of finding any minima quickly.""----------------First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being ""reduced to finding any minima quickly"".------------------ Figure 1-------- --------I don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.-------- --------Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.------------------ Misuse of the transient phase / minimization phase concept----------------In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?------------------ Only 1 dataset-------- --------You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.-------- ---------- Many figures are unclear-------- --------For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.-------- ---------- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.--------  ---------- Lack of confidence intervals----------------The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.------------------ Lack of information regarding learning rate----------------There is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.------------------ Lack of information regarding the absolute distance of interpolated points----------------In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big ""hump"" between them, it means that those points are more ""brittle"" than if they are far apart and there is a big ""hump"" between them.-------- --------Minor points:-------- ---------- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.--------  ---------- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write ""best viewed in zoom"" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.",4: Ok but not good enough - rejection,3: The reviewer is fairly confident that the evaluation is correct,"The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.    The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims.     A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1472,https://openreview.net/forum?id=rkuDV6iex,An Empirical Analysis Of Deep Network Loss Surfaces,"The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.","An Empirical Analysis Of Deep Network Loss Surfaces. The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. ----------------It would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.    The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims.     A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1473,https://openreview.net/forum?id=rkuDV6iex,An Empirical Analysis Of Deep Network Loss Surfaces,"The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.","An Empirical Analysis Of Deep Network Loss Surfaces. I appreciate the work but I do not think the paper is clear enough. --------Moreover, the authors say ""local minimia"" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. --------The authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. --------It is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. --------Some sentences like the one given below suggest that the study is too superficial:  --------""One of the interesting empirical observation is that we often observe is that the incremental improvement--------of optimization methods decreases rapidly even in non-convex problems.""",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.    The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims.     A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.",3: The reviewer is fairly confident that the evaluation is correct,Reject,1.0
2017_1474,https://openreview.net/forum?id=rkuDV6iex,An Empirical Analysis Of Deep Network Loss Surfaces,"The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.","An Empirical Analysis Of Deep Network Loss Surfaces. This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.----------------Pros:----------------- Important analysis--------- Good visualizations----------------Cons:----------------- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)--------- Some fonts are very small (e.g. Fig. 5)",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.    The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims.     A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue.",3: The reviewer is fairly confident that the evaluation is correct,Reject,0.0
2017_1490,https://openreview.net/forum?id=H1_EDpogx,Near-data Processing For Machine Learning,"In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein. By offloading the computational burden of CPU and saving the need for transferring raw data in its entirety, NDP exhibits a great potential for acceleration and power reduction. Despite this potential, specific research activities on NDP have witnessed only limited success until recently, often owing to performance mismatches between logic and memory process technologies that put a limit on the processing capability of memory. Recently, there have been two major changes in the game, igniting the resurgence of NDP with renewed interest. The first is the success of machine learning (ML), which often demands a great deal of computation for training, requiring frequent transfers of big data. The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore processors that can accommodate extra computation for data processing. Sparked by these application needs and technological support, we evaluate the potential of NDP for ML using a new SSD platform that allows us to simulate in-storage processing (ISP) of ML workloads. Our platform (named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD that can execute various ML algorithms using the data stored in the SSD. For thorough performance analysis and in-depth comparison with alternatives, we focus on a specific algorithm: stochastic gradient decent (SGD), which is the de facto standard for training differentiable learning machines including deep neural networks. We implement and compare three variants of SGD (synchronous, Downpour, and elastic averaging) using ISP-ML, exploiting the multiple NAND channels for parallelizing SGD. In addition, we compare the performance of ISP and that of conventional in-host processing, revealing the advantages of ISP. Based on the advantages and limitations identified through our experiments, we further discuss directions for future research on ISP for accelerating ML.","Near-data Processing For Machine Learning. For more than a decade, near data processing has been a key requirement for large scale linear learning platforms, as the time to load the data exceeds the learning time, and this has justified the introduction of approaches such as Spark----------------Deep learning usually deals with the data that can be contained in a single machine and the bottleneck is often the CPU-GPU bus or the GPU-GPU-bus, so a method that overcomes this bottleneck could be relevant.----------------Unfortunately, this work is still very preliminary and limited to linear training algorithms, so of little interest yet to ICLR readership. I would recommend publication to a conference where it can reach the large-scale linear ML audience first, such as ICML. This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task. Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR, but can the flash memory FPGA handle it?----------------For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it)",4: Ok but not good enough - rejection,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper is well motivated and clearly written, and is representative of the rapidly growing interdisciplinary area of hardware-software co-design for handling large-scale Machine Learning workloads. In particular, the paper develops a detailed simulator of SSDs with onboard multicore processors so that ML computations can be done near where the data resides.    Reviewers are however unanimously unconvinced about the potential impact of the simulator, and more broadly the relevance to ICLR. The empirical section of the paper is largely focused on benchmarking logistic regression models on MNIST, which reviewers find underwhelming. It is conceivable that the results reflect performance on real hardware, but the ICLR community would atleast expect to see realistic deep learning workloads on larger datasets such as Imagenet, where scalability challenges have been throughly studied. Without such results, the impact of the contribution is hard to evaluate and the claimed gains are bit of a leap of faith.     The authors make several good points in their response about the paper - that their method is expected to scale, that high quality simulations can given insights that can inform hardware manufacturing, and that their approach complements other hardware and algorithmic acceleration strategies. They are encouraged to resubmit the paper with a stronger empirical section, e.g., benchmarking training and inference of Inception-like models on ImageNet.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1491,https://openreview.net/forum?id=H1_EDpogx,Near-data Processing For Machine Learning,"In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein. By offloading the computational burden of CPU and saving the need for transferring raw data in its entirety, NDP exhibits a great potential for acceleration and power reduction. Despite this potential, specific research activities on NDP have witnessed only limited success until recently, often owing to performance mismatches between logic and memory process technologies that put a limit on the processing capability of memory. Recently, there have been two major changes in the game, igniting the resurgence of NDP with renewed interest. The first is the success of machine learning (ML), which often demands a great deal of computation for training, requiring frequent transfers of big data. The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore processors that can accommodate extra computation for data processing. Sparked by these application needs and technological support, we evaluate the potential of NDP for ML using a new SSD platform that allows us to simulate in-storage processing (ISP) of ML workloads. Our platform (named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD that can execute various ML algorithms using the data stored in the SSD. For thorough performance analysis and in-depth comparison with alternatives, we focus on a specific algorithm: stochastic gradient decent (SGD), which is the de facto standard for training differentiable learning machines including deep neural networks. We implement and compare three variants of SGD (synchronous, Downpour, and elastic averaging) using ISP-ML, exploiting the multiple NAND channels for parallelizing SGD. In addition, we compare the performance of ISP and that of conventional in-host processing, revealing the advantages of ISP. Based on the advantages and limitations identified through our experiments, we further discuss directions for future research on ISP for accelerating ML.","Near-data Processing For Machine Learning. Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.--------The paper itself is well-written, but unfortunately addresses a lot of things only to medium depth (probably due length constraints).--------My opinion is that a journal with an in-depth discussion of the technical details would be a better target for this paper.----------------Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach.--------With such a big focus on GPU (and more specialized hardware such as TPUs), the one question that comes to mind: By how much does this - or do you expect it to - beat the latest and greatest GPU on a real task?----------------I don't consider myself an expert on this topic even though I have some experience with SystemC.",6: Marginally above acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This paper is well motivated and clearly written, and is representative of the rapidly growing interdisciplinary area of hardware-software co-design for handling large-scale Machine Learning workloads. In particular, the paper develops a detailed simulator of SSDs with onboard multicore processors so that ML computations can be done near where the data resides.    Reviewers are however unanimously unconvinced about the potential impact of the simulator, and more broadly the relevance to ICLR. The empirical section of the paper is largely focused on benchmarking logistic regression models on MNIST, which reviewers find underwhelming. It is conceivable that the results reflect performance on real hardware, but the ICLR community would atleast expect to see realistic deep learning workloads on larger datasets such as Imagenet, where scalability challenges have been throughly studied. Without such results, the impact of the contribution is hard to evaluate and the claimed gains are bit of a leap of faith.     The authors make several good points in their response about the paper - that their method is expected to scale, that high quality simulations can given insights that can inform hardware manufacturing, and that their approach complements other hardware and algorithmic acceleration strategies. They are encouraged to resubmit the paper with a stronger empirical section, e.g., benchmarking training and inference of Inception-like models on ImageNet.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
2017_1492,https://openreview.net/forum?id=H1_EDpogx,Near-data Processing For Machine Learning,"In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein. By offloading the computational burden of CPU and saving the need for transferring raw data in its entirety, NDP exhibits a great potential for acceleration and power reduction. Despite this potential, specific research activities on NDP have witnessed only limited success until recently, often owing to performance mismatches between logic and memory process technologies that put a limit on the processing capability of memory. Recently, there have been two major changes in the game, igniting the resurgence of NDP with renewed interest. The first is the success of machine learning (ML), which often demands a great deal of computation for training, requiring frequent transfers of big data. The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore processors that can accommodate extra computation for data processing. Sparked by these application needs and technological support, we evaluate the potential of NDP for ML using a new SSD platform that allows us to simulate in-storage processing (ISP) of ML workloads. Our platform (named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD that can execute various ML algorithms using the data stored in the SSD. For thorough performance analysis and in-depth comparison with alternatives, we focus on a specific algorithm: stochastic gradient decent (SGD), which is the de facto standard for training differentiable learning machines including deep neural networks. We implement and compare three variants of SGD (synchronous, Downpour, and elastic averaging) using ISP-ML, exploiting the multiple NAND channels for parallelizing SGD. In addition, we compare the performance of ISP and that of conventional in-host processing, revealing the advantages of ISP. Based on the advantages and limitations identified through our experiments, we further discuss directions for future research on ISP for accelerating ML.","Near-data Processing For Machine Learning. While the idea of moving the processing for machine learning into silicon contained within the (SSD) data storage devices is intriguing and offers the potential for low-power efficient computation, it is a rather specialized topic, so I don't feel it will be of especially wide interest to the ICLR audience. The paper describes simulation results, rather than actual hardware implementation, and describes implementations of existing algorithms. --------The comparisons of algorithms' train/test performance does not seem relevant (since there is no novelty in the algorithms) and the use of a single layer perceptron on MNIST calls into question the practicality of the system, since this is a tiny neural network by today's standards. I did not understand from the paper how it was thought that this could scale to contemporary scaled networks, in terms of numbers of parameters for both storage and bandwidth. ----------------I am not an expert in this area, so have not evaluated in depth. ",5: Marginally below acceptance threshold,"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper","This paper is well motivated and clearly written, and is representative of the rapidly growing interdisciplinary area of hardware-software co-design for handling large-scale Machine Learning workloads. In particular, the paper develops a detailed simulator of SSDs with onboard multicore processors so that ML computations can be done near where the data resides.    Reviewers are however unanimously unconvinced about the potential impact of the simulator, and more broadly the relevance to ICLR. The empirical section of the paper is largely focused on benchmarking logistic regression models on MNIST, which reviewers find underwhelming. It is conceivable that the results reflect performance on real hardware, but the ICLR community would atleast expect to see realistic deep learning workloads on larger datasets such as Imagenet, where scalability challenges have been throughly studied. Without such results, the impact of the contribution is hard to evaluate and the claimed gains are bit of a leap of faith.     The authors make several good points in their response about the paper - that their method is expected to scale, that high quality simulations can given insights that can inform hardware manufacturing, and that their approach complements other hardware and algorithmic acceleration strategies. They are encouraged to resubmit the paper with a stronger empirical section, e.g., benchmarking training and inference of Inception-like models on ImageNet.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1505,https://openreview.net/forum?id=HkNEuToge,Energy-based Spherical Sparse Coding,"In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors and reconstructions are evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.","Energy-based Spherical Sparse Coding. The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block.----------------Positives:--------The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives.----------------Negatives:--------The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). ----------------The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a variant of convolutional sparse coding with unit norm code vectors using cosine distance to evaluate reconstructions. The performance gains over baseline networks are quite minimal and demonstrated on limited datasets, therefore this work fails to demonstrate practical usefulness, while the novelty of the contribution is too slight to stand on its own merit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1506,https://openreview.net/forum?id=HkNEuToge,Energy-based Spherical Sparse Coding,"In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors and reconstructions are evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.","Energy-based Spherical Sparse Coding. This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach. The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary). The experiments do not show significant improvement against baselines. ----------------Pros: --------- Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required. --------- The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity. ----------------Cons:--------- Unrolling sparse coding inference as a feed-foward network is not new. --------- The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class. --------- It does not show the proposed method could outperform baseslines in real-world tasks.",5: Marginally below acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a variant of convolutional sparse coding with unit norm code vectors using cosine distance to evaluate reconstructions. The performance gains over baseline networks are quite minimal and demonstrated on limited datasets, therefore this work fails to demonstrate practical usefulness, while the novelty of the contribution is too slight to stand on its own merit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,1.0
2017_1507,https://openreview.net/forum?id=HkNEuToge,Energy-based Spherical Sparse Coding,"In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors and reconstructions are evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.","Energy-based Spherical Sparse Coding. First, I'd like to thank the authors for their answers and clarifications.--------I find, the presentation of the multi-stage version of the model much clearer now.----------------Pros:----------------+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.----------------+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. ----------------Cons:----------------+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.----------------+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)--------------------------------------The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).----------------Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.----------------Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.----------------Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.----------------Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.----------------Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.------------------------Minor comments:----------------I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).",6: Marginally above acceptance threshold,4: The reviewer is confident but not absolutely certain that the evaluation is correct,"This paper proposes a variant of convolutional sparse coding with unit norm code vectors using cosine distance to evaluate reconstructions. The performance gains over baseline networks are quite minimal and demonstrated on limited datasets, therefore this work fails to demonstrate practical usefulness, while the novelty of the contribution is too slight to stand on its own merit.",4: The reviewer is confident but not absolutely certain that the evaluation is correct,Reject,0.0
